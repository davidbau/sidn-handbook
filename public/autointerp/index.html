<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/autointerp/index.html">Automated Interpretability</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Automated Interpretability</h1>


<div>
  <h5>December 3, 2024 • <em>David Atkinson, Sheridan Feucht</em></h5>

  Is it possible to automate the process of interpreting units in a neural network? 
  Maybe, if we turn LLMs back on themselves. The papers for today's class focus on 
  how we can use LLMs to explain the inputs/outputs of units within a network. 

  <h2>
    MAIA: A Multimodal Automated Interpretability Agent
  </h2>
  TODO (David)
  
  <h2>
    Language Models Can Explain Neurons in Language Models
  </h2>
  TODO (David)

  <h2>
    Explaining Black Box Text Modules in Natural Language With Language Models
  </h2>
  <p>
    <a href="https://arxiv.org/abs/2305.09863">This paper</a> provides another 
    approach to describing the selectivity of units within language models—one that 
    they also apply to fMRI data! 
    The first authors of this paper are Chandan Singh, a senior researcher at Microsoft Research, 
    and Aliyah R. Hsu, a fifth-year PhD student at UC Berkeley. It seems that this project was 
    primarily completed at Microsoft Research, but they also collaborate with Alexander Huth, a professor 
    of neuroscience at UT Austin, and his PhD student, Richard Antonello (who is now a postdoc at Columbia). 
  </p>
  <p>
    This paper introduces a method for analyzing <i>text modules</i>, which they define as any 
    function that maps text to a continuous scalar value. This could be a neuron within an LLM or a voxel 
    in an fMRI scan (when responding to language stimuli). To analyze these modules, they introduce a method 
    called <b>Summarize And SCore (SASC)</b>, which takes some text module <i>f</i> and generates a natural language 
    description of the module, as well as a confidence score of how good the explanation is. SASC is a two-step process:
  </p>
  <ol>
    <li>
        <b>Summarization.</b> Based on a reference corpus, ngrams that activate <i>f</i> the most are sampled. 
        A pre-trained "helper LLM" is then used to generate a summary of those ngrams, which acts as a description of what <i>f</i> responds to. 
    </li>
    <li>
        <b>Synthetic Scoring.</b> To evaluate an explanation of <i>f</i>, the helper LLM is used to generate synthetic data 
        conditioned on that explanation. Then, the mean difference between the text module evaluated on synthetic text \( f(Text^+)\) 
        and the text module evaluated on unrelated synthetic text \( f(Text^-) \) is calculated. Their score is measured 
        in units of standard deviations; for example, a SASC score of \( 1\sigma_f \) indicates that synthetic data based 
        on that explanation increased activations of <i>f</i> by one standard deviation from the mean. 
    </li>
  </ol>
  <p>
    Their Figure 1 shows an example of this process for a module that responds to ngrams like "wow I never".  
    As they mention, the efficacy of this method depends a lot on the length of ngrams fed through the model; however, longer 
    ngrams require more computation time. Another thing to note is that in practice, they 
    use a large generic corpus to calculate \( f(Text^-) \), instead of synthetically generating "neutral" text. 
  </p>
  <img src="images/sasc.png" style="max-width:90%; width:900px;" class="mx-auto d-block">

  <h3>Synthetic Module Evaluation</h3>
  <p>
    First, the authors see how well SASC works when trying to recover descriptions of synthetic text modules. They use a dataset 
    from <a href="https://github.com/ruiqi-zhong/Meta-tuning">Zhong et al. (2021)</a> consisting of keyphrase descriptions of 
    examples in a dataset (e.g. <i>related to math</i>, <i>contains sarcasm</i>), and then use a 
    <a href="https://huggingface.co/hkunlp/instructor-xl">text embedding model</a> to embed input examples and output 
    the negative Euclidean distance between the input and the keyphrase description. This gives us text modules that 
    we know the "ground truth" explanation for. 
</p>
<p>
    They find that SASC successfully identifies 88% of the ground-truth explanations. If the reference corpus is restricted, 
    or if a lot of noise is added to <i>f</i>, SASC is still successful about 67% of the time. However, they do use examples 
    from the Zhong et al. (2021) dataset as their reference corpus, which seems like it might inflate the efficacy of this method, 
    even in the restricted setting. 
</p>

<h3>BERT Evaluation</h3>
<p>
    Instead of analyzing individual neurons in BERT, the authors analyze <i>transformer factors</i> from
    <a href="https://arxiv.org/abs/2103.15949">Yun et al. (2021).</a> These are features found via 
    sparse over-complete dictionary learning, in a paper that was a precursor to 
    <a href="https://transformer-circuits.pub/2023/monosemantic-features">Anthropic's SAE investigations</a>.
    <img src="images/humanvsasc.png" style="max-width:90%; width:900px;" class="mx-auto d-block">

    They find that, using their scoring method with GPT-3, SASC explanations score higher than human explanations. 
    However, scores become worse in later layers. 
</p>
<p>
    To further evaluate these explanations, they fit a logistic regression to the factor coefficients to perform 
    specific tasks like emotion classification, news topic classificiation, and movie review sentiment classification. 
    When the top 25 regression coefficients are examined qualitatively, they find that e.g. the feature labeled 
    "professional sports teams" contributes heavily to classification of news articles being sports-related. 
</p>

<h3>fMRI Comparison</h3>
Here are two interesting highlights from their fMRI analysis (read the full paper for details). 
One is that explanation scores for fMRI voxels are much lower than they are for early layers in BERT (but similar to middle BERT layers). 
<img src="images/sascranges.png" style="max-width:90%; width:900px;" class="mx-auto d-block">

The other thing is that if you fit a topic model to all of the explanations found by SASC, fMRI explanations have a 
much higher proportion of explanations related to the topic <i>action, movement, relationships...</i>. This is apparently 
consistent with <a href="https://pubmed.ncbi.nlm.nih.gov/27121839/">prior findings</a> showing that the largest axis of 
variation in fMRI voxels is between social and physical concepts.


  <h2>Code Resources</h2>
  <p>TODO</p>


  
</main>
</div>
</div>
</body>
</html>

</div>
