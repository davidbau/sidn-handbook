<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/history/index.html">A History of Interpretable Machine Learning</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">A History of Interpretable Machine Learning</h1>


<h5>September 5, 2024 • <em>David Bau</em></h5>

<p>
This course arrives in late 2024, as interest in interpretable machine learning is growing.
Interest is being driven by remarkable gains in capabilities in large-scale generative AI,
especially large transformer language models such as ChatGPT and text-to-image synthesis
diffusion  models such as DallE. The question of the moment is "how do these
huge generative neural networks work?"

<p>
Yet the puzzle of interpretability is inherent to <em>all</em>
machine learning, and the problem has challanged computer scientists
for many years.  In this chapter, we take a moment to trace the hundred
years of intellectual debate about interpretable machine learning
that have preceding the current generative moment in AI.

<h2>From Lovelace to Turing to Minksy and Wolpert</h2>

<p>
Deeply ingrained in our conception of computer science is the
belief that <em>programs reflect the insights and understanding of
the programmer</em>. This notion was famously articulated by
Ada Lovelace who created programs for the Babbage analytical engine.

<blockquote class="blockquote">
The Analytical Engine has no pretensions whatever to <b>originate</b> anything.
It can do whatever <b>we know</b> how to order it to perform.
<address>- Ada Lovelace</address>
</blockquote>

<p>
Yet Alan Turing disagreed that computer science would forever be
tethered to the boundaries of human knowledge; he argued that
it would be possible to teach machines how to <em>learn</em> automatically.
<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf"
>In his 1950 essay on machine intelligence</a>, Turing
pointed out that machine larning would immediately lead
to the challenge of human interpretability:

<blockquote class="blockquote">
An important feature of a learning machine is that its teacher
will often be very <b>largely ignorant</b> of quite what is going on inside,
although he may still be able to some extent to predict his pupil's behavior.
<address>- Alan Turing
(<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf">1950, p. 458</a>)
</address>
</blockquote>

<p>
While Turing was fascinated by the inherent human ignorance about
the internals of machine-learned systems, that blindness was
one reason for the slow uptake of machine learning methods
in early decades of computer science.

<h3>No Free Lunch and the Role of the Programmer</h3>

<p>
For those who might believe that removing human insight from
machine learning is a <em>good</em> thing, it is important
for machine learning scientists to absorb the lessons of the
<a href="https://arxiv.org/abs/2007.10928">No-Free-Lunch (NFL)
theorem by Wolpert and Macready</a>:

<blockquote class="blockquote">
All learning methods perform equally well (and equally badly)
when averaged over all possible problems.
<address>- David Wolpert</address>
</blockquote>

<p>
In other words, learning algorithms only succeed because
they are tuned for particular kinds of problems; those
same learning methods are guaranteed to fail on just as many
other problems.

<p>
A visual constructive proof of the NFL theorem can be found
at an old <a href="https://web.archive.org/web/20230315015432/https://mlu.red/muse/52449366310.html"
>anonymous blog entry, here</a>.

<img src="images/nfl-illustration.svg" style="max-width:50%; width:500px;" class="mx-auto d-block">

<p>
The NFL means that machine learning provides no refuge for
the responsibility of the human programmer. There is no such
thing as a system that can "learn everything."
Humans <b>are stuck making decisions</b>
about what kinds of biases we want our ML systems to have, and
what kinds of problems we need our ML systems to solve.

<h3>Human Responsibility is Unavoidable</h3>

<p>
The irreducible nature of the responsibility of
the programmer famously led Marvin Minksy to steer AI funding
in the 1970s away from machine learning methods such as
<a href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon"
>Rosenblatt's neural network methods</a>, towards "good old fashioined"
<a href="https://projects.csail.mit.edu/films/aifilms/AIFilms.html"
>symbolic AI exemplified by the LISP community</a>.  Minsky believed
it was important to put human intention, logic, and interpretability
front and center of the development of artificial intelligence.

<p>Minsky is famous for the following parable about
a his advice to a young Jerry Sussman:

<blockquote class="blockquote">
<p>In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
<p>"What are you doing?", asked Minsky.
<p>"I am training a randomly wired neural net to play Tic-Tac-Toe" Sussman replied.
<p>"Why is the net wired randomly?", asked Minsky.
<p>"I do not want it to have any preconceptions of how to play”, Sussman said.
<p>Minsky then shut his eyes.
<p>"Why do you close your eyes?", Sussman asked his teacher.
<p>"So that the room will be empty."
<p>At that moment, Sussman was enlightened.
</blockquote>

<p>
Yet in the end, it proved to be Minsky's early AI pioneers, not the
machine learning programmers, who had closed their eyes to the profound
challenge of interpretability.  By the 1980s, machine learning was becoming
so useful that it could not be avoided.

<p>
Rather than retaining human insight by avoiding
machine learning, computer scientists would need to confront
the unsolved puzzle of how to understand self-programmed
machine-learned systems. Beginning around 1985, under the leadership
of statisticians and pychologists, they embarked on this journey
with two very different approaches.

<h2>Approach 1: Intrinsically Interpretable Models</h2>

<p>The community's first answer to ML interpretability
was to make the problem easier by setting aside ML algorithms
that were "uninterpretable," like neural networks, and instead focus
on "<em>interpretable machine learning</em>" methods
that were designed to be understandable to humans.

<p>As an example, we will examine two of the main approaches to
interpretable ML that date from the 1980s that are still
widely used today: Decision Trees and Generalized Additive Models.
Both these ideas sprang from clever learning algorithms devised by
the statistician <a href="https://en.wikipedia.org/wiki/Leo_Breiman"
>Leo Breiman</a>, who is behind many of the foundational ideas of
machine learning.

<h3>Decision Trees</h3>

<p>A decision tree is a classification algorithm consisting of a series
of decision steps. Typically, at each step, a single variable is
examined, and a choice between next steps is taken depending on
a threshold value for the variable.  Decision trees
are highly interpretable and are often simple enough that they can
be executed "by hand" by people. In the real world, decision
trees are still often used exactly in this way: for example, here is
the standard high-speed emergency room decision tree called START.
It is designed to be simple enough for EMTs to memorize and apply
in seconds to triage the most serious emergency cases.

<img src="images/start-decision-tree.jpg" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Classic approach: CART trees</h3>

<p>
The most popular decision tree learning algorithm is called CART
(<a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-olshen-charles-stone">Brieman, et al, 1984</a>), and it works
by constructing a decision tree with a simple greedy heuristic:

<ol>
<li>For each input variable, a threshold is found that maximizes
the "purity" of the separated training samples, where
purity is quantified by the Gini value (in which \( p_k \) denotes
the portion of a new partition that is occupied by class \( k \)).

\[ 1 - \sum_{k} p_{k}^2 \]

<li>The tree is built by adding a choice and threshold for
the variable that achieves the highest highest Gini index
at each step, with each step subdividing the choices
until the accuracy is as high as you want
</ol>

<p>
The resulting decision tree boundaries look like rectilinear planes.
You can see the effect concretely in
<a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/06_decision_trees.ipynb"
>this colab notebook, by <a href="https://github.com/ageron/">Aur&eacute;lien Geron</a>,
which illustrates how to use Scikit-learn to run the standard CART decision tree algorithm.

<img src="images/decision-scatter.png" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Current Research: Optimal Decision Trees</h3>

<p>
CART decision trees are not optimal; there can be more economical
sets of choices that lead to higher accuracy classifiers than the choices
found by this greedy approach.  Optimal decision tree algorithms remain an
active area of research today. For example, in 2019, 
<a href="https://arxiv.org/abs/1904.12847">Hu, Rudin and Seltzer</a> published
the Optimal Sparse Decision Trees algorithm
(<a href="https://github.com/xiyanghu/OSDT">OSDT github here</a>), 
and followup work by <a href="https://arxiv.org/abs/2209.08040"
>Xin, et al</a> relaxes the search to allow users to choose the
most interpretable tree out of a set of near-optimal choices.
(<A href="https://github.com/ubc-systopia/treeFarms">Github here</a>).

<p>
Decision trees are great for human-executable decisionmaking,
but what if we just need the decisions to be human-understandable
without necessarily being human-executable?  Can we do better?

<p>
Here I'll briefly mention a second interpretable ML
approach that is popular in industry.

<h3>Generalized Additive Models</h3>

<p>
<a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">Generalized Additive Models</a>
 (Hastie, et al 1986)</a>
are a type of model that is based on the fact that any multivariate function
can be decomposed into a sum of univariate functions of the input variables \( x_i \):

\[ y = f_1(x_1) + f_2(x_2) + f_3(x_3) + \cdots \]

<p>
(This fact is known as the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov Arnold representation theorem</a>.

<p>
A sum of univariate functions like this can be very interpretable.
For example, suppose you need to predict somebody's income based on the year,
their age, and their level of education.  Rather than using a decision tree
(which is based on discontinous decisions) or a neural network (which is hard
to understand), you could learn univariate functions \( f_{year}(year) \),
\( f_{age}(age) \) and \( f_{education}(education) \), that, when added up,
make a prediction for \( y \):

<img src="images/gam-example-income.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

<p>
As you can see, this kind of model is very interpretable: reading individual
function can directly give you the intuition about how changing one variable
will affect the output.

<h3>Classic Approach: Backfitting Splines</h3>

<p>
GAMs were devised by <a href="https://pdodds.w3.uvm.edu/files/papers/others/1986/hastie1986a.pdf"
>Hastie and Tibshirani</a> in 1986, building on a 1985 algorittm by <a href="https://www.jstor.org/stable/2288477">Breiman and Friedman</a> to compute such models.

<p>
The 1985 Brieman paper describes the <a href="https://en.wikipedia.org/wiki/Backfitting_algorithm"
>Backfitting algorithm</a> for finding an additive model by beginning with a naive set of \( f_i \),
and then iteratively improving them by distributing the residual error to each function.

<p>
Typically the individual functions are constrained to be smooth splines (and sometimes
other constraints are imposed, such as convexity or monotonicitiy of the individual \( f_i \)
components). This both helps avoid overfitting, helps ensure that the resulting model
is more interpretable.

<p>
<a href="https://colab.research.google.com/github/davidbau/sidn-handbook/blob/main/public/history/colab/tour_of_pygam.ipynb"
>This colab notebook, by the authors of pyGAM</a> demonstrates the pyGAM
package, which applies backfitting and illustrates and the concepts of GAMs very nicely.

<h3>Current Research: Neural Additive Models and KANs</h3>

<p>
Recently, <a href="https://arxiv.org/abs/2004.13912">Agrawal, et al</a>
(on a team with Rich Caurana and Geoff Hinton) proposed <em>Neural Additive Models</em>
which are a type of GAM that uses neural networks instead of spline-smoothed
functions for the \( f_i \).

<p>
This year, <a href="https://arxiv.org/abs/2404.19756">Liu, et al</a>
(on a team with Max Tegmark) proposed <em>Kolmogorov-Arnold Networks</em> (KANs),
that take the opposite tack: instead of using neural networks to create GAMs,
they use spline-based GAMs as a drop-in for neural network MLPs, claiming
better interpretability and good scaling.

<p>
In lower-dimensional settings where input variables have understandable meanings,
both decision trees and GAMs are still used in practice to achieve debuggable,
interpretable machine learned models. It is worth understanding both the classic
approaches and modern research proposals around both these approaches, because
they exemplify what it means for a learned system to be interpretable by
humans.

<p>
However, as data volumes grew in the late 1990s with the growth of the internet
and the digitization of society, it became clear that these early interpretable
machine learning approaches did not scale as well as some more opaque methods.
For example, in natural language modeling, advocates of decision trees
(see <a href="https://www.sciencedirect.com/science/article/pii/S0167639398000181"
>Potamianos</a>) were reporting negative results compared to ngram models.

<h2>Approach 2: Post-Hoc Interpretation</h2>

<p>
Opaque ML models require a different approach to interpretability.
When <a href="https://papers.baulab.info/papers/Rumelhart-1986.pdf"
>Rosenblatt, Hinton, and Williams</a> reopened the door
to deep neural networks in their seminal backpropagation paper in 1986,
they did not frame their work as "machine learning," because their
learning algorithm was not designed to be understandable by
programmers. Rather, they were hunting for a plausible computational
model of the human brain, and they asked the scientific question
of whether this model could learn reasonable algorithmic solutions
from training data alone.

<p>
For example, they examined the weights of a model trained to classify
symmetric versus nonsymmetric rows of pixels, and they concluded
it was successful because it learned an understandable, correct
algorithm that they could examine and comprehend after-the-fact.

<img src="images/rumelhart-figure.jpg" style="max-width:30%; width:300px;" class="mx-auto d-block">

<p>
The field of interpretable machine learning underwent an earthquake in 2012 when
<a href="https://papers.baulab.info/papers/Krizhevsky-2012.pdf">Krizhevsky et al</a>
published the AlexNet neural network that smashed the competitors in the ImageNet
object classification challenge.  At that moment, the industrial-strength
machine learning community collided with the biologically-motivated
neural network scientists, and they came to the realization that big
performance gains could be achieved if we were willing to use models
like deep neural networks that have many layers of internal
variables that are not designed to be interperetable.

<p>
Yet in the move from intrinsically interpretable machine learning do
deep neural networks, the type of <em>post-hoc</em> interpretabilility
methods of the psychologists became more urgent. Yet this type of
exploratory, open-ended approach to understanding computer systems
was foreign to the way computer scientists typically approached
problems.  How post-hoc interpretability began to develop brings
us to the second part of the history, which we will cover in the
next chapter of the handbook.


</main>
</div>
</div>
</body>
</html>

