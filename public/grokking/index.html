<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/stages/index.html">Transformer Stages</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Grokking</h1>

<div<h5>October 28, 2024 • <em>Sri Harsha and Nikhil</em></h5>

<h2>Introduction</h2>

<p>
Grokking is a fascinating phenomenon where a model, after a
period of apparent stagnation, suddenly experiences a rapid and significant
improvement in performance. This abrupt transition is like <b>epiphany</b>
moment, where the model gains a deep understanding of its task, similar to a
human's moment of clarity after grappling with a complex concept. The term
"grok" originates from <a href="https://en.wikipedia.org/wiki/Stranger_in_a_Strange_Land">Robert Heinlein's</a> science fiction, meaning to understand
something fully and deeply. Grokking challenges conventional expectations of
gradual learning, suggesting an alternative dynamic where models might initially
show little progress before suddenly achieving generalization.

<p>
The three papers that will be explored in this page, the first beeing <a href="https://openreview.net/pdf?id=9XFSbDPmdW">Nanda et al 2023</a>,
Progress measures for grokking via mechanistic interpretability which shows grokking
phenomenon on small transformers trained on modular addition tasks. The second paper <a href="https://arxiv.org/pdf/2305.18741">Murty et al 2023</a>,
is about Grokking of Hierarchical Structure in Vanilla Transformers which explores hierarchically
generalization. The third <a href="hhttps://arxiv.org/pdf/2308.09543">Y. Hu et all 2024</a> Delays, Detours, and Forks in the Road:
Latent State Models of Training Dynamics, explores how randomness in data order and initialization
impacts model training dynamics and outcomes.


<h2>Emergent behaviours</h2>
<p>
Emergent behaviors in machine learning models often arise unexpectedly when models
are scaled up, leading to new capabilities such as in-context learning and
chain-of-thought prompting. However, these behaviors also present risks, including
overfitting and unintended consequences in real-world applications. For instance,
<a href="https://arxiv.org/pdf/2201.03544">YPan et al</a>. discuss the risks associated with recommender systems, in case of YouTube
since engineers couldn’t really measure the SWB(Subjective well being) they use other
metrics to measure and optimize  like click-through rates or watch-time. These objectives
don’t make a good estimate of SWB so this led YouTube to overemphasize watch-time and harm
user satisfaction and also recommended extreme political (controversial) content to users.

<p>
The emergence of these behaviors is surprising to researchers because they appear
suddenly and are not easily predictable based on traditional metrics.
Ganguli et al<b>link paper</b>. highlight the paradox that while scaling laws predict performance
improvements, the specific new capabilities that emerge are unpredictable. In fact there
could still be unknow capabilities which are not triggered yet or discovered.
Barak et al. further note that sudden phase changes can occur even without
changes in data size, emphasizing the need for metrics that can detect these
transitions before they happen. Understanding these emergent behaviors requires
novel approaches beyond conventional statistical methods, as they can have
significant implications for both model performance and societal impact.



<h2>Modular addition experiment</h2>
<img src="images/paper1_img1.png" style="max-width:80%; width:600;" class="mx-auto d-block">

<p>
In this experiment they study modular addition, where a model takes inputs
a, b ∈ {0, . . . , P −1} for some prime P and predicts their sum c mod P. Small transformers trained
with weight decay on this task consistently exhibit grokking. They reverse engineered the weights of these transformers
and find that they perform this task by mapping the inputs onto a circle and performing addition
on the circle. Specifically, we show that the embedding matrix maps the inputs a, b to sines
and cosines at a sparse set of key frequencies wk. The attention and MLP layers then combine
these using trigonometric identities to compute the sine and cosine of wk(a + b), and the
output matrices shift and combine these frequencies.
<img src="images/paper1_img2.png" style="max-width:80%; width:600;" class="mx-auto d-block">
<p>
They found four lines of evidence to differentiate the phases of grokking.

<p>
1. Network weights exhibit a periodic structure. When a Fourier transformation is  applied many components are sparse and supported by few key frequencies.
<p>
2. Neuron-logit WL which is the last learnable param matrix. Which transforms hidden activations into logits.
This can be well approximated using sinusoidal functions of key frequencies. MLP activations are projected on
to these sinusoidal functions produce trigonometric identities from the neurons.
Unembedding matrix WL has only a rank of 10 where each direction corresponds to either cosine or sine of only 5
key frequencies. Projecting MLP activations to WL only produces multiples of cos (wk(a + b)) and sin (wk(a + b))
where a and b are the inputs. Hence we can safely say that the sum is not computed in MLP.
<p>
3. The MLP and attention heads can be approximated well using a 2nd degree polynomials of trigonometric functions of a single frequency.
Attention heads and most neurons are well approximated by 2nd degree polynomials of sins and cosines of a single frequency.
The corresponding direction in WL also contains the same frequency. Hence model computations are localized across all the frequencies.
<p>
4. Ablating key frequencies reduces model performance but the other 95% has improves the performance. Ablating various components and
replacing them with Fourier multiplication algorithm does not harm the performance sometimes improves it.
This way the interpretability is faithful. 

<h2>Progress metrics for grokking via mech-interp</h2>
<p>
The paper introduces two progress metrics which improve prior to and when grokking occurs.
<p>
- Restricted loss: ablating every non key frequency.
<p>
- excluded loss: ablating key frequencies


</main>
</div>
</div>
</body>
</html>
