<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/stages/index.html">Transformer Stages</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Grokking</h1>

<div<h5>October 28, 2024 • <em>Sri Harsha and Nikhil</em></h5>

<h2>Introduction</h2>

<p>
Grokking is a fascinating phenomenon where a model, after a
period of apparent stagnation, suddenly experiences a rapid and significant
improvement in performance. This abrupt transition is like <b>epiphany</b>
moment, where the model gains a deep understanding of its task, similar to a
human's moment of clarity after grappling with a complex concept. The term
"grok" originates from <a href="https://en.wikipedia.org/wiki/Stranger_in_a_Strange_Land">Robert Heinlein's</a> science fiction, meaning to understand
something fully and deeply. Grokking challenges conventional expectations of
gradual learning, suggesting an alternative dynamic where models might initially
show little progress before suddenly achieving generalization.

<p>
The three papers that will be explored in this page, the first beeing <a href="https://openreview.net/pdf?id=9XFSbDPmdW">Nanda et al 2023</a>,
Progress measures for grokking via mechanistic interpretability which shows grokking
phenomenon on small transformers trained on modular addition tasks. The second paper <a href="https://arxiv.org/pdf/2305.18741">Murty et al 2023</a>,
is about Grokking of Hierarchical Structure in Vanilla Transformers which explores hierarchically
generalization. The third <a href="hhttps://arxiv.org/pdf/2308.09543">Y. Hu et all 2024</a> Delays, Detours, and Forks in the Road:
Latent State Models of Training Dynamics, explores how randomness in data order and initialization
impacts model training dynamics and outcomes.

<h2>Authors</h2>
<h4>Grokking mech-intrep</h4>
<p>
<b>Neel Nanda</b> runs the Google DeepMind mechanistic interpretability team.
<p>
<b>Lawrence Chan</b> is a PhD at UC Berkeley advised by Anca Dragan and Stuart Russell
<p>
<b>Tom Lieberum</b> is a Research Engineer at DeepMind.
<p>
<b>Jacob Steinhardt</b> Assistant Professor at Department of Statistics, UC Berkeley
<h2>Emergent behaviours</h2>
<p>
Emergent behaviors in machine learning models often arise unexpectedly when models
are scaled up, leading to new capabilities such as in-context learning and
chain-of-thought prompting. However, these behaviors also present risks, including
overfitting and unintended consequences in real-world applications. For instance,
<a href="https://arxiv.org/pdf/2201.03544">YPan et al</a>. discuss the risks associated with recommender systems, in case of YouTube
since engineers couldn’t really measure the SWB(Subjective well being) they use other
metrics to measure and optimize  like click-through rates or watch-time. These objectives
don’t make a good estimate of SWB so this led YouTube to overemphasize watch-time and harm
user satisfaction and also recommended extreme political (controversial) content to users.

<p>
The emergence of these behaviors is surprising to researchers because they appear
suddenly and are not easily predictable based on traditional metrics.
<a href="https://arxiv.org/pdf/2202.07785">Ganguli et al</a>. highlight the paradox that while scaling laws predict performance
improvements, the specific new capabilities that emerge are unpredictable. In fact there
could still be unknow capabilities which are not triggered yet or discovered.
Barak et al. further note that sudden phase changes can occur even without
changes in data size, emphasizing the need for metrics that can detect these
transitions before they happen. Understanding these emergent behaviors requires
novel approaches beyond conventional statistical methods, as they can have
significant implications for both model performance and societal impact.

<p>
Alpha zero learns a lot of human chess concepts between 10 k to 30k training steps. Reinvents opening theories between 25k to 60k.

<h2>Modular addition experiment</h2>
<img src="images/paper1_img1.png" style="max-width:80%; width:600;" class="mx-auto d-block">

<p>
In this experiment they study modular addition, where a model takes inputs
a, b ∈ {0, . . . , P −1} for some prime P and predicts their sum c mod P. Small transformers trained
with weight decay on this task consistently exhibit grokking. They reverse engineered the weights of these transformers
and find that they perform this task by mapping the inputs onto a circle and performing addition
on the circle. Specifically, we show that the embedding matrix maps the inputs a, b to sines
and cosines at a sparse set of key frequencies wk. The attention and MLP layers then combine
these using trigonometric identities to compute the sine and cosine of wk(a + b), and the
output matrices shift and combine these frequencies.
<img src="images/paper1_img2.png" style="max-width:80%; width:600;" class="mx-auto d-block">
<p>
They found four lines of evidence to differentiate the phases of grokking.

<p>
1. Network weights exhibit a periodic structure. When a Fourier transformation is  applied many components are sparse and supported by few key frequencies.
<p>
2. Neuron-logit WL which is the last learnable param matrix which transforms hidden activations into logits.
This can be well approximated using sinusoidal functions of key frequencies. MLP activations are projected on
to these sinusoidal functions produce trigonometric identities from the neurons.
Unembedding matrix WL has only a rank of 10 where each direction corresponds to either cosine or sine of only 5
key frequencies. Projecting MLP activations to WL only produces multiples of cos (wk(a + b)) and sin (wk(a + b))
where a and b are the inputs. Hence we can safely say that the sum is not computed in MLP.
<p>
3. The MLP and attention heads can be approximated well using a 2nd degree polynomials of trigonometric functions of a single frequency.
Attention heads and most neurons are well approximated by 2nd degree polynomials of sins and cosines of a single frequency.
The corresponding direction in WL also contains the same frequency. Hence model computations are localized across all the frequencies.
<p>
4. Ablating key frequencies reduces model performance but the other 95% has improves the performance. Ablating various components and
replacing them with Fourier multiplication algorithm does not harm the performance sometimes improves it.
This way the interpretability is faithful.

<img src="images/paper1_img4.png" style="max-width:80%; width:600;" class="mx-auto d-block">
The paper introduces two progress metrics which improve prior to and when grokking occurs.
<p>
- Restricted loss: ablating every non key frequency.
<p>
- excluded loss: ablating key frequencies

<h2>Phase changes</h2>

<img src="images/paper1_img3.png" style="max-width:80%; width:600;" class="mx-auto d-block">

<p>
The experiment shows three phases in the training. <b>Memorization</b> of the training data, <b>circuit formation</b>, where the network
learns a mechanism that generalizes; and <b>cleanup</b>, where weight decay removes the memorization components. Surprisingly,
the sudden transition to perfect test accuracy in grokking occurs during cleanup, after the generalizing mechanism is learned.
<p>
Memorization (Epochs 0k–1.4k). A decline of both excluded and train loss, with
test and restricted loss both remaining high and the Gini coefficient staying relatively flat. In other
words, the model memorizes the data, and the frequencies wk used by the final model are unused.
<p>
Circuit formation (Epochs 1.4k–9.4k). In this phase, excluded loss rises, sum of squared weights
falls, restricted loss starts to fall, and test and train loss stay flat. This suggests that the model’s
behavior on the train set transitions smoothly from the memorizing solution to the Fourier multiplication algorithm. The fall in the sum of squared weights suggests that circuit formation likely
happens due to weight decay. Notably, the circuit is formed well before grokking occurs.
<p>
Cleanup (Epochs 9.4k–14k). In this phase, excluded loss plateaus, restricted loss continues to drop,
test loss suddenly drops, and sum of squared weights sharply drops. As the completed Fourier
multiplication circuit both solves the task well and has lower weight than the memorization circuit,
weight decay encourages the network to shed the memorized solution in favor of focusing on the
Fourier multiplication circuit. This is most cleanly shown in the sharp increase in the Gini coefficient
for the matices WE and WL, which shows that the network is becoming sparser in the Fourier basis.

<p>
<a href="https://www.neelnanda.io/grokking-paper">Interative grokking mech-interp</a>


<h2>Structural Grokking</h2>
<p>
The problem addressed in this paper is understanding whether and how vanilla transformer models
(standard transformers without architectural modifications) can learn and generalize hierarchical
structures inherent in human language. This question arises because hierarchical structure, how
smaller units in sentences form larger, nested constituents is critical for human language comprehension
and generalization to new sentences.
<p>
Historically, research has suggested that transformers, due to their sequential and attention-based design,
might lack inductive biases for hierarchical structure, struggling to generalize beyond the training data
to structurally novel sentences. Some studies have argued that transformers only learn shallow patterns
in the data without truly capturing these deeper, hierarchical relationships.

<h2>Structural Grokking</h2>
<p>
Some of the early work on hierarchical generalization(<a href="https://aclanthology.org/2022.findings-acl.106.pdf">Muller et al</a>) shows
evidence that generalization does not occur or poor accuracy on certain datasets.
This paper <a href="https://arxiv.org/pdf/2305.18741">Murty et al 2023</a> argues that this occurs due early stopping
of training. They also claim that, simply by training for longer, mean accuracy across random seeds reaches 80%,
and several seeds achieve near-perfect generalization performance.

<img src="images/paper2_img1.png" style="max-width:100%; width:800;" class="mx-auto d-block">

<h2>Experimentation Structural grokking</h2>
<img src="images/paper2_img2.png" style="max-width:100%; width:800;" class="mx-auto d-block">
<p>
They selected datasets that require hierarchical generalization, where training data can be
explained by both hierarchical and non-hierarchical rules:
<p>
Dyck Language Modeling (Dyck-LM): The language of well-nested brackets with 20 types and
max nesting depth of 10. This task involves sequences of well-nested brackets with
varying types and depths, requiring the model to predict closing brackets based on nested structure.
<p>
Question Formation: Convert English sentences into questions. The task involves
transforming English declarative sentences into questions by moving auxiliary
verbs appropriately, which requires hierarchical manipulation.
<p>
Tense Inflection: Map from sentences and tense markers to appropriately re-inflected sentences.
This task involves inflecting verbs in sentences according to tense markers,
necessitating hierarchical understanding to place verb inflections accurately.
Input: "She is reading a book" with a target tense of "past".
Expected Output: "She was reading a book".

<p>
The models used in the experiment are transformer LMs with {2, 4, 6, 8, 10} layers.
For each depth, we train models with 10 random seeds for 300k (400k for Dyck) steps.
A simple <b>greedy decoding</b> is used from the model during testing.
<img src="images/paper2_img3.png" style="max-width:100%; width:800;" class="mx-auto d-block">

</main>
</div>
</div>
</body>
</html>
