<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The Development of Activation Steering</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Mathjax -->
<script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">The Development of Activation Steering</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">

<h1 class="mt-5">The Development of Activation Steering</h1>

<h5>November 24, 2024 â€¢ <em>Alex Loftus, Rohit Gandikota, Dmitrii Troitskii</em></h5>

<p>
We now examine the emergence of activation steering as a method for controlling large language models. This technique has developed rapidly over the past year, with three key papers establishing its theoretical foundations and practical applications.
</p>

<p>
Our examination focuses on how activation steering evolved from initial insights about model representations to a practical technique for controlling model behavior. We trace this development through three papers that build on each other, each making distinct contributions to our understanding of how to manipulate transformer internal states.
</p>

<p>
The first paper, "Understanding How CodeLLMs (Mis)Predict Types" by Lucchetti and Guha, demonstrates the potential of activation steering through its application to type prediction in code models. The second, "Inference-Time Intervention" by Li et al., formalizes steering techniques and shows their effectiveness for enhancing model truthfulness. The third paper, "Representation Surgery" by Singh et al., provides theoretical foundations by characterizing optimal steering functions and establishing formal guarantees.
</p>

<h2>Understanding How CodeLLMs (Mis)Predict Types</h2>

<p>
Lucchetti and Guha investigate type prediction in code language models, focusing on Python and TypeScript. Their key insight is that models often contain robust mechanisms for tasks that can be accessed through activation steering, even when surface behavior suggests otherwise.
</p>

<p>
The authors show that CodeLLMs' type prediction abilities are sensitive to semantically irrelevant changes in input code. However, they demonstrate that by intervening in model activations during inference, they can recover accurate predictions despite such distractors. Most striking is their finding that steering vectors computed from one programming language transfer effectively to another - suggesting models learn fundamental, language-agnostic representations of programming concepts.
</p>

<h2>Inference-Time Intervention</h2>

<p>
Li et al. develop a systematic framework for steering model behavior by identifying and manipulating "truth-correlated" directions in activation space. Their work introduces key techniques that have become standard in activation steering:
</p>

<ul>
<li>Using linear probing to identify relevant attention heads</li>
<li>Computing steering vectors that capture desired transformations</li>
<li>Applying targeted interventions during inference</li>
</ul>

<p>
A crucial innovation in their work is focusing interventions on specific attention heads rather than the full residual stream. This allows for more precise control while minimizing disruption to other model capabilities.
</p>

<h2>Representation Surgery</h2>

<p>
Singh et al. provide theoretical foundations for activation steering by characterizing optimal steering functions under different constraints. They prove that in many cases, minimal intervention can be achieved through simple affine transformations of activations.
</p>

<p>
Their work establishes formal guarantees about the effects of steering and introduces methods for balancing competing objectives like maintaining model capabilities while steering behavior. The paper also provides practical guidance about where in a network to apply steering interventions for maximum effect with minimal disruption.
</p>

<h2>Discussion Questions</h2>

<ul>
<li>How do the theoretical guarantees from Representation Surgery relate to the empirical findings about language transfer in the CodeLLM paper?</li>
<li>What are the tradeoffs between targeting specific attention heads versus intervening in the full residual stream?</li>
<li>How might these techniques generalize beyond code and truthfulness to other aspects of model behavior?</li>
<li>What are the limitations of current steering methods and what theoretical advances might help overcome them?</li>
</ul>

</main>
</div>
</div>
</body>
</html>