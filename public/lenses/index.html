<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/lenses/index.html">Lenses</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Lenses</h1>


<h5>October 01, 2024 â€¢ <em>Prajnan Goswami, Ritik Bompilwar</em></h5>

<p>
  The launch of GPT-3 by <a href="https://arxiv.org/abs/2005.14165">Brown et al. (2020)</a> marked 
  a turning point, drawing widespread attention to the potential of Large Language Models (LLMs) 
  and their applications such as ChatGPT, Copilot, etc. In parallel, diffusion models emerged as a 
  powerful tool for synthesizing high-fidelity images as demonstrated by 
  <a href="https://arxiv.org/abs/2006.11239">Ho et al.(2020)</a> and 
  <a href="https://arxiv.org/abs/2112.10752"> Rombach et al. (2021)</a>. 
  These new generative AI approaches led to a paradigm shift in scaling up models 
  and data to extract more performance. 

<h2>Scaling Trends in the Evolution of Generative Models (In a Nutshell) </h2>  
<p>
  A study by <a href="https://arxiv.org/abs/2001.08361"> Kaplan et al.</a> highlights the significance of 
  model size, dataset scale, and compute used in training. Their analysis indicates that larger models 
  will continue to perform better. In the table below, we can clearly observe 
  how these models have scaled up over time: 
  <table>
    <thead>
      <tr>
        <th>Year</th>
        <th>Model</th>
        <th>Parameters</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2019</td>
        <td><a href="https://cdn.openai.com/better-language-models
          /language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a></td>
        <td>1.5 billion</td>
      </tr>
      <tr>
        <td>2020</td>
        <td><a href="https://arxiv.org/abs/2005.14165">GPT-3</a></td>
        <td>175 billion</td>
      </tr>
      <tr>
        <td>2020</td>
        <td><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></td>
        <td>256 million</td>
      </tr>
      <tr>
        <td>2021</td>
        <td><a href="https://arxiv.org/abs/2006.11239">Stable Diffusion</a></td>
        <td>1 billion</td>
      </tr>
      <tr>
        <td>2023</td>
        <td><a href="https://arxiv.org/abs/2307.01952">Stable Diffusion XL</a></td>
        <td>3.5 billion</td>
      </tr>
      <tr>
        <td>2024</td>
        <td><a href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1</a></td>
        <td>405 billion</td>
      </tr>
    </tbody>
  </table>

  <!-- <ul>
    <li>
    2019, <a href="https://cdn.openai.com/better-language-models
    /language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, <b>1.5</b> billion parameters.
    </li>
    <li>
    2020, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, <b>175</b> billion parameters.
    </li>
    <li>
      2020, <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>, <b>256</b> million parameters.
    </li>
    <li>
      2021, <a href="https://arxiv.org/abs/2006.11239">Stable Diffusion</a>, <b>1</b> billion parameters.
    </li>
    <li>
      2023, <a href="https://arxiv.org/abs/2307.01952">Stable Diffusion XL</a>, <b>3.5</b> billion parameters.
    </li>
    <li>
    2024, <a href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1</a>, <b>405</b> billion parameters.
    </li>
  </ul> -->
</p>  

<h2>Looking Through the Lens of Interpretability</h2>

<p> Although scaling up these approaches has led to significant capabilities 
  and performance improvements, understanding the hidden representations 
  and interpret how the model progresses toward generating its final output 
  through all the intermediate layers remains a challenge. 
  
<p>
  For example, in the paper <i>'Locating and Editing Factual Associations 
  in GPT'</i>, <a href='https://arxiv.org/abs/2202.05262'>Meng et al.</a> 
  demonstrates that when a harmful context is provided to a GPT model, 
  the model <b><i>knows</i></b> the correct answer but imitates and <b><i>states</i></b> the wrong one". 
  Hence, in such scenarios it is critical to identify the components in these GPT-like models
  that causes incorrect  imitations to emerge over the course of the model's processing
  (<a href="https://arxiv.org/abs/2307.09476">Halawi et al., Overthinking the Truth</a>).


<p>
  Now what if we apply linear probing from the 
  <a href="/probing/">previous chapter</a> to a Large Language Model(LLM). 
  The process would involve training a separate probe for each layer. 
  Additionally, each probe would require a predefined set of labels 
  (e.g., sentiment, part-of-speech tags etc.) to evaluate specific 
  types of information the model might encode. In other words, the 
  results may not reflect the model's internal working for the original task
  it was trained on.
  
<p>
  To address these challenges, we need a mechanism to directly look at (ðŸ‘€ ðŸ”Ž) the hidden
  representations of a large scale generative AI model without any external probes.

<p>
  This chapter will focus on some of these technqiues which offer a clearer view of 
  how these large-scale model processes and represents information at each layer.


<h3>Visualizing GPT through a Lens ðŸ”Ž</h3>

<p>
  The first attempt on how to interpret the internal workings of a 
  GPT-like model was introduced in an anonymous blog post 
  <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">Interpreting GPT: the logit lens â€” LessWrong.</a> 
  The idea was fairly straightforward. In large language models(LLMs), predictions are formed in a series 
  of steps across multiple Transformer layers. The Logit Lens "peeks ðŸ‘€" at these 
  intermediate stages by taking the intermediate outputs at each layer and projecting 
  them directly into the vocabulary space. In other words, the intermediate output from 
  different layers is passed through the same output head that the final layer uses to 
  predict the next token as shown in the figure below. 
  <figure id="figure1">
    <img src="images/logit_vs_tuned_lens_fig1.png" style="max-width:80%; width:800px;" class="mx-auto d-block">
  </figure> 

  <p>
    What makes this approach special is that, rather then relying on external probes, Logit lens uses 
    its own prediction head to interpret the hidden representations. By doing so, we can easily track 
    the evolution of the model's understanding at every processing stage. That being said, this first
    attemt had few limitations.
  </p>
  <p>
    Belrose et al. highlighted three specific limitations of Logit Lens in their paper 
    <a href="https://arxiv.org/abs/2303.08112">Eliciting Latent Predictions from 
    Transformers with the Tuned Lens</a>.  

    <ul>
      <li>
        Logit Lens didnâ€™t work well for other models such as 
        <a href="https://arxiv.org/abs/2211.05100">BLOOM (Scao et al.)</a> and
        <a href="https://arxiv.org/abs/2205.01068">OPT 125M (Zhang et al.)</a>.
      </li>
      <li>
        It is biased towards some vocabulary until the final layer of GPT. 
        In simple terms, this means that the output of the intervention layer
        is skewed towards certain words when compared to the final output of the 
        GPT model itself.
        
        <p>
        <img src="images/logit_lens_bias.png" style="max-width:80%; width:600px;" class="mx-auto d-block">

        <p>
        The x-axis represents the different layers of the neural network, and 
        the y-axis represents the difference (in bits) between the output of 
        the method and the final output of the model.
      </li>
      <li>
        And finally Logit Lens is prone to representational drift. What it really
        means is that the hidden representations at the intervention layer does not align 
        with the expected input representations of the final layer. 
      </li>
    </ul>
  </p>

  In light of this, <a href="https://arxiv.org/abs/2303.08112">Belrose et al.</a> introduced 
  a small modification to address the limitations of Logit Lens. In simple terms, the idea
  is to introduce a <b><i>translator</i></b> that learns to map the hidden state at an intermediate
  layer to the expected input of the final layer as show in <a href="#figure1">Figure 1</a>. This  
  <b><i>translator</i></b> is indeed a <b>prob</b> which requires training. The key difference
  with probing approaches from the <a href="/probing/">previous chapter</a> is that the 
  <b><i>translator</i></b> probs learn a mapping from the GPT model's final output itself
  instead of a pre-determined set of labels, i.e, the model's final output distribution
  are used as <i>soft labels</i>. As a result, the <b><i>translator</i></b> probs are not
  incetivized to learn additional information unlike previous probing approaches.
  
  <!--We refer readers to <i>Section 3</i> of the paper
  <a href="https://arxiv.org/abs/2303.08112">Eliciting Latent Predictions from 
  Transformers with the Tuned Lens</a> for the implementation details.-->

  <!--<ul>
    <li>
      Formally, the <b><i>translator</i></b> is an affine transformation and can be written as:

      \[ TunedLens_{l}(h_{l}) = LogitLens(A_{l}) \]
      
      ,where \(A_l\), \(h_l\) and \(b_l\) represents a basis matrix, 
      the GPT's hidden state at layer (\l\) and a bias term respectively.
    </li>
    <li>
      The authors train these parameters by minimizing the KL divergence between
      the <b>Tuned Logits</b> and the <b>final layer</b> logits. Intuitively,
      this can be viewed as <b>minimizing the distance</b> between the logit layers 
      and the final layer by using the the <b>final layer's output distribution</b> 
      as a <b>soft label</b>.
    </li>
    <li>
      These <b><i>translators</i></b> are trained during the pre-training stage 
      of the GPT-like models such as GPT-2, BLOOM and OPT.
    </li>
  </ul>-->


  <p>
    With this adjustment made to the Logit Lens, the Tuned Lens showed 
    more meaningful results starting from the early layers of the model
    as shown in <a href="#figure3">Figure 3</a>.
  
    <figure id="figure3">
      <img src="images/output_comparison_lens.png" style="max-width:80%; width:600px;" class="mx-auto d-block">
    </figure> 
  
<!-- <p>
      <i><strong>Prajnan's opinion :</strong></i> It's worth noting that although Tuned Lens 
      was able draw out meaningful results in the early layers, it requires
      training a <b>translator</b> for each layer of the model. Furthermore,
      the training these <b>translator</b> probes require access to the 
      validation dataset used during LLMs pre-training which may not always be 
      available to analyze existing open-sourced model.
      Whereas Logit Lens can be used on any pre-trained LLMs.
 </p> -->

 <blockquote style="background-color: rgba(179, 174, 174, 0.484);">
  <p>
    <i><strong>Prajnan's opinion:</strong></i> It's worth noting that although Tuned Lens 
    was able to draw out meaningful results in the early layers, it requires
    training a <b>translator</b> for each layer of the model. Furthermore,
    the training these <b>translator</b> probes require access to the 
    validation dataset used during LLMs pre-training which may not always be 
    available to analyze existing open-sourced model.
    Whereas Logit Lens can be used on any pre-trained LLMs.
  </p>
</blockquote>

<h3>Patchscopes ðŸ©º</h3>
<!-- <p>
  <figure id="figure4">
    <img src="images/patchscope.png" style="max-width:80%; width:600px;" class="mx-auto d-block">
  </figure> 
</p> -->

<p>
  <figure id="figure4">
    <img src="images/Patchscope_eg_1.png" style="max-width:80%; width:600px;" class="mx-auto d-block">
  </figure> 
</p>

<p>

  <p>
    Influenced by the efforts of Logit Lens and Tuned Lens <a href="https://arxiv.org/abs/2401.06102">Asma et al.</a> , presented a framework to decode the internal working of LLMs in a human interpretable format. 
    The key idea is to use a target LLM to explain the internal working of the LLM under study. For example,  in the figure above, we are interested in the hidden representation of layer 4 that encodes "UK". This can be interpreted using an inspecting prompt. We replace x in the inspection prompt with the hidden state and continue generating the output in the target LLM. The results of the target LLM indicates that 'layer 4' indeed resolves UK as a country.
    With this approach the authors provides  several applications of the framework to answer a wide range of questions about an LLM's computation. We highly recommend reading the  <a href="https://pair.withgoogle.com/explorables/patchscopes/#appendix-a-a-brief-review-of-transformers">interactive blog</a> to understand the framework's capability.
     
  
  </p>



<h4>Other Lens Approaches for LLMs</h4>

There are several other approaches toward interpretability of Generative Language Models. 

<h3>Diffusion Lens</h3>


<p>
  
Diffusion based text to image models have been a game changer in the field of generative AI. Such text-to-image (T2I) models consist of a 
of two main components: the text encoder and the diffusion model. The text encoder takes in the text input and encodes it into a 
latent space representation, while the diffusion model takes in this latent space representation and generates an image. However the process by which the encoder produces the text representation is still not well understood.
</p>

</p>
<img src="images/DiffusionLens1.png" style="max-width:80%; width:800px;" class="mx-auto d-block">
</p>

<p>
<a href="https://aclanthology.org/2024.acl-long.524.pdf">Toker et al. (2024)</a> proposed Diffusion Lens to visualize the intermediate representations
in the text encoder of T2I models. Authors examined the computational process of the text encoder in the Stable Diffusion (<a href="https://arxiv.org/abs/2112.10752">Rombach
et al., 2022</a>) and Deep Floyd (<a href="https://github.com/deep-floyd/IF">StabilityAI, 2023</a>) to analyse the model's capabilty of conceptual combination and it's memory retrieval process.
The process involves:
<ul>
  <li>
    <strong>Layer-wise Visualization:</strong> At each layer of the text encoder, the intermediate outputs are extracted and projected into a human-interpretable space. This projection allows us to observe how textual representations evolve, 
    revealing the gradual construction of semantic meaning.
  </li>
  <li>
    <strong>Conceptual Mapping:</strong> By analyzing the latent representations, the Diffusion Lens identifies how individual concepts from the input prompt are encoded and combined. This mapping highlights the model's ability to understand and synthesize complex instructions.
  </li>
  <li>
    <strong>Memory Retrieval Analysis:</strong> Diffusion Lens assesses how the model retrieves relevant information from its training data. By examining the activation patterns across layers, we can infer the mechanisms behind the model's knowledge retrieval and application.
  </li>
</ul>
<p>

<h4>Key Insights from the Analysis</h4>

<img src="images/DiffuisonLensInights.png" style="max-width:80%; width:800px;" class="mx-auto d-block">


<h5>Conceptual Combination Analysis</h3>
<ul>
  <li>
    Complex prompts require more computation to achieve a faithful representation compared to simpler prompts.
  </li>
  <li>
    Complex representations are built gradually. The early layer represenations are like "bag of concepts", that encode concepts separately or together without capturing their true relationship.
    Subsequent layers encodes the relationship between the concepts more accurately.
  </li>
  <li>
    The order in which the objects during computation is influence by their linear or syntactical precedence in the prompt. 
    SD's CLIP (<a href="https://arxiv.org/abs/2103.00020">Radford et al., 2021</a>) encoder tends to reflect linear precedence, while Deep Floyd's T5 (<a href="https://arxiv.org/abs/1910.10683">Raffel et al., 2019</a>) encoder reflects syntactical precedence.
  </li>
</ul>

<h5>Memory Retrieval Analysis</h3>
<ul>
  <li>
    Common concepts like "Kangaroo" emerge in early layers while less common concepts like "Dik-dik" (an animal) gradually emerge in subsequent layers, with most accurate represenation predominantly appearing in upper layers.
  </li>
  <li>
    Finer details materializes in the later layers.
  </li>
  <li>
    Knowledge retrieval is gradual, unfolds as the computation progresses. This observation contrdicts from prior research (<a href="https://arxiv.org/pdf/2306.00738"></a>Arad et al., 2023</a>) on knowledge encoding in languagae models which suggests that the knowledge is encoded in specific layers.
  </li>
  <li>
    Memory retrieval patterns in Deep Floyd's T5 encoder are different from Stable Diffusion's CLIP encoder. T5's memory retrival exhibits a more incremental pattern compared to CLIP's. 
  </li>
</ul>

<h2>Colab Notebook and other Code Resources</h2>


<h4>Logit Lens</h4>
<p>
The Colab Notebook for the Logit Lens can be found <a href="https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA?usp=sharing">here</a>.
</p>

<h4>PatchScope</h4>
<p>
The official repository for PatchScope can be found <a href="https://github.com/PAIR-code/interpretability/tree/master/patchscopes/code">here</a>. It contains the code for the PatchScope analysis along with some jupyter notebooks.
The interactive website for PatchScope can be found <a href="https://pair.withgoogle.com/explorables/patchscopes//">here</a>.
</p>

<h4>Diffusion Lens</h4>
<p>
The Github Repository for the Diffusion Lens can be found <a href="https://github.com/tokeron/DiffusionLens">here</a>.
</p>





</main>
</div>
</div>
</body>
</html>
