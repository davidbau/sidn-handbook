<!DOCTYPE html>
<html>
<head>
    <title>AI Safety: Bridging Perspectives</title>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@300;400;500&family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Source Sans 3', -apple-system, BlinkMacSystemFont, sans-serif;
            font-size: 15px;
            line-height: 1.6;
            font-weight: 300;
            max-width: 750px;
            margin: 0 auto;
            padding: 20px;
            color: #2d3748;
            background-color: white;
        }
        h1, h2, h3 {
            font-family: 'Merriweather', Georgia, serif;
            color: #1a202c;
            font-weight: 300;
        }
        h1 {
            font-size: 2em;
            line-height: 1.3;
        }
        h2 {
            font-size: 1.4em;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        h3 {
            font-size: 1.1em;
            margin-top: 1.2em;
        }
        .highlight-box, .counter-argument, .existential-perspective,
        .skeptics-perspective, .benchmark-details {
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
            font-size: 0.95em;
        }
        .counter-argument {
            background-color: #fff3cd;
            border: 1px solid #ffeeba;
        }
        .existential-perspective {
            background-color: #ffebeb;
        }
        .skeptics-perspective {
            background-color: #ebffeb;
        }
        .benchmark-details {
            background-color: #f7fafc;
        }
        .author-byline {
            margin: 1em 0 2em 0;
            font-size: 0.9em;
            color: #666;
        }
        .author-name {
            font-weight: 400;
        }
        .figure-caption {
            font-style: italic;
            color: #666;
            font-size: 0.9em;
        }
        p, ul, ol {
            margin-bottom: 1em;
        }
        li {
            margin-bottom: 0.3em;
        }
    </style>
</head>
<body>
    <h1>AI Safety: Opinions and Progress</h1>
    <div style="margin: -10px 0 30px 0;">
        <div style="font-size: 0.95em; color: #555;">
            <p style="margin: 0;">
                By <strong>Rohit Gandikota</strong> and <strong>Jiachen Zhao</strong>
            </p>
            <p style="margin: 5px 0 0 0; font-size: 0.9em;">
                Published on December 1, 2024
            </p>
        </div>
    </div>
    <p>The discussion around AI safety has become increasingly polarized, with two distinct camps emerging:</p>


    <h2>The Great AI Safety Debate</h2>

    <p>The discussion around AI safety has become increasingly polarized, with two distinct camps emerging:</p>

    <div class="existential-perspective">
        <h3>The Existential Risk Perspective</h3>
        <p>Some researchers and experts, including figures like Eliezer Yudkowsky and Stuart Russell, warn about the potential catastrophic risks of advanced AI systems. They argue that as AI systems become more capable, they could pose existential threats to humanity through various mechanisms, from unaligned goals to potential misuse in creating weapons of mass destruction.</p>
    </div>

    <div class="skeptics-perspective">
        <h3>The Skeptics' View</h3>
        <p>On the other side, researchers like Emily M. Bender and Timnit Gebru argue that current AI systems are merely "stochastic parrots" - sophisticated pattern matching systems that predict the next token based on training data, without true understanding or agency. From this perspective, concerns about existential risk are overblown and divert attention from more immediate issues like bias and environmental impact.</p>
    </div>
    <h2><a href="https://arxiv.org/abs/2403.03218">WMDP: : Measuring and Reducing Malicious Use With Unlearning</a></h2>
        <p><strong>First Authors:</strong></p>
        <ul>
        <li><strong>Nathaniel Li</strong> (Center for AI Safety, UC Berkeley) - Research Scientist specializing in machine learning security and safety evaluation metrics</li>
        <li><strong>Alexander Pan</strong> (UC Berkeley, Center for AI Safety) - PhD student working with Prof. Jacob Steinhardt focusing on language model safety and adversarial robustness</li>
    </ul>
    <p><strong>Senior Author:</strong></p>
    <ul>
        <li><strong>Dan Hendrycks</strong> (Center for AI Safety) - Director of CAIS, known for work in AI safety benchmarks, evaluation, and recently for policies</li>
    </ul>
    <p>The work brought together researchers from 23 institutions including MIT, Harvard, Stanford, and Scale AI (one of the larger collaborations in safety research).</p>


    
    <div class="benchmark-details">
        <h3>Inside the WMDP Benchmark</h3>
        <p>The WMDP benchmark consists of 3,668 multiple-choice questions carefully crafted by subject matter experts across three domains. Each question was developed with specific threat models in mind:</p>

        <ul>
            <li><strong>Biosecurity (1,273 questions)</strong>: Covering areas like dual-use virology, bioweapons research, reverse genetics, and viral vector research. The questions assess knowledge that could enable the development or enhancement of biological threats.</li>

            <li><strong>Cybersecurity (1,987 questions)</strong>: Testing knowledge across the full attack chain - from reconnaissance and weaponization to exploitation and post-exploitation. These questions evaluate a model's capability to assist in cyber attacks.</li>

            <li><strong>Chemical Security (408 questions)</strong>: Examining knowledge about synthesis, procurement, and deployment of chemical agents. This section focuses on identifying capabilities that could enable chemical weapons development.</li>
        </ul>

        <p>Importantly, the benchmark underwent rigorous filtering to remove sensitive or export-controlled information, ensuring it can't serve as a direct guide for malicious actors. The entire dataset cost over $200,000 to develop and involved extensive consultation with technical experts in each domain.</p>
    </div>

    <div class="figure-container">
        <img src="images/wmdp_overview.png" style="max-width:100%; width:1200px; alt="WMDP Overview">
        <p class="figure-caption">Figure 1: Overview of the WMDP Benchmark's three main components: biosecurity, cybersecurity, and chemical security evaluation</p>
    </div>

    <p>The WMDP benchmark offers a pragmatic middle ground in this debate. Instead of focusing on abstract risks or dismissing concerns entirely, it addresses specific, concrete risks that could arise from AI systems being misused for developing weapons of mass destruction.</p>

    <div class="counter-argument">
        <strong>Counter Perspective:</strong>
        <p>Creating benchmarks for hazardous capabilities could inadvertently provide a roadmap for malicious actors. However, the authors argue that their careful filtering process mitigates this risk.</p>
    </div>

    <h2>Understanding the Unlearning Approach: RMU</h2>

    <div class="figure-container">
        <img src="images/rmu_method.png" style="max-width:100%; width:1200px; alt="RMU Architecture">
        <p class="figure-caption">Figure 2: Architecture of the Representation Misdirection for Unlearning (RMU) method</p>
    </div>

    <p>The paper introduces RMU (Representation Misdirection for Unlearning), a novel technique for removing specific knowledge from AI models while preserving general capabilities. This addresses a key challenge in AI safety: how to make models safer without significantly degrading their useful capabilities.</p>

    <div class="figure-container">
        <img src="images/rmu_compare.png" style="max-width:100%; width:1200px; alt="Performance Results">
        <p class="figure-caption">Figure 3: Performance comparison before and after applying RMU</p>
    </div>

    <h2>Key Findings and Implications</h2>

    <div class="highlight-box">
        <p class="key-point">Important Results:</p>
        <ul>
            <li>Models can be made significantly safer on specific domains while maintaining general capabilities</li>
            <li>The unlearning process is robust against various attempts to recover the removed knowledge</li>
            <li>The approach scales to large language models</li>
        </ul>
    </div>

    <div class="counter-argument">
        <strong>Critical Considerations:</strong>
        <p>The paper's results show some degradation in related benign knowledge (like basic virology) when removing hazardous information. This raises questions about the true separability of harmful and beneficial knowledge.</p>
    </div>

    <h2>Broader Implications for AI Safety</h2>

    <p>WMDP represents a new approach to AI safety that bridges theoretical concerns with practical solutions. It demonstrates that:</p>
    <ol>
        <li>Specific safety concerns can be measured and addressed systematically</li>
        <li>Safety improvements don't necessarily require sacrificing all related capabilities</li>
        <li>A middle ground exists between extreme positions in the AI safety debate</li>
    </ol>

    <div class="highlight-box">
        <p>The path forward in AI safety likely involves combining multiple approaches: technical solutions like WMDP, policy frameworks, and ethical guidelines.</p>
    </div>

    <h2>Conclusion</h2>

    <p>While the debate about AI safety continues, WMDP shows that concrete progress is possible. By focusing on specific, measurable risks while acknowledging the complexity of the challenge, we can work towards safer AI systems without falling into either excessive alarm or complacency.</p>

    <div style="margin-top: 30px; border-top: 2px solid #eee; padding-top: 20px;">
        <h2>Critical Questions About WMDP and Unlearning</h2>

        <div style="background-color: #f5f5f5; padding: 20px; border-radius: 8px; margin: 20px 0;">
            <h3>Is Gibberish Really Safety?</h3>
            <p>The paper demonstrates that after unlearning, models output gibberish when asked about hazardous topics. While this prevents coherent harmful responses, it raises several important questions:</p>

            <ul style="margin-top: 15px;">
                <li>Could adversaries distinguish between genuinely unknown topics and artificially unlearned ones?</li>
                <li>Does outputting gibberish make it obvious which topics have been intentionally removed?</li>
                <li>Might this "signature" of unlearning actually guide malicious actors toward sensitive areas?</li>
                <li>Is it really an effective language model if it speaks gibberish ?</li>
            </ul>
        </div>


        <p style="font-style: italic; margin-top: 20px;">The path to truly safe AI systems likely requires a combination of approaches, with unlearning being just one piece of a much larger puzzle.</p>
    </div>

    Here is a small snippet of RMU code for easy understanding:
    <div class="code-example" style="background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 15px 0;">
        <pre><code>
# Example implementation of RMU unlearning
def rmu_unlearning(model, forget_data, retain_data, c=6.5, alpha=1200):
    # Get random unit vector for steering direction
    u = get_random_unit_vector()
    
    # Calculate forget loss to increase activation norms on harmful data
    forget_loss = 0
    for x in forget_data:
        activations = model.get_layer_activations(x)
        forget_loss += torch.norm(activations - c * u) ** 2
        
    # Calculate retain loss to preserve general capabilities
    retain_loss = 0 
    for x in retain_data:
        activations = model.get_layer_activations(x)
        retain_loss += torch.norm(activations - 
                   model_orig.get_layer_activations(x)) ** 2
        
    return forget_loss + alpha * retain_loss
        </code></pre>
    </div>

<h2><a href="https://arxiv.org/abs/2403.05030">Defending Against Unforeseen Failure Modes
 with Latent Adversarial Training</a></h2>
    <p><strong>Lead Authors:</strong></p>
    <ul>
        <li><strong>Stephen Casper</strong> (MIT CSAIL) - PhD researcher at Harvard very interested in AI robustness and safety </li>
        <li><strong>Lennart Schulze</strong> (Columbia University) - PhD student with Prof. Carl Vonderick focusing on world model representation emergence in deep models</li>
    </ul>
    <p><strong>Senior Author:</strong></p>
    <ul>
        <li><strong>Dylan Hadfield-Menell</strong> (MIT CSAIL) - Leading a lab in AI alignment</li>
    </ul>

    <div class="highlight-box">
      <h3>Motivation</h3>
<p>
    Despite the relentless efforts of developers, even their most rigorous diagnostics and exhaustive debugging can't always prevent AI systems from exhibiting dangerous, unintended behaviors.
    The sheer vastness of the attack surface makes it a Herculean task to identify every possible input that could provoke these malicious actions.
    Red-teaming and adversarial training (AT) are often deployed in a valiant effort to bolster robustness, yet these methods often falter when confronted with failure modes that diverge from the attacks seen during training.
</p>
    </div>

    <h3>Latent Adversarial Training</h3>

    <div class="counter-argument">
      <b>Adversarial training without examples that elicit failure</b>
        <p> Across the latents, a model gradually develops more compressed,
 abstract, and structured representations of the concepts it uses to process information. This makes it
 possible for latent space attacks to activate neural circuitry that elicits failures without
 requiring inputs that trigger them</p>
    </div>

            <div class="figure-container">
        <img src="images/lat-motive.png" style="max-width:100%; width:1200px; alt="LAT motivation">
        <p class="figure-caption">Figure 4:  The motivation for LAT is based on
 how models develop more compressed, abstract, and structured representations across their latents.
 Wehypothesize that many failures that are difficult to elicit from the input space may be easier to
 elicit from the latent space.</p>
    </div>

        <h3>Method</h3>

    <div class="figure-container">
        <img src="images/formula-lat0.png" style="max-width:100%; width:1200px; alt="LAT motivation">
        <p class="figure-caption">
        </p>
    </div>

    <div class="figure-container">
        <img src="images/formula-lat.png" style="max-width:100%; width:1200px; alt="LAT motivation">
        <p class="figure-caption">
        </p>
    </div>

LAT needs to select which layer to attack, i.e., apply the perturbation during training. The choice may differ from case to case and needs to be tuned as a hyper-parameter.


<h3>Experimental Results</h3>

<div class="figure-container">
    <img src="images/eval-lat.png" style="max-width:100%; width:1200px; alt="Evaluation">
    <p class="figure-caption">Figure 5:  Evaluation procedures.</p>
</div>

The experiments compare methods based on
    <ul>
        <li> their performance on clean evaluation data </li>
        <li> their robustness to novel classes of adversarial examples not encountered during training </li>
        <li> their robustness to trojans implanted during pretraining </li>
    </ul>


<div class="figure-container">
    <img src="images/ret-eval-lat.png" style="max-width:100%; width:1200px; alt="Evaluation">
    <p class="figure-caption">Figure 6:  Evaluation results for image classification.</p>
</div>


<div class="paper-comparison" style="background-color: #f7fafc; padding: 20px; margin: 20px 0; border-radius: 5px;">
    <h2>Comparative Analysis of Safety Approaches</h2>

    <h3>Methodological Differences</h3>
    <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
        <tr style="background-color: #edf2f7;">
            <th style="padding: 12px; border: 1px solid #e2e8f0;">Aspect</th>
            <th style="padding: 12px; border: 1px solid #e2e8f0;">WMDP/RMU</th>
            <th style="padding: 12px; border: 1px solid #e2e8f0;">Latent Adversarial Training</th>
        </tr>
        <tr>
            <td style="padding: 12px; border: 1px solid #e2e8f0;"><strong>Primary Goal</strong></td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Remove specific harmful capabilities while preserving general abilities</td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Make models robust against unforeseen failure modes</td>
        </tr>
        <tr>
            <td style="padding: 12px; border: 1px solid #e2e8f0;"><strong>Technical Approach</strong></td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Targeted activation steering with forget/retain loss</td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Adversarial perturbations in latent space</td>
        </tr>
        <tr>
            <td style="padding: 12px; border: 1px solid #e2e8f0;"><strong>Evaluation Method</strong></td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Benchmark-based evaluation (WMDP)</td>
            <td style="padding: 12px; border: 1px solid #e2e8f0;">Robustness against novel attacks and trojans</td>
        </tr>
    </table>

    <h3>Key Trade-offs</h3>
    <ul style="margin-left: 20px;">
        <li><strong>Specificity vs. Generality:</strong> WMDP/RMU targets specific harmful capabilities with precise control, while LAT provides broader robustness against general failure modes.</li>
        <li><strong>Implementation Complexity:</strong> RMU requires careful tuning of forget/retain loss balance, while LAT's approach is more straightforward but may require more computational resources.</li>
        <li><strong>Scalability:</strong> RMU shows strong results on larger models (up to 7B parameters), while LAT's effectiveness has been primarily demonstrated on smaller models.</li>
    </ul>

    <h3>Complementary Strengths</h3>
    <p>These approaches could potentially be combined:</p>
    <ul style="margin-left: 20px;">
        <li>Use WMDP/RMU to remove known harmful capabilities</li>
        <li>Apply LAT to provide additional robustness against unforeseen failure modes</li>
        <li>Leverage both approaches' insights about latent space manipulation for safety</li>
    </ul>

    <div class="insight-box" style="background-color: #ebf8ff; padding: 15px; margin: 20px 0; border-radius: 5px;">
        <p><strong>Key Insight:</strong> While both papers work with model internals, they represent different philosophies in AI safety: WMDP takes a targeted approach to removing specific capabilities, while LAT aims for broader robustness through adversarial training in latent space. This difference highlights the complementary nature of current safety research.</p>
    </div>
</div>


<div class="implementation-resources" style="background-color: #f7fafc; padding: 20px; margin: 20px 0; border-radius: 5px;">
    <h2>Code Resources and Implementation Details</h2>
    
    <h3>Official Repositories</h3>
    <ul>
        <li>WMDP Benchmark & RMU Implementation: <a href="https://wmdp.ai">https://wmdp.ai</a></li>
        <li>Latent Adversarial Training: <a href="https://github.com/thestephencasper/latent_adversarial_training">GitHub Repository</a></li>
    </ul>

    <h3>Interactive Notebooks</h3>
    <h4>WMDP Evaluation</h4>
    <ul>
        <li><a href="https://colab.research.google.com/github/centerforaisafety/wmdp/blob/main/run_rmu_zephyr.ipynb">RMU Implementation Guide</a></li>
    </ul>


    <h3>Usage Notes</h3>
    <ul>
        <li>All implementations use PyTorch and support both GPU and CPU execution</li>
        <li>Code is tested with popular model architectures including Llama, GPT, and BERT variants</li>
        <li>For better documentation and examples, please refer to the respective repositories</li>
    </ul>
</div>

</body>
</html>