<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/formulation/index.html">Formulation</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Formulation</h1>


<h5>October 4, 2024 â€¢ <em>Alex Loftus, Michael Ripa, Dmitrii Troitskii</em></h5>

<p>We now take a break from exploring new particular new methods to zoom out and think: What exactly are the objects we are working with? Can we get specific about exactly what we're doing, and will it lead to new insights and research directions?</p>

<p>Creating a common notation for the mathematical structure of the transformer, and being careful about exactly what we mean when we talk about the residual stream, attention, and feedforward networks gives us a unified mental map that we can use when we think through other papers.</p>

<p>Our central question in this class isn't: 'what are the details of this method'. Instead, it is 'what is the overall structure of the transformer'.</p>

<p>We will focus on three papers. The first,<a href="https://arxiv.org/abs/2405.00208">a primer on transformer language models</a> by Javier Ferrando, is a review paper that serves to create a common notation around techniques we will encounter for the rest of the course. The second, <a href="https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits">a mathematical framework for transformer circuits</a>, was the anthropic post that introduced the idea of the residual stream, \( W_{OV} \) and \( W_{QK} \) circuits, and other fundamental ideas. Like other anthropic posts, it focuses on toy models and intuition. It focuses almost entirely on the attention operation, ignoring feedforward networks for future work. The third, <a href="https://arxiv.org/abs/2209.02535">Analyzing Transformers in Embedding Space</a> by Mor Geva, is the second in a pair of papers that follow up on the anthropic post to extend them to include feedforward networks. This paper, however, focuses on extending the method developed in the first paper to include every weight matrix in the model, avoiding the use of calculations dependent on activations.

<p>The anthropic line of papers in particular inherit from the <a href="https://distill.pub/2020/circuits/">distill circuits thread</a>, which attempted to reverse engineer vision models. The framework paper was the first to switch to language models.

<!-- <h3>scoring rubric</h3>
<p>
  <ul>
    <li>+1: introduction poses the question, links the papers
    <li>+1: authors are identified and described
    <li>+1: background context is described, key previous works cited
    <li>+1: diagrams show the main methods or results visually
    <li>+1: contrast is drawn, identifying differences between the papers
    <li>+1: Code examples are included, demonstrating the methods
    <li>+1: Discussion with your own opinions, questions, or perspective
  </ul>
</p>

<p><em>Introduction</em> that poses the question, links the papers, describes authors and background context
introduction rubric:
<ul>
  <li>+1: crisply describes the problem being solved. Cites and links the main papers.
  <li>+1: authors - identifies the authors, says a few words about who they are, where they worked, what they are known for.
  <li>+1: background - give context. how the problem was preivously tackled, why the problem emerged as an interesting topic in the history. Use the paper's own literature review to identify related older papers. Then cite a few of the key older papers.
</ul>
</p>

<h2>A Need for Formulation</h2>
<p>rubric details:
  <ul>
    <li>+1: diagram - describe methods in the paper visually
    <li>+1: contrast - don't just say what each paper does, but focuses on the differences between them.
    <li>+1: Code - help your readers by giving them concrete code samples to run. Colab notebooks with all the installation steps.
    <li>+1: discussion - go beyond the facts and share your own perspective.
  </ul>
</p>
<p>Example of a <a href="/neurons/">link</a> to the neurons page</p>
<p>Example of a <em>formula</em>: \( h_i \in \mathbb{R} \) and an <em>image</em>
<img src="images/bigdavid.jpg" style="max-width:50%; width:500px;" class="mx-auto d-block">
<p><b>bold text</b>
<p>this is <a href="https://transformer-circuits.pub/2022/toy_model/index.html">an inline link</a></p> -->

<h2><a href="https://arxiv.org/abs/2405.00208">a primer on transformer language models</a></h2>
<p>
This primer was published recently and serves to unify mathematical notation for techniques described in the interpretability literature. We will focus on everything prior to section 3, since the rest of the paper focuses on specific applications. </p>

<p>
The authors are Javier Ferrando, Marta Ruiz Costa-jussa, Gabriele Sarti, and Arianna Bisazza. Javier is at Universitat Politecnica de Catalunya in Barcelona. He spent the summer at Berkeley, I spoke to him a few times while I was in Barcelona this summer. </p> <p>The final author, Marta Ruiz Costa-jussa, is a research scientist at FAIR Meta in Paris; she was a researcher at Ramon y Cajal / ERC at the Universitat Politecnica de Catalunya for seven years prior. She's been working on safety and gender bias in LLMs recently. </p><p>The other authors, Gabriele Sarti and Arianna Bisazza, are in the netherlands (University of Groningen).
</p>

<p>
  There have been a few primers on this topic in the past, but interpretability moves quickly and notation hasn't been particularly standardized, to the best of my knowledge. This paper serves to formalize many of the ideas we will explore in the mor geva paper and in the mathematical framework paper, so we will make sure we understand this notation first. We will then use it to describe the mor geva paper and the mathematical framework paper.
</p>

<h4>Notes</h4>
<p>First: The primer paper is confusing to read for mathematicians because notationally, the vector \( x_i \) is considered a row vector, and consequently, linear transformations on it are right-multiplications.</p>

\[
  \mathbf{x_i} W = \begin{bmatrix}
    x_{1} & x_{2} & \cdots & x_{d}
  \end{bmatrix} 
\begin{bmatrix}
  \leftarrow \mathbf{w}_{1} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w}_{d} \rightarrow
\end{bmatrix}
\]

<p>\( W \) has \( d \) rows and \( m \) columns. You can think of this matrix multiplication as scaling each row by its corresponding weight in \( \mathbf{x}_i \), and then summing vertically along the row axis. The output row vector is in \( m \) dimensions.</p>
<p> Second: There is a mathematical trick that shows we can lightly ignore the layer norm when building intuition. This is because the weights of the affine transformation can be folded into the following linear layer recursively until the end of the residual stream. </p>
<p>This is also true with bias vectors: we can just fold them into the weights and creating a dimension that is always 1.</p>

<h3> Prediction as a sum of component outputs </h3>
<p> <img src="images/primer_prediction.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block"> </p>
<p>We get to logits by passing the last token at the end of the residual stream \(x_n^L\) through the unembedding matrix \( W_u \).
We can think of the output of the residual stream as a sum of the attention head outputs and of the feedforward layers. Writing this out leads to an interesting algebraic manipulation: we can simply move the unembedding matrix \(W_U\) inside the sum. This leads us to the conclusion that, at any moment along the residual stream, we can see what's happening in logit space. This is the idea behind, for instance, the logit lens.
</p>
<p> In practice, attention and feedforward layer are calculated sequentially. The attention operation happens per-head, with each head independently summed into the residual stream, and the feedforward layer is applied independently to each position in the sequence.</p>

<p> Notation is as follows: </p>

<p> <ul>
  <li>Initial embeddings \( \mathbf{X} \in \mathbb{R}^{n \times d} \) are row vectors corresponding to a row of \( W_E \in \mathbb{R}^{\mathcal{V} \times d} \) </li>
  <li> The final layer residual stream state is projected into the vocabulary space via \( W_U \in \mathbb{R}^{d \times \mathcal{V}} \). </li>
  <li>An intermediate layer representation \( \mathbf{x}_i^l \) is the representation for token \( i \) in layer \( l \). \( \mathbf{x} \) is \( \mathbf{x}_i^0 \). </li>
  <li> \( \mathbf{X}^l \in \mathbb{R}^{n \times d} \) represents activations for layer \( l \) </li>
  <li> \( \mathbf{X}_{\leq i}^l \) is the layer \( l \) representation up to position \( i \). </li>
</ul></p>

<h3>Attention Block</h3>
<img src="images/primer_attn_output.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block">
<p>Attention is the communication between residual stream tokens. The outputs of the attention operation are written into the residual stream.</p>

<p>The attention operation at the \( l_{th} \) layer is simply the sum of all attention head outputs at that layer into the residual stream.</p>

<h4>How does attention modify a single token?</h4>
<img src="images/primer_attn.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block">
<p>Zoom into token \( i \). The attention operation for this token at layer \( l \), head \( h \), works as follows:
  <ol>
    <li>Loop through every other token in the sequence \( x_j \) for \( j \leq i \).</li>
    <li>Read each \( x_j \) into a space that can be writeable into the residual stream</li>
    <li>Weight that \( x_j \) by its attention with \( x_i \), and then write it into the residual stream</li>
  </ol>
</p>
<p>
This operation can be thought of as scaling each \( x_j \) by its attention score with \( x_i \) and then transforming it into a space that can be written into the residual stream (We can also think of the transformation as happening first, because of linearity).
</p>
<p>

\[
\sum_{j \leq i} a_{i,j}^{l,h} \begin{bmatrix}
    \mathbf{x}_{j,1} & \cdots & \mathbf{x}_{j,d}
  \end{bmatrix}
\begin{bmatrix}
  \leftarrow \mathbf{w}_{1,OV} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w}_{d, OV} \rightarrow
\end{bmatrix}, W_{OV} \in \mathbb{R}^{d \times d}
\]

</p>
<p>Remember that \( W_{OV} \) is low-rank: maximum rank \( n \). This is because \( W_O \) and \( W_V \) are both \( d_h \times d \). The hyperplane the columns and rows span is independent of the particular activations; there's just a different set-in-stone hyperplane for each \( W_{OV} \).</p>


<h4> How is the attention score calculated? </h4>
<img src="images/primer_a_i.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block">
</p>
<p>The attention vector \(\mathbf{a}_i^{l,h} \) has attention values \( a_{i,j}^{l,h} \). Each are calculated by</p>
  <ol>
    <li> Reading the corresponding \( x_j \) into a space such that its dot product with \( x_i \) is monotonic with its attention score: \( W_{QK}^h x_j^{\top l-1} \) </li>
    <li> Taking the actual dot product of the two to get an attention value, and then normalizing by \( \sqrt{d_k} \) 
    <li> Softmaxing the result to get values between 0 and 1 </li>
    </li>
    
  </ol>

<p>I prefer to think of the attention <i>matrix</i>, which is a nonsymmetric adjacency matrix \( \mathbf{A} \) where the attention between token \( i \) and token \( j \) is the \( (i, j)_{th} \) entry. The vector \( \mathbf{a}_i \), calculated directly above, is the row of this matrix corresponding to token \( i \). We can also think of the individual \( \mathbf{a}_{i,j} \) getting calculated, as below. We build the matrix by adding rows to the matrix on the left, and columns to the matrix on the right.</p>

<p>
  \[
  a_{i,j} \text{ is monotonic with:} \begin{bmatrix}
    \mathbf{x}_{i,1} & \cdots & \mathbf{x}_{i,d}
  \end{bmatrix}

  \begin{bmatrix}
    & &\\
    & W_{QK} &\\ 
    & &\\
  \end{bmatrix}

  \begin{bmatrix}
    \mathbf{x}_{j,1} \\
    \vdots \\
    \mathbf{x}_{j,d}
  \end{bmatrix}, W_{QK} \in \mathbb{R}^{d \times d}

  \]
</p>
<p>Remember that \( W_{QK} \) is low-rank: maximum rank \( n \). This is because \( W_Q \) is \( d \times n \) and vice-versa for \( W_K \). The hyperplane the columns and rows span is independent of the particular activations; there's just a different set-in-stone hyperplane for each \( W_{QK} \).</p>


<h3> Feedforward Block </h3>
<p><img src="images/primer_ffn.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block"></p>
<p>The feedforward block is applied independently to each position in the sequence. It is a two layer neural network with a nonlinearity in between. We generally project into a higher dimensional space (often \( 4d \)) and then back into the residual stream. </p>


<h4> Pattern Detecting </h4>
  <img src="images/primer_ffn_kv.png" style="max-width:60%%; width:60%60%60%60%60%60%60%60%60%60%px;" class="mx-auto d-block">
<p> We can think of the columns of \( W_{in}^l \) acting as pattern detectors over the activations.

Each neuron defines how strongly its corresponding row vector in \( W_{out} \) should be activated. The result is a linear combinationn of row vectors of \( W_{out} \).
</p>


\[
\text{FFN} = g\left( \begin{bmatrix} x_{1} & x_{2} & \cdots & x_{d} \end{bmatrix} 
  \begin{bmatrix}
    \top & & \top \\
    \mathbf{w_u} & \cdots & \mathbf{w_{d_{\text{FFN}}}} \\
    \perp & & \perp
  \end{bmatrix} \right)
 \begin{bmatrix}
  \leftarrow \mathbf{w_u} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w_{d_{\text{FFN}}}} \rightarrow
  \end{bmatrix} \\
\]
\[ =
  \begin{bmatrix} n_{1} & \cdots & n_{d_{\text{FFN}}} \end{bmatrix}   
  \begin{bmatrix}
  \leftarrow \mathbf{w_u} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w_{d_{\text{FFN}}}} \rightarrow
  \end{bmatrix}, W_{in} \in \mathbb{R}^{d \times d_{\text{FFN}}}, W_{out} \in \mathbb{R}^{d_{\text{FFN}} \times d}
\]


<!-- <h3> attention block </h3>
<ul>
 <li> \( \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) = \sum_{j \leq i} a_{i,j}^{l,h} \mathbf{x}_j^{l-1} \mathbf{W}_{OV}^{l,h} \). Importantly here, \( \mathbf{W}_{OV}^{l,h} = \mathbf{W}_O^{l,h} \mathbf{W}_V^{l,h} \), and \( \mathbf{W}_O^{l,h} \in \mathbb{R}^{d \times d_h} \) and \( \mathbf{W}_V^{l,h} \in \mathbb{R}^{d_h \times d} \).
<li> \( \mathbf{a}_i^{l,h} = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_Q^{l,h} (\mathbf{X}_{\leq i}^{l-1} \mathbf{W}_K^{l,h})^\top}{\sqrt{d_k}} \right) = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_{Qk}^{h} \mathbf{X}_{\leq i}^{(l-1)\top}}{\sqrt{d_k}} \right) \)
<li>\( \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) = \sum_{h=1}^H \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) \)</li>
<li> Output of the attention block: \( \mathbf{x}_i^{\text{mid},l} = \mathbf{x}_i^{l-1} + \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) \)</li>
</p>

<h3> MLP block </h3>
<ul>
  <li> \( \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) = g(\mathbf{x}_i^{mid,l} \mathbf{W}_{in}^l) \mathbf{W}_{out}^l \), where \( g(\cdot) \) is a nonlinearity, and \( \mathbf{W}_{in}^l \in \mathbb{R}^{d \times d_{\text{ffn}}} \) and \( \mathbf{W}_{out}^l \in \mathbb{R}^{d_{\text{ffn}} \times d} \).</li> 
  <li> \( \mathbf{x}_i^{\text{out},l} = \mathbf{x}_i^{\text{mid},l} + \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) \)</li>
</ul> -->

<p></p>
<h2>a mathematical framework for transformer circuits</h2>
<h3>Authors</h3>
<p>This paper was authored by a team primarily from Anthropic, a prominent AI research company. The lead authors are:</p>

<ul>
  <li><strong>Nelson Elhage</strong>: A research scientist at Anthropic, known for his work on AI safety and interpretability.</li>
  <li><strong>Neel Nanda</strong>: An independent AI safety researcher, formerly at Anthropic, with a focus on transformer interpretability.</li>
  <li><strong>Catherine Olsson</strong>: A research scientist at Anthropic, specializing in AI alignment and interpretability.</li>
</ul>

<p>Other key contributors from Anthropic include Tom Henighan, Nicholas Joseph, and Ben Mann. The paper also features contributions from a diverse group of researchers in the field of AI, including:</p>

<ul>
  <li>Dario Amodei: Co-founder and Chief Scientist at Anthropic.</li>
  <li>Chris Olah: Known for his work on neural network interpretability, formerly at OpenAI and Google Brain.</li>
  <li>Jared Kaplan: A physicist and AI researcher, known for scaling laws in machine learning.</li>
</ul>

<p>This collaborative effort brings together expertise from various backgrounds in AI research, safety, and interpretability.</p>

<h3>Overview</h3>
<p> This paper also introduces a new notation for transformers. It was published before the primer paper. </p>

<h3>Tensor Products</h3>
<p>
  The authors utilize the notion of a tensor product to describe the operations of transformers. For convenience, we will define the tensor product here. Given two matrices \( A \) and \( B \), their tensor product \( A \otimes B \) is defined as the block matrix:
</p>
<p>
  \[
  A \otimes B = \begin{bmatrix}
    a_{11}B & a_{12}B & \cdots & a_{1n}B \\
    a_{21}B & a_{22}B & \cdots & a_{2n}B \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}B & a_{m2}B & \cdots & a_{mn}B
  \end{bmatrix}
  \]
</p>

<h3>Zero Layer Transformer</h3>
<p> A zero layer transformer is a transformer with no layers. It is simply a product of the unembedding matrix and the embedding matrix. </p>
<p>\[ T = W_u W_E \] </p>
<p> The key intuition here is that this model does not move information across tokens.</p>

<h3>One Layer Attention Only Transformer</h3>

<p>In traditional attention, the output of an attention head is as follows: </p>
<p>1. Compute vector value for each token: \( v_i = W_v x_i \) </p>
<p>2. Compute "result" vector: \( r_i = \sum_{j} a_{i,j} v_j \) </p>
<p>3. Compute output vector for each token: \( h(x)_i = W_O r_i \) </p>

<p> We can reformulate this in a way which allows for some new intuitions using tensor products. </p>
<p> h(x) = (Id \otimes W_O) \cdot (A \otimes Id) \cdot (Id \otimes W_v) \cdot x </p>

<p> This can be rewritten as (by mixed product property): </p>
<p> h(x) = (A \otimes W_O W_v) \cdot x </p>

<p> Here, \( A = \text{softmax} \left( x^T W_{Q}^T W_K x \right) \) </p>

<h4> Intuitions </h4>

<p> 1. Attention heads "mix" information across the residual stream. The attention head output at a particular position \( i \) is a linear combination of the residual stream up to that position. </p>
<p> 2. Asides from softmax, all the operations in this layer are linear. If we were to fix \( A \), we would have a linear transformer. </p>
<p> 3. W_Q and W_K always operate on the residual stream in the same way across different positions (as do \( W_V \) and \(W_O\)). </p>
<p> 4. In a certain sense, attention heads are closed under multiplication: \[ (A^{h_2} \otimes W^{h_2}_{OV}) \cdot (A^{h1} \otimes W^{h_1}_{OV}) = (A^{h_2} A^{h_1} \otimes W^{h_2}_{OV} W^{h_1}_{OV}) \].</p>


<h2>analyzing transformers in embedding space</h2>

<!-- 
<blockquote class="blockquote">
  this is a blockquote
</blockquote>

<p>align latex:

\[ \hat{x} = W^T h = \begin{bmatrix}
\phantom{0} & w_1^T & \phantom{0} \\ \hline
\phantom{0} & w_2^T & \phantom{0} \\ \hline
 & \vdots & \\ \hline
\phantom{0} & w_n^T & \phantom{0}
\end{bmatrix} h
=
\begin{bmatrix}
w_1^T h \\
w_2^T h \\
\vdots \\
w_n^T h
\end{bmatrix}
\] -->

<h2> Code Examples </h2>

A colab notebook exploring these ideas can be found <a href="https://colab.research.google.com/drive/1MrQAZ9WgKPkUwNM_q5bQO07sggOdpG7X?usp=sharing">here</a>.

<h2>Discussion Questions</h2>

<ul>
<li>Why is it that concatenating attention head outputs and then running the result through the output weight matrrix \( W_o \) is the same as running through \( W_{OV} \) inside attention heads? (page 3 of primer, bottom of the page)
<li>second question
</ul>


</main>
</div>
</div>
</body>
</html>

