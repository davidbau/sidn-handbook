<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/autocircuits/index.html">Automatic Circuit Discovery</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Automatic Circuit Discovery</h1>


<div>
  <h5>October 22, 2024 • <em>David Atkinson, Nikhil Prakash</em></h5>

  <p><a href="https://sidn.baulab.info/circuits/">Last week</a>, we discussed three papers which manually found circuits in models.
    This week, we examine a pair of papers which attempt to automate (part of) this process.
    The first, <a href="https://arxiv.org/abs/2304.14997">Towards Automated Circuit Discovery for Mechanistic Interpretability</a> (Conmy et al., 2023), proposes an iterative pruning algorithm based on activation patching.
    The second, <a href="https://arxiv.org/abs/2310.10348">Attribution Patching Outperforms Automated Circuit Discovery</a> (Syed et al., 2023), proposes a faster alternative to activation patching, called attribution patching.
  </p>

  <h2>
    Towards Automated Circuit Discovery for Mechanistic Interpretability
  </h2>
  <p>
    <a href="https://arxiv.org/abs/2304.14997">This paper</a> was published at
    NeurIPS 2023.<br />
    <strong>Arthur Conmy</strong> Independent; now a Research Engineer at Google DeepMind<br />
    <strong>Augustine N. Mavor-Parker</strong> UCL PhD student; now at a startup<br />
    <strong>Aengus Lynch</strong> UCL PhD student<br />
    <strong>Stefan Heimersheim</strong> Institute of Astronomy, University of Cambridge; now at Apollo Research<br />
    <strong>Adrià Garriga-Alonso</strong> FAR AI<br />
    Much of this work was done during Redwood Research's REMIX program.
  </p>
  Automated Circuit DisCovery (ACDC) is an attempt to formalize and automate the process of circuit discovery.
  As you read through last week's papers, you might have noticed regularities in the process by which researchers identified circuits.
  The authors of ACDC frame this as a three step process, which they describe as "The Mechanistic Interpretability Workflow".
  <ol>
    <li>In the first step, researchers identify a (usually simple) task that the model can successfully perform, curate a dataset to elicit the behavior, and choose a metric to measure task success.
      For example, in the IOI paper, the task is completing sentences with missing indirect objects, the dataset is a set of those sentences, and the metric is logit difference.</li>
    <li>Next, researchers must choose which components of the model are candidates for the circuit.
      For the attention mechanism, you might choose, for instance, full attention heads, key, query, and value matrices, or even individual attention head output dimensions.</li>
    <li>Finally, researchers use patching to remove model components that are not part of the circuit and see how that affects the task performance metric.</li>
  </ol>

  <p>In ACDC, the authors present a simple algorithm to automate this third step.</p>

  <h3>The ACDC Algorithm</h3>
  <img src="images/acdc_alg.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">

  <h3>Evaluating ACDC</h3>
  <p>The authors introduce two baselines: <a href="https://arxiv.org/abs/2104.03514">Subnetwork Probing</a> (SP; Cao, Sanh, and Rush 2021) and <a href="https://arxiv.org/abs/1905.10650">Head Importance Score for Pruning</a> (HISP; Michel, Levy, and Neubig, 2019).
    SP optimizes a mask over model components using gradient descent, while HISP uses a first-order Taylor approximation to estimate the effect of replacing a head's output with corrupted inputs.
  </p>
  <p>
    To evaluate the performance of each candidate algorithm, "ground truth circuits" are used.
    These are circuits that other researchers have found using manual patching, as well as a new task, Induction: 
    <!-- <a href="https://openreview. net/forum?id=NpsVSN6o4ul">IOI</a> (Wang et al., 2023),
    <a href="https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attention-only">Docstring</a> (Heimersheim and Janiak, 2023) -->
  </p>
    <img src="images/acdc_tab1.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">

  <p>Specifically, the ROC curve over model edge identification success is used, varying the threshold \(\tau\).</p>
 

  <h3>Results</h3>
  <img src="images/acdc_fig3.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">
  <p>Interestingly, ACDC performs quite poorly on the Docstring and tracr-reverse tasks, and underwhelmingly on tracr-xproportion.
    Across all tasks, ACDC's AUC is 0.596, which is less than SP's 0.692.</p>
  
  <p>Beyond this main result, another experiment worth highlighting is the authors' use of reset networks, which are networks with permuted query, key, and value matrices.
    In principle, these networks should have no ability to perform the task, and so any circuit found in them should be spurious.
    What we see is that SP and HISP do seemingly find circuits with low loss on the task, while ACDC does not.
  </p>
  <img src="images/acdc_fig4.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">


  <h3>Discussion</h3>
  <p>The results are relatively poor, especially compared to subgraph probing.
  To me (David), the value of ACDC—and the reason it's so well known—has less to do with its proposed algorithm, and much more to do with its framing.
  The core idea of ACDC is that circuit discovery <em>is</em> automatable, and that we can evaluate automation algorithms with reference to ground truth circuits.
  That's a claim that has held up reasonably well.</p>


  <h2>
    Attribution Patching Outperforms Automated Circuit Discovery
  </h2>
  <p>
    <a href="https://arxiv.org/abs/2310.10348">This paper</a> was published in
    BlackboxNLP Workshop at EMNLP 2024.<br />
    <strong>Aaquib Syed</strong> Undergrad at University of Maryland, College Park<br />
    <strong>Can Rager</strong> Independent<br />
    <strong>Authur Conmy</strong> Research Engineer at Google DeepMind<br />
  </p>
  
  <p>
    Automatic Circuit Discovery (ACDC) algorithm use activation patching to prune out edges from the computation graph to get the circuit responsible for performing a specific task. 
    However, computing the relevance of an edge in the computation graph requires one forward pass, implying the number of forward passes requires to get the entire circuit 
    grows with the size of the model. Hence, making the ACDC algorithm intractable for larger models. One of the alternatives of activation patching is <strong>attribution patching</strong>,
    which approximates the impact of activation patching on model's output using the first-order Taylor series. Hence, this work proposes to use attribution patching to estimate the
    relevance of an edge in the computation graph to speed up the circuit discovery algorithm.
  </p>
  
  <h3>Attribution Patching</h3>
  <p>
    <img src="images/attrib.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

    Attribution patching is a technique to estimate the impact of activation patching using only two forward passes and one backward pass. More specifically, it linearly 
    approximates the metric after corrupting a single edge in the computational graph using the first-order Taylor series.
  </p>
  <p>
    \(L(x_{clean} \vert do(E=e_{corr})) \thickapprox L(x_{clean}) + (e_{corr} - e_{clean})^\intercal \frac{\partial}{\partial e_{clean}}L(x_{clean} \vert do(E=e_{clean}))\)
  </p>

  <h3>Edge Attribution Patching</h3>
  <p>
    Edge Attribution Patching (EAP) consists of two steps: 1) Use the above equation to obtain attribution scores for the importance of all edges in the computational graph and 2)
    sort these scores and keep the top k edges to form the circuit.
  </p>

  <h3>Results</h3>
  <p>
    <img src="images/results.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

    Authors assesses the circuits identified using EAP to evaluate its efficicacy. More specifically, authors discovered three circuits Indirect 
    Object Identification (IOI), Docstring, and Greater-than tasks and compared them with the circuits proposed in existing literature. The results
    indicate that the EAP outperforms ACDC.
  </p>

  <h3>How faithful are Attribution Patching's approximations</h3>
  <p>
    <img src="images/faithful.png" style="max-width:80%; width:800px;" class="mx-auto d-block">
    Attribution scores are neither correlated to activation scores nor gives good approximation of interpolated activation patching score.
  </p>

  <h3>EAP followed by ACDC Performs Better</h3>
  <p>
    <img src="images/attrib_acdc.png" style="max-width:80%; width:800px;" class="mx-auto d-block">
  </p>


  <h2>Code Resources</h2>
  <p>See <a href="https://colab.research.google.com/github/ArthurConmy/Automatic-Circuit-Discovery/blob/main/notebooks/colabs/ACDC_Main_Demo.ipynb#scrollTo=140b667d">this</a> notebook from Arthur Conmy for an example of running ACDC, and <a href="https://colab.research.google.com/github/ArthurConmy/Automatic-Circuit-Discovery/blob/main/notebooks/colabs/ACDC_Implementation_Demo.ipynb#scrollTo=ba47c650">here</a> for another Arthur Conmy-authored notebook showing more implementation details.</p>
  <p>
    For EAP, try this
    <a href="https://colab.research.google.com/github/davidbau/sidn-handbook/blob/main/public/autocircuits/colab/eap_ioi_demo.ipynb">colab notebook</a> to
    identify the IOI circuit with GPT2-small using EAP. You may want to compare
    it with the colab from previous class and check the efficicacy improvement yourself
    when using attibution patching over activation patching.
  </p>

  
</main>
</div>
</div>
</body>
</html>

</div>
