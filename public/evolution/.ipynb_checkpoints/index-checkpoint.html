<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evolution of LLM Stages across Model Sizes</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-50">
    <div class="max-w-4xl mx-auto px-4 py-8">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">Evolution of LLM Stages</h1>
            <div class="text-sm text-gray-600 flex items-center gap-2">
                <time>December 12, 2024</time>
                <span>â€¢</span>
                <div class="italic">
                    Rohit Gandikota, Alex Loftus, Philip, Ritik Bompilwar, Can Rager
                </div>
            </div>
        </header>
        <main class="prose prose-lg max-w-none">
            <section class="mb-12">
                <p class="text-gray-700 mb-6">
                    With Large Language Models (LLMs), we have been seeing the trend where larger models show emergent and better capabilities. In this project, we ask the question: "What are the added layers being used for by an LLM?". We first establish a model of mechanistic information flow in the models, we call "stages", and investigates the internal mechanisms of LLMs and how they change as model size increases.
                    Specifically, we will look at the following questions:
                </p>

                <ul class="list-disc pl-6 space-y-2 text-gray-700">
                    <li>What are the early layers in LLMs doing?</li>
                    <li>Where are the facts localized in models?</li>
                    <li>At what layers does the model transfer the knowledge of fact?</li>
                    <li>When does the model have enough information to know the answer?</li>
                    <li>Where is "processing" really happening in the models?</li>
                </ul>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Related Work</h2>
                <p class="text-gray-700">
                    This project builds upon a rich body of research dedicated to understanding the inner workings of LLMs.
                    Seminal works have explored attention mechanisms and their role in passing extracted information <span class="citation">[1]</span>, the emergence of factual knowledge
                    representation <span class="citation">[2]</span>, and the development of techniques to probe and analyze these models <span class="citation">[3]</span>. Our work contributes to this ongoing discourse by providing a comparative analysis of different LLM sizes and
                    applying these methods to unravel the complexities of their internal representations and processing.
                </p>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Methods</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens</h3>
                                <p class="text-gray-700">
                                    Visualize probability distribution over vocabulary tokens at each layer of the model, tracking prediction evolution through the network.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens Loss</h3>
                                <p class="text-gray-700">
                                    Examine loss changes at each layer to understand how the model refines its understanding and converges to final predictions.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Tracing</h3>
                                <p class="text-gray-700">
                                    Identify specific layers and components responsible for predictions through systematic input manipulation.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Attention Knockout</h3>
                                <p class="text-gray-700">
                                    Assess contribution of attention mechanisms by selectively disabling them at different layers.
                                </p>
                            </div>
                        </div>
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Intrinsic Dimension Analysis</h3>
                                <p class="text-gray-700">
                                    Quantify complexity of information representation at each layer to understand compression and expansion patterns.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Probes</h3>
                                <p class="text-gray-700">
                                    Measure accuracy of intermediate representations to reveal when sufficient information for task execution is acquired.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Intervention Effect</h3>
                                <p class="text-gray-700">
                                    Quantify impact of input feature alterations on predictions to identify causal relationships.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Representational Similarity Analysis</h3>
                                <p class="text-gray-700">
                                    Analyzing layer similarities across different model sizes reveals patterns in how representations evolve through the network.
                                </p>
                        </div>
                    </div>
                </div>

                <h2 class="text-3xl font-bold text-gray-900 mb-6">Our Hypothesis</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <p class="text-gray-700">
                            We hypothesis the following stages when an LLM processes a text to predict the next token:
                        <img src="images/stages.png" alt="LLM Stages we hypothesis" class="w-full rounded-lg"/>
                    </p>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Analysis</h2>
                
                <div class="space-y-8">
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Current LLM Performance</h3>
                        <img src="images/compare.png" style="max-width:70%" alt="LLM Trends" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            The current trend in large language models (LLMs) is to scale up existing models by increasing the number of parameters.
                            As models get larger, they seem to perform better on various tasks <span class="citation">[4, 5]</span>. It is interesting to see that the metrics show only 2 points increase from 70B to 405B. But this does not reflect the emergent capabilties in the 405B (huh .. i wonder what it says about the evaluations we do)
                        </p>
                    </div>


                    <!-- Logit Lens Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">What are the early layers doing?</h3>
                        <div class="flex flex-row gap-4 w-full">
                          <div class="w-1/2">
                            <img src="images/llama1b-logit.png" alt="Logits" class="w-full rounded-lg"/>
                          </div>
                          <div class="w-1/2">
                            <img src="images/llama8b-logit.png" alt="Logits" class="w-full rounded-lg"/>
                          </div>
                        </div>
                        <p class="text-gray-700">
                           We find an interesting artifact, smaller models repeat the input token as "next token" at early layers. Larger models, however, do not have this effect at early layers. At this point, we are unclear if this artifact is infact from the model or from the logit lens methodology itself. We aim to explore this further. 
                        </p>
                        <img src="images/logitloss.png" style="max-width:100%" alt="Logit loss" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            We scale up this experiment with wiki-10k prompts dataset <span class="citation">[6]</span> and compare the cross entropy between the logit lens distribution of the layer and the actual output distribution of the model. We see similar effect that larger models do not repeat the same input token at earlier layers 
                        </p>
                    </div>


                    

                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Internal Processing Across Model Sizes</h3>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-4">
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 1" class="w-full rounded-lg"/>
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 2" class="w-full rounded-lg"/>
                        </div>
                        <p class="text-gray-700">
                            Logit lens visualizations provide insights into how the probability distribution over vocabulary tokens evolves as information propagates through the layers of different-sized LLMs [3-5].
                        </p>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <h3 class="text-2xl font-bold mb-4">Layer Similarity Across the Model and Inter-Model</h3>
                    <img src="/api/placeholder/800/400" alt="Layer Similarity Analysis" class="w-full mb-4 rounded-lg"/>
                    <p class="text-gray-700">
                        Representational Similarity Analysis (RSA) is a multivariate technique that compares diverse data types by examining shared patterns in their similarity matrices. Originally proposed by 
                        <a href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full?utm_source=FWEB&utm_medium=NBLOG&utm_campaign=ECO_10YA_top-research" target="_blank">Kriegeskorte et al.</a> 
                        to link brain activity patterns to computational models and behavior, RSA works by subjecting the brain/model to different stimuli and extracting activity patterns. Pairwise correlations are computed to create a dissimilarity matrix (RDM), which captures the representational geometry of the data. This enables comparison across different data types and models.
                    </p>
                    
                    <div class="mt-4 flex flex-col items-center">
                        <img src="images/computing_RDMs.png" 
                             alt="RSA Computation Process" 
                             class="w-1/2 rounded-lg mb-4"/>
                        <p class="text-gray-600 text-sm italic text-center">
                            Computation of the Representational Dissimilarity Matrix (RDM).
                        </p>
                    </div>
                    
                    <div class="mt-6">
                        <h5 class="text-xl font-bold mb-3">How does RSA apply to LLMs?</h5>
                        <p class="text-gray-700">
                            <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Li et al.</a> used RSA to study structural similarity between LLMs and neural responses. 
                            <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Yousefi et al.</a> used TSA to decode task-critical information from model representations without using parameterized probes. This technique helps compare representational geometry across LLM sizes and layers to reveal computational phases.
                        </p>
                    </div>
                    
                    <div class="mt-6">
                        <h5 class="text-xl font-bold mb-3">Methodology for Layer-wise RSA in LLMs</h5>
                        <p class="text-gray-700">
                            The following methodology outlines a structured approach to analyze layer-wise representational structures in LLMs using RSA:
                        </p>
                        <ol class="list-decimal list-inside space-y-2 mt-2 text-gray-700">
                            <li>
                                <strong>Input Processing:</strong> Start with a set of <span class="font-bold">N</span> sentences or questions from domains like biology and cybersecurity. Feed these inputs into LLMs (e.g., Llama 1B, 3B, 8B) to ensure diversity in both input complexity and model capacity.
                            </li>
                            <li>
                                <strong>Layer-Wise Activation Capture:</strong> Extract neural activations (hidden states) for each input at each layer <span class="font-bold">L</span>. This provides a layer-by-layer snapshot of how the modelâ€™s internal representations evolve.
                            </li>
                            <li>
                                <strong>Representation Processing:</strong>
                                <ul class="list-disc list-inside ml-6 mt-1">
                                    <li><strong>Mean Pooling:</strong> Average token-level activations per input to produce a single vector representation. This simplifies the representation while retaining essential information.</li>
                                    <li><strong>Normalization:</strong> Optionally normalize these vectors to ensure that differences in vector magnitude do not unfairly influence similarity measurements.</li>
                                </ul>
                            </li>
                            <li>
                                <strong>RDM Generation:</strong> Compute pairwise similarities (e.g., correlation) between the processed input vectors. Convert these similarities into dissimilarities (<code>RDM = 1 - similarity</code>) to form an <span class="font-bold">NÃ—N</span> Representational Dissimilarity Matrix for each layer.
                            </li>
                            <li>
                                <strong>Vectorizing RDMs:</strong> Extract the upper-triangular portion of each RDM and flatten it into a vector. This standard format allows direct comparisons between layersâ€™ RDMs.
                            </li>
                            <li>
                                <strong>Layer Similarity Analysis:</strong> Compare these RDM-derived vectors across layers (and across models) to identify patterns and trends. This helps reveal how representation changes throughout the network depth and how these changes scale with model size, shedding light on potential computational phases within the model.
                            </li>
                        </ol>
                    </div>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Discussion</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <p class="text-gray-700 mb-4">The findings of this project raise several interesting questions and avenues for future research:</p>
                    <ul class="list-disc pl-6 space-y-2 text-gray-700">
                        <li>What are the precise mechanisms underlying the observed shifts in information processing as models scale up?</li>
                        <li>How do different training datasets and objectives influence the development of computational phases within LLMs?</li>
                        <li>Can we leverage insights from RSA to develop more efficient and interpretable LLM architectures?</li>
                    </ul>
                </div>
            </section>


            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Citations</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <ol class="list-decimal list-inside space-y-2">
                        <li>Geva, M., Bastings, J., Filippova, K., & Globerson, A. (2023). Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767.</li>
                        <li>Meng, K., Bau, D., Andonian, A., & Belinkov, Y. (2022). Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35, 17359-17372.</li>
                        <li>Alain, G. (2016). Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644.</li>
                        <li>Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., ... & Ganapathy, R. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</li>
                        <li>Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., ... & Garg, S. (2024). Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.</li>
                        <li>NeelNanda/wiki-10k Â· Datasets at Hugging Face. (n.d.). https://huggingface.co/datasets/NeelNanda/wiki-10k</li>
                    </ol>
                </div>
            </section>



            
        </main>
    </div>
</body>
</html>