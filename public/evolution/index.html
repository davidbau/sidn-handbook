<!DOCTYPE html>
<html lang="en">
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/evolution/index.html">Evolution of LLM stages</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Evolution of LLM stages</h1>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evolution of LLM Stages across Model Sizes</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-50">
    <div class="max-w-4xl mx-auto px-4 py-8">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">Evolution of LLM Stages</h1>
            <div class="text-sm text-gray-600 flex items-center gap-2">
                <time>September 5, 2024</time>
                <span>â€¢</span>
                <div class="italic">
                    Rohit Gandikota, Alex Loftus, Philip, Ritik Bompilwar, Can Rager
                </div>
            </div>
        </header>
        <main class="prose prose-lg max-w-none">
            <section class="mb-12">
                <p class="text-gray-700 mb-6">
                    This project first establishes a mexhanistic flow of information in language models, we call "stages", and investigates the internal mechanisms of LLMs and how they change as model size increases.
                    Specifically, we will look at the following questions:
                </p>

                <ul class="list-disc pl-6 space-y-2 text-gray-700">
                    <li>What are the early layers in LLMs doing?</li>
                    <li>Where are the facts localized in models?</li>
                    <li>At what layers does the model transfer the knowledge of fact?</li>
                    <li>When does the model have enough information to know the answer?</li>
                    <li>Where is "processing" really happening in the models?</li>
                </ul>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Related Work</h2>
                <p class="text-gray-700">
                    This project builds upon a rich body of research dedicated to understanding the inner workings of LLMs.
                    Seminal works have explored attention mechanisms and their role in passing extracted information <span class="citation">[1]</span>, the emergence of factual knowledge
                    representation [CITATION], and the development of techniques to probe and analyze these models [CITATION]. Our
                    work contributes to this ongoing discourse by providing a comparative analysis of different LLM sizes and
                    applying novel methodologies to unravel the complexities of their internal representations and processing.
                </p>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Methods</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens</h3>
                                <p class="text-gray-700">
                                    Visualize probability distribution over vocabulary tokens at each layer of the model, tracking prediction evolution through the network.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens Loss</h3>
                                <p class="text-gray-700">
                                    Examine loss changes at each layer to understand how the model refines its understanding and converges to final predictions.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Tracing</h3>
                                <p class="text-gray-700">
                                    Identify specific layers and components responsible for predictions through systematic input manipulation.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Attention Knockout</h3>
                                <p class="text-gray-700">
                                    Assess contribution of attention mechanisms by selectively disabling them at different layers.
                                </p>
                            </div>
                        </div>
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Intrinsic Dimension Analysis</h3>
                                <p class="text-gray-700">
                                    Quantify complexity of information representation at each layer to understand compression and expansion patterns.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Probes</h3>
                                <p class="text-gray-700">
                                    Measure accuracy of intermediate representations to reveal when sufficient information for task execution is acquired.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Intervention Effect</h3>
                                <p class="text-gray-700">
                                    Quantify impact of input feature alterations on predictions to identify causal relationships.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Representational Similarity Analysis (RSA)</h3>
                                <p class="text-gray-700">
                                    Representational Similarity Analysis (RSA) is a multivariate technique that compares diverse data types by examining shared patterns in their similarity matrices. Originally proposed by 
                                    <a href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full?utm_source=FWEB&utm_medium=NBLOG&utm_campaign=ECO_10YA_top-research" target="_blank">Kriegeskorte et al.</a> 
                                    to link brain activity patterns to computational models and behavior, RSA works by subjecting the brain/model to different stimuli and extracting activity patterns. Pairwise correlations are computed to create a dissimilarity matrix (RDM), which captures the representational geometry of the data. This enables comparison across different data types and models.
                                </p>
                                <div class="mt-4">
                                    <img src="images/computing_RDMs.png" 
                                         alt="RSA Computation Process" 
                                         class="w-full rounded-lg mb-4"/>
                                    <p class="text-gray-600 text-sm italic text-center">
                                        Computation of the Representational Dissimilarity Matrix (RDM).
                                    </p>
                                </div>
                                <div class="mt-6">
                                    <h5 class="text-xl font-bold mb-3">How does RSA apply to LLMs?</h5>
                                    <p class="text-gray-700">
                                        <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Li et al.</a> used RSA to study structural similarity between LLMs and neural responses. 
                                        <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Yousefi et al.</a> used TSA to decode task-critical information from model representations without using parameterized probes. This technique helps compare representational geometry across LLM sizes and layers to reveal computational phases.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Analysis and Diagrams</h2>
                
                <div class="space-y-8">
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Current Trends in LLMs</h3>
                        <img src="/api/placeholder/800/400" alt="LLM Trends" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            The current trend in large language models (LLMs) is to scale up existing models by increasing the number of parameters.
                            As models get larger, they seem to perform better on various tasks [1, 2].
                        </p>
                    </div>

                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Internal Processing Across Model Sizes</h3>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-4">
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 1" class="w-full rounded-lg"/>
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 2" class="w-full rounded-lg"/>
                        </div>
                        <p class="text-gray-700">
                            Logit lens visualizations provide insights into how the probability distribution over vocabulary tokens evolves as information 
                            propagates through the layers of different-sized LLMs [3-5].
                        </p>
                    </div>
                </div>
                            
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Discussion</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <p class="text-gray-700 mb-4">The findings of this project raise several interesting questions and avenues for future research:</p>
                    <ul class="list-disc pl-6 space-y-2 text-gray-700">
                        <li>What are the precise mechanisms underlying the observed shifts in information processing as models scale up?</li>
                        <li>How do different training datasets and objectives influence the development of computational phases within LLMs?</li>
                        <li>Can we leverage insights from RSA to develop more efficient and interpretable LLM architectures?</li>
                    </ul>
                </div>
            </section>


            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Citations</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <ol class="list-decimal list-inside space-y-2">
                        <li>Brown, T., et al. (2020). "Language Models are Few-Shot Learners." Advances in Neural Information Processing Systems, 33, 1877-1901</li>
                        <li>Vaswani, A., et al. (2017). "Attention Is All You Need." Advances in Neural Information Processing Systems, 30, 5998-6008</li>
                        <li>Petroni, F., et al. (2019). "Language Models as Knowledge Bases?" Proceedings of EMNLP-IJCNLP 2019</li>
                        <li>Tenney, I., et al. (2019). "BERT Rediscovers the Classical NLP Pipeline." ACL 2019</li>
                        <li>Li, J., et al. (2021). "Comparing Representational Similarity of Neural Networks and Brains." Nature Communications, 12(1), 1-12</li>
                        <li>Yousefi, M., et al. (2021). "Decoding Task-Critical Information from Language Models." arXiv preprint arXiv:2106.01930</li>
                    </ol>
                </div>
            </section>

            
        </main>
    </div>
</body>
</html>