<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
  <div class="container-fluid">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item active">
      <a class="nav-link" href="/index.html">Probing</a>
     </li>
    </ul>
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
     </li>
    </ul>
  </div>
</nav>

<div class="container">
  <div class="row">
    <div class="col-2 position-fixed pt-5">
      <nav id="toc" data-toggle="toc"></nav>
    </div>
    <div class="col-2"></div>
    <main class="col-8">
      <h1 class="mt-5">Probing</h1>
      <h5>September 19, 2024  â€¢ <em>Rahul Chowdhury, Ritik Bompilwar</em></h5>

      <h2>What is Probing?</h2>
      <p>
        Probing is an attempt by computer scientists to understand the workings of neural networks.
        The most popular way of probing is by learning to make sense of a representation of a neural network 
        by keeping the information in its purest form as much as possible. The information under scrutiny is usually 
        a human interpretable property of known data that is believed could be decoded in the easiest way if the information 
        is present in the representation and also in the data that is passed through the neural network. 
      </p>
      <p>
        There is also a second line of work that is more concerned if a particular property that is learned is used in the 
        training objective. This line of work is unbothered by the presence of a particular feature which could be learned
        as a by-product of a training objective. This is investigated by switching off the features that are believed to be representing 
        a property and observing the reaction of the neural network's predictability to that intervention. 
      </p>

      <h2>Linear Probing</h2>
      <p>
        Linear Probing is a learning technique to assess the information content in the representation layer of a neural network. 
        This is done to answer questions like what property of the data in training did this representation layer learn that will 
        be used in the subsequent layers to make a prediction. The other question to ask is how we can test the presence of such property 
        without adulterating the purity of information. Computer scientists believed that associating a representation with a said property with 
        parametric learning paradigm could be done with less adulteration by a linear classifier since a complex model with non-linear activations might memorize the data and force the representation 
        to project to the labels associated with the data [1]. One way to test if the probing learnable function is memorizing the label
        along with the data instead of drawing a valid decision line is by checking the selectivity of the probing. 
      </p>
      <p>
        In this test, a probe for a feature under scrutiny is trained with random labels and the same probing structure is trained with 
        original labels, and the probe trained with random labels serves as a control and their performances are recorded [1]. 
        If the probe achieves high accuracy on the random labels and the difference between the accuracy of the probe trained with original and the noisy labels is not significant, 
        it is said to be low on selectivity and is memorizing the probing dataset. Non-linear probes have been alleged to have this property, and that is why a linear probe is 
        entrusted with this task. Finally, good probing performance would hint at the presence of the said property, which has the potential of 
        being used in making final decisions to choose a label in the farthest layer of the neural network. 
      </p>
      <h2>Amnesic Probing</h2>
      <p>
        Probing usually deals with discovery and localization of concepts in a trained neural network, and 
        investigates what the neural network has learned and how they could be responsible for the outcome of training.
        There is another line of research that has an orthogonal approach to understanding the dependencies of a neural network on 
        said features when it takes a decision [1]. This line of research wants to study the reaction of the neural network if that feature that might be a
        representation of a said property is suppressed via intervention. Intervention is usually done in the deep layers rather than input
        since intervention in the input involves querying a lot of neurons. The goal of this intervention is to test if the features learning identified properties 
        are at all used during making a decision. This could also be a test to observe if the learned representation is actually the feature of the probed label or is 
        it just a mere correlation without causation. 
      </p>

      <h3>Intervention Mechanism</h3>
      <p>
        Iterative Null Space Projection algorithm was used to transform features that could be used to identify particular 
        property [1]. This is usually done through repeated projection of weight matrix used to project features into the distribution of labels 
        to null space, followed by learning a new matrix to linearly transform the null projected features into the distribution of labels. 
        This operation is repeated because null space projection can nullify the contribution by the number of dimensions of the labels. In order to 
        ensure most of the dimensions that could have information about the property, multiple linear projections are required to minimize the rank of the matrix
        to a point where the information that could be transformed linearly to utilize the said property is diminished [1]. This is repeated until the linear projection 
        can no longer classify the features into the target labels better than random projections [1]. This repeated linear projection gets rid of the chance of the 
        embedding being processed linearly to utilize information that could represent the said property [1].
      </p>

      <h3>How Does Intervention Help?</h3>
      <p>
        After performing linear intervention, we check if that intervention causes a drop in performance. This brings forth questions: Is the absence of the said 
        property causing the drop? Or was it a mere correlation that could have caused the drop, and were there other features entangled in the embedding that could have been the 
        reason for the original performance? A great way to test if our hypothesis that the said property that the feature is representing is 
        by simultaneously training another set of matrices in the same Iterative Null Space Projection algorithm but with random labels. 
        If the linear intervention finds the neutralized feature caused a drop in performance better than the random linear intervention [1], it could be concluded 
        that the said property might be representing the property and the information in that embedding was essential in determining the classes of an input data. 
      </p>
      <h2>Emergent World Representations</h2>
       <p>
         Language models have shown an amazing range of capabilities, but the question of what they learn from data has been a subject of debate. Do the models memorize a collection of surface statistics, or do they form certain interpretable representations of the process that generates the sequences they see?
       </p>
       <p>
         Li et al. (2023) proposed the existence of world representations in language models. The focus of the paper is to investigate the form of internal representations in the language model performing a specific task in confined settings. To investigate this world representation, the authors chose the popular game of Othello (try it <a href="https://cardgames.io/reversi/">here</a>). If we think of the board as the "world," then the game provides an appealing experimental testbed to explore world representations.
       </p>
       <!-- <p> Othello is a straightforward two-player game played on an 8Ã—8 board. Players alternate placing black or white discs, starting with four discs (two black and two white) placed in the center. 
        Black moves first. Each move must flip at least one opponentâ€™s disc by outflankingâ€”or sandwichingâ€”the opponentâ€™s discs between two of the playerâ€™s own. The game ends when neither player has any legal moves left.
       </p> -->
       <p> The authors trained an 8-layer GPT model with an 8-head attention mechanism and a 512-dimensional hidden space on two sets of training data. The first set called "championship" is smaller in size (140K games) but consists of strategic moves by expert human players.
        The second dataset called "synthetic" is larger in size (2.4M games) consisting of legal but otherwise random moves. The goal was to study how much Othello-GPT can learn from pure sequence information with minimal inductive bias. The model was evaluated on the validation set 
        with error rate as the metric. Othello-GPT had 0.01% error rate on synthetic data, 5.17% error rate on championship data while the untrained model had 93.29% error rate.
       </p>
       <h4>Is the Model Memorizing?</h4>
       <p> To test whether the model was simply memorizing game sequences, the authors created a skewed dataset by removing all games that started with one of the four possible opening moves (specifically, they removed games starting with the move C5). 
        This eliminated 25% of all possible game openings. Despite never seeing these sequences during training, Othello-GPT still performed with a low error rate of 0.02% on them. This indicated that the model is not just memorizing but has learned to generalize from the patterns in the data.
       </p>
       <h3>Intervention Technique for Probing Internal Representations</h3>
       <p>Authors trained probes that predict the board state from the network's internal activations after a given sequence of moves. They found that linear probess had higher error rates than non-linear probes (2 layer MLP), indicating that the probe may be recovering a nontrivial representation of board state in the network's activations.</p>
       <p> <figure> <img  src="images/intervention.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
        <p>
            To evaluate whether Othello-GPTâ€™s internal representations of the board state directly influence its move predictions, the authors introduced targeted interventions by altering activations across multiple layers, starting from an initial layer Lâ‚›. Utilizing gradient descent, they adjusted specific activations to transition the board state from B to Bâ€² by modifying certain state elements. This precise alteration ensures that only a segment of the board state changes, thereby affecting the set of legal moves. The authors then assessed the success of these interventions by probing the modified activations and verifying if the modelâ€™s move predictions corresponded with the updated board state.
          </p>
        <p> <figure> <img  src="images/intervention_result.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
            <p>
                To systematically test the causal impact of internal board representations on Othello-GPTâ€™s predictions, the authors developed a benchmark set comprising 2,000 intervention cases, divided into natural (reachable) and unnatural (unreachable) board states. For each test case, they modified the modelâ€™s internal activations to alter the board state and monitored the subsequent move predictions. By comparing these predictions with the expected set of legal moves, they calculated error rates. The results showed a significant reduction in errors from baseline levels, even in the unnatural subset, which underscores that the emergent internal representations play a causal role in shaping the modelâ€™s decision-making processes.
              </p>
        </p>
        <h3>Latent Saliency Maps</h3>
        <p> <figure> <img  src="images/latent_saliency_maps.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
            <p>
                <p>
                    The latent saliency maps are generated by assessing how changes to each tileâ€™s internal representation affect the modelâ€™s prediction for a specific move p. For each tile s on the board B, the authors apply the intervention technique to alter the representation of s and observe the resulting change in the prediction probability of p. They calculate the saliency S<sub>s</sub> as the difference between the original prediction probability p<sub>0</sub> and the new probability p<sub>s</sub> after intervention (S<sub>s</sub> = p<sub>0</sub> - p<sub>s</sub>). This saliency value indicates how much tile s influences the prediction of move p. By computing S<sub>s</sub> for all tiles and visualizing them, they create a latent saliency map that highlights the most influential tiles affecting the modelâ€™s decision.
                  </p>
      <h2>Code Resources</h2>
      <p> The "Emergent World Representations" paper has provided the code here: <a href="https://github.com/likenneth/othello_world">Othello World</a>. Aditionally, Neel Nanda has created a TransformerLens version of the Othello-GPT,which allows for inspecting each MLP neuron in the model, boosting the mechanistic interpretability. Try the Colab Notebook here: <a href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb">Othello GPT Colab</a>.<p>
      <p> The "Amnesic Probing" paper has provided the code here: <a href="https://github.com/yanaiela/amnesic_probing">Amnesic Probing</a></p>.
      <h2>References</h2>
      <ol>
        <li>Elazar, Yanai, et al. "Amnesic probing: Behavioral explanation with amnesic counterfactuals." Transactions of the Association for Computational Linguistics 9 (2021): 160-175.</li>
      </ol>
           Rahul Chowdhury's Project Github Profile:the code here: <a href="https://github.com/rchowdhubnor/CS7180InterpretLLMTS.git">Project Code</a></p>.
    </main>
  </div>
</div>
</body>
</html>
