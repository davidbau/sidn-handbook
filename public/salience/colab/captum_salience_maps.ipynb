{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install captum"
      ],
      "metadata": {
        "id": "B7eF20otR161"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using The Captum Library for Image Salience Mapping\n",
        "===================================================\n",
        "\n",
        "Based on https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html"
      ],
      "metadata": {
        "id": "Q12Gst1ddZHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR43OACYRgIC"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNFp5LrPRgID"
      },
      "source": [
        "Model Interpretability using Captum\n",
        "===================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsKv7LiLRgIE"
      },
      "source": [
        "Captum helps you understand how the data features impact your model\n",
        "predictions or neuron activations, shedding light on how your model\n",
        "operates.\n",
        "\n",
        "Using Captum, you can apply a wide range of state-of-the-art feature\n",
        "attribution algorithms such as `Guided GradCam` and\n",
        "`Integrated Gradients` in a unified way.\n",
        "\n",
        "In this recipe you will learn how to use Captum to:\n",
        "\n",
        "-   Attribute the predictions of an image classifier to their\n",
        "    corresponding image features.\n",
        "-   Visualize the attribution results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VCJ5zHdRgIE"
      },
      "source": [
        "Before you begin\n",
        "================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjdC_k5iRgIE"
      },
      "source": [
        "Make sure Captum is installed in your active Python environment. Captum\n",
        "is available both on GitHub, as a `pip` package, or as a `conda`\n",
        "package. For detailed instructions, consult the installation guide at\n",
        "<https://captum.ai/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8K4DhzSRgIE"
      },
      "source": [
        "For a model, we use a built-in image classifier in PyTorch. Captum can\n",
        "reveal which parts of a sample image support certain predictions made by\n",
        "the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpzh6-0gRgIE"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "model = torchvision.models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()\n",
        "\n",
        "response = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "center_crop = transforms.Compose([\n",
        " transforms.Resize(256, max_size=300),\n",
        " transforms.CenterCrop(224),\n",
        "])\n",
        "\n",
        "normalize = transforms.Compose([\n",
        "    transforms.ToTensor(),               # converts the image to a tensor with values between 0 and 1\n",
        "    transforms.Normalize(                # normalize to follow 0-centered imagenet pixel RGB distribution\n",
        "     mean=[0.485, 0.456, 0.406],\n",
        "     std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "input_img = normalize(center_crop(img)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvC-ev0PRgIF"
      },
      "source": [
        "Computing Attribution using Occlusion\n",
        "=====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpGOBv6RgIF"
      },
      "source": [
        "Among the top-3 predictions of the models are classes 208 and 283 which\n",
        "correspond to dog and cat.\n",
        "\n",
        "Let us attribute each of these predictions to the corresponding part of\n",
        "the input, using Captum's `Occlusion` algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0eWIf-URgIF"
      },
      "outputs": [],
      "source": [
        "from captum.attr import Occlusion\n",
        "\n",
        "occlusion = Occlusion(model)\n",
        "\n",
        "strides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\n",
        "target=208,                       # Labrador index in ImageNet\n",
        "sliding_window_shapes=(3, 45, 45) # choose size enough to change object appearance\n",
        "baselines = 0                     # values to occlude the image with. 0 corresponds to gray\n",
        "\n",
        "attribution_dog = occlusion.attribute(\n",
        "    input_img,\n",
        "    strides = strides,\n",
        "    target=target,\n",
        "    sliding_window_shapes=sliding_window_shapes,\n",
        "    baselines=baselines)\n",
        "\n",
        "\n",
        "target=283,                       # Persian cat index in ImageNet\n",
        "attribution_cat = occlusion.attribute(\n",
        "    input_img,\n",
        "    strides = strides,\n",
        "    target=target,\n",
        "    sliding_window_shapes=sliding_window_shapes,\n",
        "    baselines=baselines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1yVghBJRgIF"
      },
      "source": [
        "Besides `Occlusion`, Captum features many algorithms such as\n",
        "`Integrated Gradients`, `Deconvolution`, `GuidedBackprop`,\n",
        "`Guided GradCam`, `DeepLift`, and `GradientShap`. All of these\n",
        "algorithms are subclasses of `Attribution` which expects your model as a\n",
        "callable `forward_func` upon initialization and has an `attribute(...)`\n",
        "method which returns the attribution result in a unified format.\n",
        "\n",
        "Let us visualize the computed attribution results in case of images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RockkDRmRgIF"
      },
      "source": [
        "Visualizing the Results\n",
        "=======================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgKjYxgyRgIF"
      },
      "source": [
        "Captum's `visualization` utility provides out-of-the-box methods to\n",
        "visualize attribution results both for pictorial and for textual inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64bNsSclRgIF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "def visualize_results(img, attribution, methodname, title):\n",
        "    # Convert the compute attribution tensor into an image-like numpy array\n",
        "    attribution_np = np.transpose(attribution[0].cpu().detach().numpy(), (1,2,0))\n",
        "    vis_types = [\"original_image\", \"heat_map\", \"blended_heat_map\"]\n",
        "    vis_signs = [\"all\", \"all\", \"positive\"] # \"positive\", \"negative\", or \"all\" to show both\n",
        "    # positive attribution indicates that the presence of the area increases the prediction score\n",
        "    # negative attribution indicates distractor areas whose absence increases the score\n",
        "    _ = viz.visualize_image_attr_multiple(\n",
        "        attribution_np,\n",
        "        np.array(center_crop(img)),\n",
        "        vis_types,\n",
        "        vis_signs,\n",
        "        [\"image\", methodname + \" attribution for \" + title, \"overlay\"],\n",
        "        show_colorbar=True,\n",
        "        )\n",
        "\n",
        "visualize_results(img, attribution_dog, \"Occlusion\", \"dog\")\n",
        "visualize_results(img, attribution_cat, \"Occlusion\", \"cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstration of Integrated Gradients\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "2EUquS1y1Ybn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "intgrad = IntegratedGradients(model)\n",
        "\n",
        "target=208,                       # Labrador index in ImageNet\n",
        "baselines = 0                     # values to occlude the image with. 0 corresponds to gray\n",
        "\n",
        "ig_attribution_dog = intgrad.attribute(input_img, baselines=baselines, target=target, n_steps=50)\n",
        "\n",
        "\n",
        "target=283,                       # Persian cat index in ImageNet\n",
        "ig_attribution_cat = intgrad.attribute(input_img, baselines=baselines, target=target, n_steps=50)\n",
        "\n",
        "visualize_results(img, ig_attribution_dog, \"IntegratedGradients\", \"dog\")\n",
        "visualize_results(img, ig_attribution_cat, \"IntegratedGradients\", \"cat\")"
      ],
      "metadata": {
        "id": "E2-zmBVld5M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstration of SmoothGrad modification of Integrated Gradients\n",
        "----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "j7CQuFII1hZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import NoiseTunnel\n",
        "\n",
        "smoothgrad = NoiseTunnel(intgrad)\n",
        "\n",
        "target=208,                       # Labrador index in ImageNet\n",
        "baselines = 0                     # values to occlude the image with. 0 corresponds to gray\n",
        "\n",
        "smoothgrad_attribution_dog = smoothgrad.attribute(\n",
        "    input_img, nt_type='smoothgrad', nt_samples=10, target=target)\n",
        "\n",
        "\n",
        "target=283,                       # Persian cat index in ImageNet\n",
        "smoothgrad_attribution_cat = smoothgrad.attribute(\n",
        "    input_img,  nt_type='smoothgrad', nt_samples=10, target=target)\n",
        "\n",
        "visualize_results(img, smoothgrad_attribution_dog, \"SmoothGrad\", \"dog\")\n",
        "visualize_results(img, smoothgrad_attribution_cat, \"SmoothGrad\", \"cat\")\n"
      ],
      "metadata": {
        "id": "1VbempOma8rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstration of GradCAM\n",
        "------------------------"
      ],
      "metadata": {
        "id": "wqIfSQSY16rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import LayerGradCam, LayerAttribution\n",
        "\n",
        "gradcam = LayerGradCam(model, model.layer4)\n",
        "\n",
        "target = 208\n",
        "gradcam_attribution_dog = gradcam.attribute(\n",
        "    input_img, target=target)\n",
        "up_gradcam_attribution_dog = LayerAttribution.interpolate(\n",
        "    gradcam_attribution_dog, input_img.shape[2:])\n",
        "\n",
        "\n",
        "target=283,                       # Persian cat index in ImageNet\n",
        "gradcam_attribution_cat = gradcam.attribute(\n",
        "    input_img, target=target)\n",
        "up_gradcam_attribution_cat = LayerAttribution.interpolate(\n",
        "    gradcam_attribution_cat, input_img.shape[2:])\n",
        "\n",
        "visualize_results(img, up_gradcam_attribution_dog, \"GradCAM\", \"dog\")\n",
        "visualize_results(img, up_gradcam_attribution_cat, \"GradCAM\", \"cat\")"
      ],
      "metadata": {
        "id": "6JaTdNy62Nft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradcam_attribution_dog.shape"
      ],
      "metadata": {
        "id": "S478Jy745q5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkwUwrHfRgIF"
      },
      "source": [
        "Final Notes\n",
        "===========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McEvsy_eRgIG"
      },
      "source": [
        "You should play with and customize this notebook.\n",
        "\n",
        "* Try some other images and target classes.  In particular, we have provided a few interesting images to try:\n",
        "\n",
        "https://sidn.baulab.info/salience/images/dog-and-cat.jpg\n",
        "https://sidn.baulab.info/salience/images/rv-and-speedboat.jpg\n",
        "https://sidn.baulab.info/salience/images/toaster-scene.jpg\n",
        "\n",
        "* Try reproducing the Adebayo results\n",
        "\n",
        "Be sure to think about and answer the reading questions from today."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}