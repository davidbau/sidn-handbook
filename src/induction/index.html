{header("Induction")}

<h5>October 10, 2024 • <em>Nikita Demidov, Prajnan Goswami</em></h5>

<p>
    Can we teach a Large Language Model (LLM) a new task without finetuning it?
    The anwer is In-context Learning(ICL) where the LLM performs a new task by prompting the model with
    <b>input-output examples</b> demonstrating the task. This remarkable behaviour was first shown in
    <a
        href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>
    and caught more traction in <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>.
    Jason Wei, who is an AI researcher at OpenAI(previously Google Brain) further showed the potential of
    ICL through his work on <a href="https://arxiv.org/abs/2201.11903">
        Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>.

<figure id="figure1">
    <img src="images/chain-of-thought.png" style="max-width:80%; width:800px;" class="mx-auto d-block">
</figure>

<h2>How can we better understand In-context Learning(ICL)?</h2>
<p>As In-context Learning led to the development of customized LLM applications tailored
    for specific use cases, it became increasingly important to understand how these <b>large
        Transformer</b> based langauge models were able to solve new tasks by merely looking
    at examples in the prompt.

<p>In the <a href="/formulation">previous chapter</a>, we delved into the inner workings of
    a <b>Transformer</b>. One key takeway from it was how attention heads can be decomposed
    into Query-Key(QK) and Output-Value(OV) circuits. More importantly it showed how these
    independent circuits mapped input tokens to output tokens.

<p>Leveraging this Mathematical Framework of Transformers, the same authors at Anthropic introduced
    the notion of Induction Heads. Chris Olah known for reverse engineering artificial neural networks
    for human interpretability first discovered this phenomenon in a 2-layer model. Together with
    Catherine Olsson, Nelson Elhage and Neel Nanda led the systematic study of this phenomenon
    to <b>better understand how in-context learning works</b>.

<p>This chapter focuses on the role of Induction Heads in In-context Learning
    and provides a high level walkthrough of how it works.


<h2>What is an Induction Head?</h2>

Induction heads were discovered in the paper <a href="https://transformer-circuits.pub/2021/framework/index.html">
    A Mathematical Framework for Transformer Circuits</a> while studying the behavior of the two-attention layer
transofmers. These heads
impressed the authors (Olsson, Elhage and Nanda) so much they authored the creation of the paper <a
    href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">In-context Learning
    and Induction Heads</a> to further explore the impact of the induction heads on the in-context learning.

The <a href="https://transformer-circuits.pub/2021/framework/index.html#induction-heads">Original Paper</a> also gives
the mechanistic interpretation of the induction head and how it works in the transformer model
<h3>Intuitive Explanation</h3>
<p>An <strong>Induction Head</strong> is a special type of attention mechanism inside transformer models, responsible
    for recognizing repeating patterns in a sequence of tokens and predicting the next token based on past patterns. To
    break it down, an induction head works as follows:</p>

<ol>
    <li><strong>Previous Token Head</strong>: This attention head looks back at the sequence of tokens to find where a
        certain token has appeared before. It doesn’t just look at a single token but captures sequences. For instance,
        if the sequence "cat sat" has appeared earlier and the model sees "cat" again, the <strong>Previous Token
            Head</strong> recalls that <span style="color: red;">"sat"</span> followed <span
            style="color: red;">"cat"</span> in the earlier context.</li>
    <li><strong>Induction Head</strong>: Once it retrieves the past sequence ("cat sat"), the <strong>Induction
            Head</strong> predicts that <span style="color: red;">"sat"</span> will likely be the next word after <span
            style="color: red;">"cat"</span>. It essentially tries
        to extend the previous pattern.</li>
</ol>

<h3>Quick recap on the Mathematical Framework for Transformer circuits</h3>
<!-- <p>The <a
      href="https://transformer-circuits.pub/2021/framework/index.html#induction-heads">
      A Mathematical Framework for Transformer Circuits</a> gives a formal definition of the induction head and supports its existence through tensor product
      definitions of the circuits.</p> -->

<p>Recall from the <a href="/formulation">previous chapter</a> the concepts of <strong>QK and OV circuits</strong>:</p>

<p><strong>Query-Key Circuit (QK)</strong>: This determines how much attention a given token (the query) pays to
    previous tokens (the keys). It calculates attention scores to decide which previous tokens are relevant for
    predicting the next token.</p>

<!-- <p>The matrix representing the QK circuit for a single attention model is:</p>

<p>
    \[
    c_{ij}^H = v^T_iW_{QK}^Hv_j
    \]
    is the attention paid by the ith token to the jth token with \(v_i\) being the ith token in the residual stream.
</p> -->

<p><strong>Output-Value Circuit (OV)</strong>: This determines how the token attended to (the "value") influences the
    final prediction of the model. It directly impacts the logits (predicted probabilities for each possible next
    token).</p>

<!-- <p>The matrix representing the OV circuit is:</p>

<p>
    \[
    (v_j^H)^TW_{OV}
    \]
    Where \(v_j\) is the residual stream vector at the source token. The result of the vector is propagated down the
    stream for the source token.
</p> -->


<p>There is also the concept of <strong>Q, K, and V compositions</strong>. Compositions are products of multiple
    attention layers that enable a more complex flow of information. They are distinguished based on which weight matrix
    is used to read in the subspace of the residual stream:</p>

<ul>
    <li><strong>Q-composition</strong> uses the \(W_Q\) matrix.</li>
    <li><strong>K-composition</strong> uses the \(W_K\) matrix.</li>
    <li><strong>V-composition</strong> uses the \(W_V\) matrix.</li>
</ul>

<p>Q and K compositions impact the results of the QK circuits, while V composition affects the OV circuit.</p>

<h3>Mechanistic Interpretation of Induction Head</h3>

<p>The paper formally defines the induction head as exhibiting the following two properties on a repeated random
    sequence of tokens:</p>

<!-- <ul>
    <li><strong>Prefix matching:</strong> The head attends to previous tokens that were followed by the current and/or
        recent tokens. The simplest induction heads match just one preceding token. However, some induction heads
        perform a fuzzy match over several preceding tokens, meaning they attend to the token that induction suggests
        will come next.</li>
    <li><strong>Copying:</strong> The head’s output increases the logit corresponding to the attended-to token.</li>
</ul>-->
<ul>
    <li><strong>Prefix matching</strong>: The head attends to previous tokens that were followed by the current one. 
        The induction head uses the <strong>K-composition (\(W_k\) matrix)</strong> to retrieve information about the preceding token that were found in Preceeding Token Head and was writen in the residual stream then it matches
        and them to the query, which is a current token. As a result, it allows to find the token that was followed by the current one.</li>
    </li>
    <li><strong>Copying</strong> is achieved through the <strong>OV circuit</strong>. The mechanistic paper proved that
        many
        transformers exhibit copying behavior through the OV circuit.</p>
    </li>
</ul>

<p>In summary, the model first learns that a prefix follows the current token, passes this information through the
    residual stream, and the induction head retrieves this information via the K-composition and copies the prefix into the output.</p>

<p>The image is the summary of how the induction head is done visually
    {img("images/Induction_Head_vis.png", 100)}
</p>

<h3>Induction Head Visualization and Colab Demo</h3>

<p>The demo we have created for the post shows the attention heads themselves on a small toy model.
    We can clearly see which head is the induction head based on the activations and the information
    contributions. You can find the link <a href="https://colab.research.google.com/drive/1qU72TDhNq7JBqHv0xY9xJg1aBFbGFWqZ?usp=sharing#scrollTo=jU4EPdSbLlYR">to the colab
        here</a></p>

<p>One of key resource to build a deep understanding of Induction Heads is the walkthrough by
    <a href="https://www.perfectlynormal.co.uk/blog-induction-heads-illustrated">Callum McDougall</a>.
    It is a really good visual representation of how the induction heads are formed with all of the underlying math.
    Callum is also an author of the fork of <a
        href="https://github.com/callummcdougall/CircuitsVis#subdirectory=python">CircuitVis</a>,
    a library that we use in our demo to demonstrate the behavior of the Attention heads.
</p>

<p>Now knowing what induction is and how it is formed, we need to now understand why they are so important,
    and why and how they contribute to the in-context learning. </p>

<h3>How Much Induction Heads Contribute to In-context Learning?</h3>

<p>
    <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>
    used these findings and presented <strong>six arguments</strong> with emperical evidence that
    <strong>induction heads contribute significantly to in-context learning</strong>.
</p>

<p>
      We highlight one of these arguments where the authors demonstrate a <strong>phase change</strong> early
      in training, during which <strong>induction heads are formed</strong> and <strong>in-context
        learning improves dramatically.</strong>
</p>

<p>{img("images/Phase_Change.png", 100)}

<p>
    During this phase change, the number of induction heads with prefix matching properties increases significantly. If
    we assume the former definition of the induction head as a prefix matching attention head,
    we see increase in heads that exhibit this property. And it is happening exactly at the time of the phase change.
</p>

<p>{img("images/Induction_Head_Formation.png", 100)}

      <p>
        <blockquote style="background-color: rgb(247, 181, 181);">
            <b>In-context learning score:</b> Difference in loss between the 50th and 500th token in 
            a context of length 512, averaged over dataset examples. 
        </blockquote>
      </p>
      <p>
        <blockquote style="background-color: rgb(247, 181, 181);">
            <b>Prefix matching score:</b>It is the average of all attention pattern entries attending 
            from a given token back to the tokens that preceded the same token in earlier repeats. 
        </blockquote>
      </p>

      
<p>To further confirm the impact of induction heads on in-context learning, the authors conducted another experiment to understand to what extent the induction heads contribute to the in-context learning score.
    For that while training and evaluating the results on the forward pass, they knocked removed the induction heads from computations and compared the in-context learning score with the ones where induction heads was present</p>
</p>
<p>
    They found out that most of the heads that have a positive in-context contribution are the induction heads.
</p>

<p>{img("images/Contribution_to_in_context_learning.png", 100)}
  <p>
  <blockquote style="background-color: rgba(179, 174, 174, 0.484);">
      <b>Nikita's Opinion:</b> The paper is acually was really easy and engaging to read, the biggest complication was to
      mechanistically understand the induction head itself. The visualizations are clean and do tell the story of the
      paper.
      On top of that, every argument has a table with to which models the support is best applied to, which shows the
      benefits and limitations for each argument. One thing I have just concerns about is to focus on the generalization
      of the in-context learning scoring methodology. While
      it does show the overall score, it is not much interperable on the concrete in-context tasks. Which is why I think
      the second paper, that does use the few-shot prediction scoring, is a great addition to the first one.
  </blockquote>
  </p>

  <h2>What needs to go right for an induction head?</h2>

  <p>
    Following the work of <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a>,
    Aditya K. Singh, a PhD student at University College London, Ted Moskovitz of Anthropic and Andrew Saxe, an expert in neurosciene and machine learning 
    built a <a href="https://arxiv.org/abs/2404.07129">framework</a> to identify:
      <ul>
        <li>
          Why do Induction Heads appear all of a sudden?
        </li>
        <li>
          Why is there more than one Induction Head? And how are they dependent on each other?
        </li>
        <li>
          What are the subciruits that enable them to emerge?
        </li>
      </ul>
  </p>
  
  <p>{img("images/pattern_preserving_framework.png", 100)}</p>

  <p>
    The high level idea is to intervene the attention head computations during training,
    thereby making it configurable to expose the activations or ablate them. As a result, 
    unlike prior work, this framework enabled targeted analysis such as 
    <i>"What gives rise to the phase change in induction head formation?"</i> by preserving(or clamping)
    the attention head computations. We will see how that works in the later part of this handbook.
  </p>

  <p>
    In order to study the <strong>copy</strong> and <strong>prefix-match</strong> behavior of Induction Heads, 
    the authors train and evaluate the model on <a href="https://github.com/brendenlake/omniglot">Omniglot</a>, 
    a synthetic handwritten dataset containing limited examples for each classication label. In simple terms, 
    the key idea is to evaluate if the model can show  <strong>copy</strong> and <strong>prefix-match</strong> 
    behavior in a controlled setting using limited data. 
  </p>

  <p>
    {img("images/omniglot.png", 30)}
  </p>

  <p>
    The training data setup shown in the figure above shows the input sequence, i.e., context + query, where 
    context includes example pairs of the handwritten alphabet and the label. The idea here is that through 
    in-context learning, the model should be able to <strong>match</strong> and <strong>copy</strong> the pattern 
    in the context to generate the target label.
  </p>

  <p>This <strong>framework</strong> coupled with the <strong>limited training data setup</strong> showed 
    various insights into the emerging behavior of induction heads and demonstrated the in-context learing (ICL) abilities.
    One <strong>key insight </strong>we would like to focus on is what gives rise to the <strong>"phase change"</strong> 
    by clamping the computations through this <strong>framework</strong>.
  </p>

  <p>
    Here is the high level idea of how the author's analyze the phase change:
    <ul>
        <li>
            First, define the computations that are of interest as shown in Figure(a).
        </li>
        <li>
            Next, use the <i>pattern ablation framework</i> to clamp individual or combination of 
            these computations. The clamping is done by specifying which computations we are interested
            in <strong>preserving</strong>. As show in the first figure, this includes
            a list of pairs (layer, head).
        </li>
        <li>
            Finally, compute and record the loss by clamping various combinations of
            the computations defined in the first step (Figure (c)).
        </li>
    </ul>
  </p>

  <p>
    {img("images/clamping.png", 100)}
  </p>
  
  <p>

  </p>

  <p>
    In Figure(c), the black curve shows the general <strong>"phase change"</strong> that occurs
    without any clamping similar to prior work. Each color codes loss curves in Figure(c) correspond 
    to the computation path take by the steps in Figure (a) and Figure (b). For example, the red 
    loss curve corresponds to clamping Step 1 (PT-attend) and Step 2 (PT-copy).
   
   <p>
    This analysis yields the formation of the following subciruits based on how the loss profile shifts
    with little to no phase change:
    <ul>
      <li>
        <span style="color: red;"><strong>Subcircuit A:</strong></span> Attending to previous token and
        copying it forward. Comprised of Step-1 PT-attend and Step-2 PT-Copy. 
      </li>
      <li>
        <span style="color: green;"><strong>Subcircuit B:</strong></span> Matching queries to keys in
        the induction head. Comprised of Step-3 Routing Q, Routing K and Step-4 IH-Match.
      </li>
      <li>
        <span style="color: rgb(12, 134, 240);"><strong>Subcircuit C:</strong></span> Copying of input label to output.
          Comprised of Step-3 Routing V and Step-5 IH-Copy.
      </li>
    </ul>
  </p>

  <p>
    <blockquote style="background-color: rgba(179, 174, 174, 0.484);">
        <b>Prajnan's take:</b> The key takeaway is that this paper provide a more systematic approach of
        why induction head emerges in In-context learning. The author's were able to emperically resolve 
        what does and doesn’t need to go right to form an induction head through their framework's intervening
        capability. That being said, this study does not guarantee  that induction heads are formed in a similar
        way for more complex tasks such as the "Chain-of-Thought" example shown in the Introduction. 
    </blockquote>
    </p>

  <h2>Colab Notebook and other Code Resources</h2>
    <p>
      Here is a <a href="https://colab.research.google.com/drive/1qU72TDhNq7JBqHv0xY9xJg1aBFbGFWqZ?usp=sharing#scrollTo=jU4EPdSbLlYR">Colab Notebook.</a>
      that can be used to identify induction heads by experimenting with different text inputs.
    </p>


{footer()}