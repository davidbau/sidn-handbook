<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>SAEs</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
  <div class="container-fluid">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item active">
      <a class="nav-link" href="/index.html">Probing</a>
     </li>
    </ul>
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
     </li>
    </ul>
  </div>
</nav>

<div class="container">
  <div class="row">
    <div class="col-2 position-fixed pt-5">
      <nav id="toc" data-toggle="toc"></nav>
    </div>
    <div class="col-2"></div>
    <main class="col-8">
      <h1 class="mt-5">SAEs</h1>
      <h5>October 3, 2024  • <em>Rahul Chowdhury, Satya Venkata Anudeep Ragata</em></h5>

      <h2>What are SAEs?</h2>
      <p>
        Sparse Autoencoders are a class of autoencoders that enforces sparsity in the encoding space and are used for
        investigating entangled concepts in neurons or their activations. This class of autoencoders have a single encoding layer of neurons with non-linear activation
        that outputs a set of coefficients that are sparse, and there is a decoding layer that takes in the the output of the encoder and multiplies them with the transpose of the linear embedding layer of 
        the encoder. [1]
      <p>
        Sparse Autoencoders are often used to estimate the properties of a neurons or their activation. Sparse autoencoders are used with an assumption that output of a single neuron will not carry the full representation of a property in its outputs. 
        Properties might be encoded in a neural network as a superposition of multiple neural activations. Thus in order to separate the concepts the original basis of feature space needs to be expanded. 
        Sparse Autoencoder first expands the basis of the neural activations into to more dimension. In order to learn the concepts for each input token we enforce sparsity in the encoded features of the Sparse Autoencoder since one token is unlikely to belong with all concepts. 
        This action makes the representation interpretable as we can now focus on the dictionary column that could be reponsible for a particular property. 
        Sparse Autoencoders could be used for both discovery and pruning through techniques that identify which columns in the dictionary light up more for a particular direction of change that could be of our interest. 
        Following identifications of the bases that correspond to a particular feature, that particular feature could be kept if that concept needs to be kept or pruned in order to suppress that from being represented in the output of a neural network.[1] 
      </p>


      <h2>Sparse Feature Circuits</h2>
      <p>

      </p>

      <h3>Linearity And Non-Linearity in Feature Space</h3>
      <p>
        f(king) - f(man) + f(woman) = f(queen) [2] is a classic example of an emergence of linearity in feature space of neural networks. 
        In this example, we could see the vector that shifts the representation from one gender to another gender is added to king which is a title representing a 
        status quo with entangled gender information, the output shifts to another title of the corresponding gender. There are two parts that needs to be understood. First, 
        the direction of change, and, second, the entanglement of information in an input token. Not all input token will have another property entangled in it, and not all shift in state will be linear. 
        We can have a look at a clock to develop an intution of the second property. If we want to shift from 1 to 2, there is a change of +1 in magnitude. Now pay attention to the vector change that brought this change from 1 to 2.
        Can we use the same vector to increment from 2 to 3 in the clock space? Not, right? This time the vector that we need to increment the same +1 would be different. This is an example portraying why might need a non-linear intervention to produce change in different context.   
      </p>

      <h3>Not All Features Are Linear</h3>
      <p>
        After performing linear intervention, we check if that intervention causes a drop in performance. This brings forth questions: Is the absence of the said 
        property causing the drop? Or was it a mere correlation that could have caused the drop, and were there other features entangled in the embedding that could have been the 
        reason for the original performance? A great way to test if our hypothesis that the said property that the feature is representing is 
        by simultaneously training another set of matrices in the same Iterative Null Space Projection algorithm but with random labels. 
        If the linear intervention finds the neutralized feature caused a drop in performance better than the random linear intervention [1], it could be concluded 
        that the said property might be representing the property and the information in that embedding was essential in determining the classes of an input data. 
      </p>
       <p>
         Language models have shown an amazing range of capabilities, but the question of what they learn from data has been a subject of debate. Do the models memorize a collection of surface statistics, or do they form certain interpretable representations of the process that generates the sequences they see?
       </p>
       <p>
         Li et al. (2023) proposed the existence of world representations in language models. The focus of the paper is to investigate the form of internal representations in the language model performing a specific task in confined settings. To investigate this world representation, the authors chose the popular game of Othello (try it <a href="https://cardgames.io/reversi/">here</a>). If we think of the board as the "world," then the game provides an appealing experimental testbed to explore world representations.
       </p>
       <!-- <p> Othello is a straightforward two-player game played on an 8×8 board. Players alternate placing black or white discs, starting with four discs (two black and two white) placed in the center. 
        Black moves first. Each move must flip at least one opponent’s disc by outflanking—or sandwiching—the opponent’s discs between two of the player’s own. The game ends when neither player has any legal moves left.
       </p> -->
       <p> The authors trained an 8-layer GPT model with an 8-head attention mechanism and a 512-dimensional hidden space on two sets of training data. The first set called "championship" is smaller in size (140K games) but consists of strategic moves by expert human players.
        The second dataset called "synthetic" is larger in size (2.4M games) consisting of legal but otherwise random moves. The goal was to study how much Othello-GPT can learn from pure sequence information with minimal inductive bias. The model was evaluated on the validation set 
        with error rate as the metric. Othello-GPT had 0.01% error rate on synthetic data, 5.17% error rate on championship data while the untrained model had 93.29% error rate.
       </p>
       <h4>Is the Model Memorizing?</h4>
       <p> To test whether the model was simply memorizing game sequences, the authors created a skewed dataset by removing all games that started with one of the four possible opening moves (specifically, they removed games starting with the move C5). 
        This eliminated 25% of all possible game openings. Despite never seeing these sequences during training, Othello-GPT still performed with a low error rate of 0.02% on them. This indicated that the model is not just memorizing but has learned to generalize from the patterns in the data.
       </p>
       <h3>Intervention Technique for Probing Internal Representations</h3>
       <p>Authors trained probes that predict the board state from the network's internal activations after a given sequence of moves. They found that linear probess had higher error rates than non-linear probes (2 layer MLP), indicating that the probe may be recovering a nontrivial representation of board state in the network's activations.</p>
       <p> <figure> <img  src="images/intervention.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
        <p>
            To evaluate whether Othello-GPT’s internal representations of the board state directly influence its move predictions, the authors introduced targeted interventions by altering activations across multiple layers, starting from an initial layer Lₛ. Utilizing gradient descent, they adjusted specific activations to transition the board state from B to B′ by modifying certain state elements. This precise alteration ensures that only a segment of the board state changes, thereby affecting the set of legal moves. The authors then assessed the success of these interventions by probing the modified activations and verifying if the model’s move predictions corresponded with the updated board state.
          </p>
        <p> <figure> <img  src="images/intervention_result.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
            <p>
                To systematically test the causal impact of internal board representations on Othello-GPT’s predictions, the authors developed a benchmark set comprising 2,000 intervention cases, divided into natural (reachable) and unnatural (unreachable) board states. For each test case, they modified the model’s internal activations to alter the board state and monitored the subsequent move predictions. By comparing these predictions with the expected set of legal moves, they calculated error rates. The results showed a significant reduction in errors from baseline levels, even in the unnatural subset, which underscores that the emergent internal representations play a causal role in shaping the model’s decision-making processes.
              </p>
        </p>
        <h3>Latent Saliency Maps</h3>
        <p> <figure> <img  src="images/latent_saliency_maps.png" alt="Othello-GPT" style="max-width:-70%; width:500px;" class="mx-auto d-block"></figure>
            <p>
                <p>
                    The latent saliency maps are generated by assessing how changes to each tile’s internal representation affect the model’s prediction for a specific move p. For each tile s on the board B, the authors apply the intervention technique to alter the representation of s and observe the resulting change in the prediction probability of p. They calculate the saliency S<sub>s</sub> as the difference between the original prediction probability p<sub>0</sub> and the new probability p<sub>s</sub> after intervention (S<sub>s</sub> = p<sub>0</sub> - p<sub>s</sub>). This saliency value indicates how much tile s influences the prediction of move p. By computing S<sub>s</sub> for all tiles and visualizing them, they create a latent saliency map that highlights the most influential tiles affecting the model’s decision.
                  </p>
      <h2>Code Resources</h2>
      <p> The "Emergent World Representations" paper has provided the code here: <a href="https://github.com/likenneth/othello_world">Othello World</a>. Aditionally, Neel Nanda has created a TransformerLens version of the Othello-GPT,which allows for inspecting each MLP neuron in the model, boosting the mechanistic interpretability. Try the Colab Notebook here: <a href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb">Othello GPT Colab</a>.<p>
      <p> The "Amnesic Probing" paper has provided the code here: <a href="https://github.com/yanaiela/amnesic_probing">Amnesic Probing</a></p>.
      <h2>References</h2>
      <ol>
        <li>Elazar, Yanai, et al. "Amnesic probing: Behavioral explanation with amnesic counterfactuals." Transactions of the Association for Computational Linguistics 9 (2021): 160-175.</li>
      </ol>
    </main>
  </div>
</div>
</body>
</html>
