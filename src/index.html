{header("Table of Contents", autotitle=False)}

<h1 class="mt-5">The Structure and Interpretation of Deep Networks</h1>

<p>A collaborative class handbook on current research methods in mechanistic interpretability.

<h2>Table of Contents</h2>

<h3>Introduction</h3>

<ul class="list-unstyled">
<li><a href="/history/">A History of Interpretable Machine Learning</a>
<li><a href="/salience/">From Salience Maps to Concept Neurons</a>
<li><a href="/survey/">An introduction to mechanistic interpretability</a>
</ul>

<h3>Understanding Representation</h3>
<ul class="list-unstyled">
<li><a href="/neurons/">Neurons</a>
<li><a href="/vectors/">Vectors</a>
<li><a href="/subspaces/">Subspaces</a>
<li><a href="/probing/">Probing</a>
<li><a href="/lenses/">Lenses</a>
<li><a href="/saes/">SAEs</a>
</ul>

<h3>Understanding Computation</h3>
<ul class="list-unstyled">
<li><a href="/formulation/">Formulation</a>
<li><a href="/induction/">Induction</a>
<li><a href="/association/">Factual Association</a>
<li><a href="/circuits/">Circuits</a>
<li><a href="/autocircuits/">Automatic Circuit Discovery</a>
<li><a href="/stages/">Transformer stages</a>
</ul>

<h3>Understanding Learning</h3>
<ul class="list-unstyled">
<li><a href="/grokking/">Grokking</a>
<li><a href="/structure/">Learning structure</a>
<li><a href="/unlearning/">Unlearning</a>
<li><a href="/icl/">In-context learning</a>
<li><a href="/attribution/">Data Attribution</a>
<li><a href="/rasp/">RASP</a>
</ul>


<h3>Understanding the World</h3>
<ul class="list-unstyled">
<li><a href="/universality/">Universality</a>
<li><a href="/humans/">Human knowledge</a>
<li><a href="/steering/">Steering</a>
<li><a href="/autointerp/">Automated interpretability</a>
<li><a href="/safety/">Safety</a>
</ul>

{footer()}
