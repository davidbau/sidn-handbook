<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evolution of LLM Stages across Model Sizes</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-50">
    <div class="max-w-4xl mx-auto px-4 py-8">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">Evolution of LLM Stages</h1>
            <div class="text-sm text-gray-600 flex items-center gap-2">
                <time>December 12, 2024</time>
                <span>â€¢</span>
                <div class="italic">
                    Rohit Gandikota, Alex Loftus, Philip, Ritik Bompilwar, Can Rager
                </div>
            </div>
        </header>
        <main class="prose prose-lg max-w-none">
            <section class="mb-12">
                <p class="text-gray-700 mb-6">
                    With Large Language Models (LLMs), we have been seeing the trend where larger models show emergent and better capabilities. In this project, we ask the question: "What are the added layers being used for by an LLM?". We first establish a model of mechanistic information flow in the models, we call "stages", and investigates the internal mechanisms of LLMs and how they change as model size increases.
                    Specifically, we will look at the following questions:
                </p>

                <ul class="list-disc pl-6 space-y-2 text-gray-700">
                    <li>What are the early layers in LLMs doing?</li>
                    <li>Where are the facts localized in models?</li>
                    <li>At what layers does the model transfer the knowledge of fact?</li>
                    <li>When does the model have enough information to know the answer?</li>
                    <li>Where is "processing" really happening in the models?</li>
                </ul>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Related Work</h2>
                <p class="text-gray-700">
                    This project builds upon a rich body of research dedicated to understanding the inner workings of LLMs.
                    Seminal works have explored attention mechanisms and their role in passing extracted information <span class="citation">[1]</span>, the emergence of factual knowledge
                    representation <span class="citation">[2]</span>, and the development of techniques to probe and analyze these models <span class="citation">[3]</span>. Our work contributes to this ongoing discourse by providing a comparative analysis of different LLM sizes and
                    applying these methods to unravel the complexities of their internal representations and processing.
                </p>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Methods</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens</h3>
                                <p class="text-gray-700">
                                    Visualize probability distribution over vocabulary tokens at each layer of the model, tracking prediction evolution through the network.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens Loss</h3>
                                <p class="text-gray-700">
                                    Examine loss changes at each layer to understand how the model refines its understanding and converges to final predictions.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Tracing</h3>
                                <p class="text-gray-700">
                                    Identify specific layers and components responsible for predictions through systematic input manipulation.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Attention Knockout</h3>
                                <p class="text-gray-700">
                                    Assess contribution of attention mechanisms by selectively disabling them at different layers.
                                </p>
                            </div>
                        </div>
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Intrinsic Dimension Analysis</h3>
                                <p class="text-gray-700">
                                    Quantify complexity of information representation at each layer to understand compression and expansion patterns.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Probes</h3>
                                <p class="text-gray-700">
                                    Measure accuracy of intermediate representations to reveal when sufficient information for task execution is acquired.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Intervention Effect</h3>
                                <p class="text-gray-700">
                                    Quantify impact of input feature alterations on predictions to identify causal relationships.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Representational Similarity Analysis</h3>
                                <p class="text-gray-700">
                                    Analyzing layer similarities across different model sizes reveals patterns in how representations evolve through the network.
                                </p>
                                
                                <details class="mt-4">
                                    <summary class="cursor-pointer font-bold text-lg">RSA Methodology</summary>
                                    <div class="mt-4 pl-4">
                                        <p class="text-gray-700">
                                            Representational Similarity Analysis (RSA) is a multivariate technique that compares diverse data types by examining shared patterns in their similarity matrices. Originally proposed by 
                                            <a href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full?utm_source=FWEB&utm_medium=NBLOG&utm_campaign=ECO_10YA_top-research" target="_blank">Kriegeskorte et al. [7]</a> 
                                            to link brain activity patterns to computational models and behavior, RSA works by subjecting the brain/model to different stimuli and extracting activity patterns. Pairwise correlations are computed to create a dissimilarity matrix (RDM), which captures the representational geometry of the data. This enables comparison across different data types and models.
                                        </p>
                                        
                                        <div class="mt-4">
                                            <div class="w-1/2 mx-auto">
                                                <img src="images/computing_RDMs.png" 
                                                     alt="RSA Computation Process" 
                                                     class="w-full rounded-lg mb-4"/>
                                                <p class="text-gray-600 text-sm italic text-center">
                                                    Computation of the Representational Dissimilarity Matrix (RDM).
                                                </p>
                                            </div>
                                        </div>
                                        
                                        
                                        <div class="mt-6">
                                            <h5 class="text-xl font-bold mb-3">How does RSA apply to LLMs?</h5>
                                            <p class="text-gray-700">
                                                <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Li et al. [8]</a> used RSA to study structural similarity between LLMs and neural responses. 
                                                <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Yousefi et al. [9]</a> used TSA to decode task-critical information from model representations without using parameterized probes. This technique helps compare representational geometry across LLM sizes and layers to reveal computational phases.
                                            </p>
                                        </div>
                                        
                                        <div class="mt-6">
                                            <h5 class="text-xl font-bold mb-3">Methodology for Layer-wise RSA in LLMs</h5>
                                            <p class="text-gray-700">
                                                The following methodology outlines a structured approach to analyze layer-wise representational structures in LLMs using RSA:
                                            </p>
                                            <ol class="list-decimal list-inside space-y-2 mt-2 text-gray-700">
                                                <li>
                                                    <strong>Input Processing:</strong> Start with a set of <span class="font-bold">N</span> sentences or questions from domains like biology and cybersecurity. Feed these inputs into LLMs (e.g., Llama 1B, 3B, 8B) to ensure diversity in both input complexity and model capacity [1].
                                                </li>
                                                <li>
                                                    <strong>Layer-Wise Activation Capture:</strong> Extract neural activations (hidden states) for each input at each layer <span class="font-bold">L</span>. This provides a layer-by-layer snapshot of how the model's internal representations evolve [2].
                                                </li>
                                                <li>
                                                    <strong>Representation Processing:</strong>
                                                    <ul class="list-disc list-inside ml-6 mt-1">
                                                        <li><strong>Mean Pooling:</strong> Average token-level activations per input to produce a single vector representation [3].</li>
                                                        <li><strong>Normalization:</strong> Optionally normalize these vectors to ensure that differences in vector magnitude do not unfairly influence similarity measurements.</li>
                                                    </ul>
                                                </li>
                                                <li>
                                                    <strong>RDM Generation:</strong> Compute pairwise similarities between the processed input vectors. Convert these similarities into dissimilarities (<code>RDM = 1 - similarity</code>) to form an <span class="font-bold">NÃ—N</span> Representational Dissimilarity Matrix for each layer [4].
                                                </li>
                                                <li>
                                                    <strong>Vectorizing RDMs:</strong> Extract the upper-triangular portion of each RDM and flatten it into a vector [5].
                                                </li>
                                                <li>
                                                    <strong>Layer Similarity Analysis:</strong> Compare these RDM-derived vectors across layers (and across models) to identify patterns and trends [6].
                                                </li>
                                            </ol>
                                        </div>
                                    </div>
                                </details>
                            </div>                        
                    </div>
                </div>

                <h2 class="text-3xl font-bold text-gray-900 mb-6">Our Hypothesis</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <p class="text-gray-700">
                            We hypothesis the following stages when an LLM processes a text to predict the next token:
                        <img src="images/stages.png" alt="LLM Stages we hypothesis" class="w-full rounded-lg"/>
                    </p>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Analysis</h2>
                
                <div class="space-y-8">
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Current LLM Performance</h3>
                        <img src="images/compare.png" style="max-width:70%" alt="LLM Trends" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            The current trend in large language models (LLMs) is to scale up existing models by increasing the number of parameters.
                            As models get larger, they seem to perform better on various tasks <span class="citation">[4, 5]</span>. It is interesting to see that the metrics show only 2 points increase from 70B to 405B. But this does not reflect the emergent capabilties in the 405B (huh .. i wonder what it says about the evaluations we do)
                        </p>
                    </div>


                    <!-- Logit Lens Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">What are the early layers doing?</h3>
                        <div class="flex flex-row gap-4 w-full">
                          <div class="w-1/2">
                            <img src="images/llama1b-logit.png" alt="Logits" class="w-full rounded-lg"/>
                          </div>
                          <div class="w-1/2">
                            <img src="images/llama8b-logit.png" alt="Logits" class="w-full rounded-lg"/>
                          </div>
                        </div>
                        <p class="text-gray-700">
                           We find an interesting artifact, smaller models repeat the input token as "next token" at early layers. Larger models, however, do not have this effect at early layers. At this point, we are unclear if this artifact is infact from the model or from the logit lens methodology itself. We aim to explore this further. 
                        </p>
                        <img src="images/logitloss.png" style="max-width:100%" alt="Logit loss" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            We scale up this experiment with wiki-10k prompts dataset <span class="citation">[6]</span> and compare the cross entropy between the logit lens distribution of the layer and the actual output distribution of the model. We see similar effect that larger models do not repeat the same input token at earlier layers 
                        </p>
                    </div>


                    <!-- Causal Tracing Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Where are the facts stored in LLMs?</h3>
                        <img src="images/causal.png" style="max-width:100%" alt="Causal" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            We follow the works on Meng et. al <span class="citation">[2]</span> to trace the knowledge storage in LLMs and specifically on models with varying sizes. We find that all the model sizes stores the facts almost at the same relative layer depths.
                        </p>
                    </div>

                    <!-- Attn Knockout Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">But, when is this extracted information transferred through tokens?</h3>
                        <img src="images/attn_knockout.png" style="max-width:100%" alt="Attn Knockout" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            Once the model extracts the facts or knowledge about the input prompt, it has to, at some point, transfer this information to the location where the model needs this information to make a decision of the next token. For this, we conduct an attention knockout experiment <span class="citation">[1]</span>, where we block attention from subject tokens to the final tokens at every layer and plot the change in the output distribution. We find that all the model sizes almost drop the accuracies at 40% through 70% of the layer depths. This suggests that these middle layers transfer information.
                        </p>
                    </div>

                    <!-- Intrinsic Dimensionality Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">When does the model use most of its dimensions to process information?</h3>
                        <img src="images/intrinsic.png" style="max-width:100%" alt="Attn Knockout" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            Okay, the model knows about the fact, transfers it to the token for prediction. Its a good time to ask a generic question - "how does the model process all this information at each layer?". For this, we conduct an intrisic dimensionality experiment and find that at the same 40% layer depth for all models, they start using high dimensions to process information. 
                        </p>
                    </div>
                    

                    <!-- Probes Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">After the processing, when does the model know about the answer?</h3>
                        <img src="images/probes.png" style="max-width:100%" alt="Probes" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                        Now that we know about the information processing, we now explore when the model has enough information to know the answer for next token. We do probing experiments similar to <span class="citation">[3]</span> and find a very interesting artifact!! Bigger models know the answer at the beginning layers while smaller models know the answer almost after the half way. 
                        </p>
                    </div>


                    <!-- Probes-Causal Compare Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">What? Is it true that larger models know answer way early?</h3>
                        <img src="images/probes_causal.png" style="max-width:100%" alt="Probes" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                        But this doesnot align with what we have seen so far! So we look at the causal importance of these layers. Are these layers really being used by the model? We found -  NO! There is a good gap of 7-9 layers between when the model has enough information to when it actually uses it.
                        </p>
                    </div>
                    
                
                    <!-- RSA Experiments -->
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">How do representations evolve across layers and model sizes?</h3>
                        <div class="space-y-6">
                            <div>
                                <h6 class="text-xl font-semibold mb-3">Layer-wise Similarity Analysis</h6>
                                <div class="flex flex-row gap-4 w-full">
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/1b.png" alt="Layer-wise RSA 1B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">Llama 1.2B</p>
                                    </div>
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/3b.png" alt="Layer-wise RSA 3B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">Llama 3.2B</p>
                                    </div>
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/8b.png" alt="Layer-wise RSA 8B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">Llama 8B</p>
                                    </div>
                                </div>
                            </div>
                    
                            <div>
                                <h6 class="text-xl font-semibold mb-3">Cross-Model Representation Comparison</h6>
                                <div class="flex flex-row gap-4 w-full">
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/8B_1B.png" alt="Cross-model RSA 8B vs 1B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">8B vs 1B Comparison</p>
                                    </div>
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/8B_3B.png" alt="Cross-model RSA 8B vs 3B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">8B vs 3B Comparison</p>
                                    </div>
                                    <div class="w-1/3 cursor-pointer" onclick="expandImage(this)">
                                        <img src="images/3B_1B.png" alt="Cross-model RSA 3B vs 1B" class="w-full rounded-lg hover:shadow-lg transition-shadow"/>
                                        <p class="text-sm text-center mt-2">3B vs 1B Comparison</p>
                                    </div>
                                </div>
                            </div>
                    
                            <p class="text-gray-700">
                                The RSA analysis reveals distinct processing stages across model sizes, with early layers showing high similarity, middle layers displaying diverse patterns, and later layers converging again. Cross-model comparisons demonstrate that different sized models develop analogous representations at similar relative depths, with stronger similarity between larger models.
                            </p>
                        </div>
                    </div>
                    
                    <script>
                    function expandImage(element) {
                        const modal = document.createElement('div');
                        modal.className = 'fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50';
                        
                        const img = element.querySelector('img').cloneNode(true);
                        img.className = 'max-h-[90vh] max-w-[90vw] object-contain';
                        
                        modal.onclick = () => modal.remove();
                        modal.appendChild(img);
                        document.body.appendChild(modal);
                    }
                    </script>
                    
                    

                    </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Discussion</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <p class="text-gray-700 mb-4">There are few observations in this work which we find very interesting and worth investigating more</p>
                    <ul class="list-disc pl-6 space-y-2 text-gray-700">
                        <li>Why is there a gap between probing accuracies and causal importance? - We believe this could be something to do with probes. We know probes can be sensitive to any seperable distributions. It could be that probes are catching this effect where the information just starts popping up. Another thought is how causal interventions work. There is a strong peak in importance, why does this happen? </li>
                        <li>How do different training datasets and objectives influence the development of computational phases within LLMs? - We have tested on very limited datasets and models. There is an oppurtunity to expand this set and observe</li>
                        <li>Can we leverage insights from RSA to develop more efficient and interpretable LLM architectures?</li>
                    </ul>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Citations</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <ol class="list-decimal list-inside space-y-2">
                        <li>Geva, M., Bastings, J., Filippova, K., & Globerson, A. (2023). Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767.</li>
                        <li>Meng, K., Bau, D., Andonian, A., & Belinkov, Y. (2022). Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35, 17359-17372.</li>
                        <li>Alain, G. (2016). Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644.</li>
                        <li>Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., ... & Ganapathy, R. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</li>
                        <li>Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., ... & Garg, S. (2024). Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.</li>
                        <li>NeelNanda/wiki-10k Â· Datasets at Hugging Face. (n.d.). https://huggingface.co/datasets/NeelNanda/wiki-10k</li>
                        <li>Kriegeskorte, N., Mur, M., & Bandettini, P. (2008). Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2, 4.</li>
                        <li>Li, J., Karamolegkou, A., Kementchedjhieva, Y., Abdou, M., Lehmann, S., & SÃ¸gaard, A. (2023). Structural similarities between language models and neural response measurements. arXiv. https://arxiv.org/pdf/2306.01930v2</li>
                        <li>Yousefi, S., Betthauser, L., Hasanbeig, H., MilliÃ¨re, R., & Momennejad, I. (2023). Decoding in-context learning: Neuroscience-inspired analysis of representations in large language models. arXiv. https://arxiv.org/abs/2310.00313</li>
                    </ol>
                </div>
            </section>



            
        </main>
    </div>
</body>
</html>