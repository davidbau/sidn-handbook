<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evolution of LLM Stages across Model Sizes</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-50">
    <div class="max-w-4xl mx-auto px-4 py-8">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">Evolution of LLM Stages</h1>
            <div class="text-sm text-gray-600 flex items-center gap-2">
                <time>September 5, 2024</time>
                <span>â€¢</span>
                <div class="italic">
                    Rohit Gandikota, Alex Loftus, Philip, Ritik Bompilwar, Can Rager
                </div>
            </div>
        </header>
        <main class="prose prose-lg max-w-none">
            <section class="mb-12">
                <p class="text-gray-700 mb-6">
                    With Large Language Models (LLMs), we have been seeing the trend where larger models show emergent and better capabilities. In this project, we ask the question: "What are the added layers being used for by an LLM?". We first establish a model of mechanistic information flow in the models, we call "stages", and investigates the internal mechanisms of LLMs and how they change as model size increases.
                    Specifically, we will look at the following questions:
                </p>

                <ul class="list-disc pl-6 space-y-2 text-gray-700">
                    <li>What are the early layers in LLMs doing?</li>
                    <li>Where are the facts localized in models?</li>
                    <li>At what layers does the model transfer the knowledge of fact?</li>
                    <li>When does the model have enough information to know the answer?</li>
                    <li>Where is "processing" really happening in the models?</li>
                </ul>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Related Work</h2>
                <p class="text-gray-700">
                    This project builds upon a rich body of research dedicated to understanding the inner workings of LLMs.
                    Seminal works have explored attention mechanisms and their role in passing extracted information <span class="citation">[1]</span>, the emergence of factual knowledge
                    representation <span class="citation">[2]</span>, and the development of techniques to probe and analyze these models <span class="citation">[3]</span>. Our work contributes to this ongoing discourse by providing a comparative analysis of different LLM sizes and
                    applying these methods to unravel the complexities of their internal representations and processing.
                </p>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Methods</h2>
                <div class="bg-white rounded-lg shadow-sm p-6 space-y-4">
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens</h3>
                                <p class="text-gray-700">
                                    Visualize probability distribution over vocabulary tokens at each layer of the model, tracking prediction evolution through the network.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Logit Lens Loss</h3>
                                <p class="text-gray-700">
                                    Examine loss changes at each layer to understand how the model refines its understanding and converges to final predictions.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Tracing</h3>
                                <p class="text-gray-700">
                                    Identify specific layers and components responsible for predictions through systematic input manipulation.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Attention Knockout</h3>
                                <p class="text-gray-700">
                                    Assess contribution of attention mechanisms by selectively disabling them at different layers.
                                </p>
                            </div>
                        </div>
                        <div class="space-y-4">
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Intrinsic Dimension Analysis</h3>
                                <p class="text-gray-700">
                                    Quantify complexity of information representation at each layer to understand compression and expansion patterns.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Probes</h3>
                                <p class="text-gray-700">
                                    Measure accuracy of intermediate representations to reveal when sufficient information for task execution is acquired.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Causal Intervention Effect</h3>
                                <p class="text-gray-700">
                                    Quantify impact of input feature alterations on predictions to identify causal relationships.
                                </p>
                            </div>
                            <div class="method-item">
                                <h3 class="font-semibold text-xl mb-2">Representational Similarity Analysis (RSA)</h3>
                                <p class="text-gray-700">
                                    Representational Similarity Analysis (RSA) is a multivariate technique that compares diverse data types by examining shared patterns in their similarity matrices. Originally proposed by 
                                    <a href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full?utm_source=FWEB&utm_medium=NBLOG&utm_campaign=ECO_10YA_top-research" target="_blank">Kriegeskorte et al.</a> 
                                    to link brain activity patterns to computational models and behavior, RSA works by subjecting the brain/model to different stimuli and extracting activity patterns. Pairwise correlations are computed to create a dissimilarity matrix (RDM), which captures the representational geometry of the data. This enables comparison across different data types and models.
                                </p>
                                <div class="mt-4">
                                    <img src="images/computing_RDMs.png" 
                                         alt="RSA Computation Process" 
                                         class="w-full rounded-lg mb-4"/>
                                    <p class="text-gray-600 text-sm italic text-center">
                                        Computation of the Representational Dissimilarity Matrix (RDM).
                                    </p>
                                </div>
                                <div class="mt-6">
                                    <h5 class="text-xl font-bold mb-3">How does RSA apply to LLMs?</h5>
                                    <p class="text-gray-700">
                                        <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Li et al.</a> used RSA to study structural similarity between LLMs and neural responses. 
                                        <a href="https://arxiv.org/pdf/2306.01930v2" target="_blank">Yousefi et al.</a> used TSA to decode task-critical information from model representations without using parameterized probes. This technique helps compare representational geometry across LLM sizes and layers to reveal computational phases.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Analysis and Diagrams</h2>
                
                <div class="space-y-8">
                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Current Trends in LLMs</h3>
                        <img src="/api/placeholder/800/400" alt="LLM Trends" class="w-full mb-4 rounded-lg"/>
                        <p class="text-gray-700">
                            The current trend in large language models (LLMs) is to scale up existing models by increasing the number of parameters.
                            As models get larger, they seem to perform better on various tasks [1, 2].
                        </p>
                    </div>

                    <div class="bg-white rounded-lg shadow-sm p-6">
                        <h3 class="text-2xl font-bold mb-4">Internal Processing Across Model Sizes</h3>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-4">
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 1" class="w-full rounded-lg"/>
                            <img src="/api/placeholder/400/300" alt="Logit Lens Visualization 2" class="w-full rounded-lg"/>
                        </div>
                        <p class="text-gray-700">
                            Logit lens visualizations provide insights into how the probability distribution over vocabulary tokens evolves as information 
                            propagates through the layers of different-sized LLMs [3-5].
                        </p>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <h3 class="text-2xl font-bold mb-4">Layer similarity across the model and inter-model.</h3>
                    <img src="/api/placeholder/800/400" alt="Layer Similarity Analysis" class="w-full mb-4 rounded-lg"/>
                    <p class="text-gray-700">
                        Analyzing layer similarities across different model sizes reveals patterns in how representations evolve through the network.
                        This comparison helps understand the internal dynamics and computational stages of language models.
                    </p>
                </div>
            </section>

            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Discussion</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <p class="text-gray-700 mb-4">The findings of this project raise several interesting questions and avenues for future research:</p>
                    <ul class="list-disc pl-6 space-y-2 text-gray-700">
                        <li>What are the precise mechanisms underlying the observed shifts in information processing as models scale up?</li>
                        <li>How do different training datasets and objectives influence the development of computational phases within LLMs?</li>
                        <li>Can we leverage insights from RSA to develop more efficient and interpretable LLM architectures?</li>
                    </ul>
                </div>
            </section>


            <section class="mb-12">
                <h2 class="text-3xl font-bold text-gray-900 mb-6">Citations</h2>
                <div class="bg-white rounded-lg shadow-sm p-6">
                    <ol class="list-decimal list-inside space-y-2">
                        <li>Geva, M., Bastings, J., Filippova, K., & Globerson, A. (2023). Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767.</li>
                        <li>Meng, K., Bau, D., Andonian, A., & Belinkov, Y. (2022). Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35, 17359-17372.</li>
                        <li>Alain, G. (2016). Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644.</li>
                    </ol>
                </div>
            </section>



            
        </main>
    </div>
</body>
</html>