<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/stages/index.html">Transformer Stages</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Grokking</h1>

<div<h5>October 28, 2024 • <em>Sri Harsha and Nikhil</em></h5>

<h2>Introduction</h2>

<p>
Grokking is a fascinating phenomenon where a model, after a
period of apparent stagnation, suddenly experiences a rapid and significant
improvement in performance. This abrupt transition is like <b>epiphany</b>
moment, where the model gains a deep understanding of its task, similar to a
human's moment of clarity after grappling with a complex concept. The term
"grok" originates from Robert Heinlein's<b>add the link here</b> science fiction, meaning to understand
something fully and deeply. Grokking challenges conventional expectations of
gradual learning, suggesting an alternative dynamic where models might initially
show little progress before suddenly achieving generalization.

<p>
The three papers that will be explored in this page, the first beeing Nanda et al 2023,
Progress measures for grokking via mechanistic interpretability<b>link peper 1</b> which shows grokking
phenomenon on small transformers trained on modular addition tasks. The second paper Murty et al 2023,
is about Grokking of Hierarchical Structure in Vanilla Transformers<b>link paper 2</b> which explores hierarchically
generalization. The third Y. Hu et all 2024 Delays, Detours, and Forks in the Road:
Latent State Models of Training Dynamics<b>link paper 3</b>, explores how randomness in data order and initialization
impacts model training dynamics and outcomes.


<h2>Emergent behaviours</h2>
<p>
Emergent behaviors in machine learning models often arise unexpectedly when models
are scaled up, leading to new capabilities such as in-context learning and
chain-of-thought prompting. However, these behaviors also present risks, including
overfitting and unintended consequences in real-world applications. For instance,
Pan et al<b>link paper</b>. discuss the risks associated with recommender systems, in case of YouTube
since engineers couldn’t really measure the SWB\(Subjective well being\) they use other
metrics to measure and optimize  like click-through rates or watch-time. These objectives
don’t make a good estimate of SWB so this led YouTube to overemphasize watch-time and harm
user satisfaction and also recommended extreme political \(controversial\) content to users.

<p>
The emergence of these behaviors is surprising to researchers because they appear
suddenly and are not easily predictable based on traditional metrics.
Ganguli et al<b>link paper</b>. highlight the paradox that while scaling laws predict performance
improvements, the specific new capabilities that emerge are unpredictable. In fact there
could still be unknow capabilities which are not triggered yet or discovered.
Barak et al. further note that sudden phase changes can occur even without
changes in data size, emphasizing the need for metrics that can detect these
transitions before they happen. Understanding these emergent behaviors requires
novel approaches beyond conventional statistical methods, as they can have
significant implications for both model performance and societal impact.



<h2>Modular addition experiment<h2>
<img src="images/paper1_img1.png" style="max-width:30%; width:300px;" class="mx-auto d-block">

<p>
In this experiment they study modular addition, where a model takes inputs
a, b ∈ {0, . . . , P −1} for some prime P and predicts their sum c mod P. Small transformers trained
with weight decay on this task consistently exhibit grokking. They reverse engineered the weights of these transformers
and find that they perform this task by mapping the inputs onto a circle and performing addition
on the circle. Specifically, we show that the embedding matrix maps the inputs a, b to sines
and cosines at a sparse set of key frequencies wk. The attention and MLP layers then combine
these using trigonometric identities to compute the sine and cosine of wk\(a + b\), and the
output matrices shift and combine these frequencies.

<p>
They found four lines of evidence to differentiate the phases of grokking.

<blockquote class="blockquote">
<p>Network weights exhibit a periodic structure
<p>Neuron-logit WL which is the last learnable param matrix. Which transforms hidden activations into logits. This can be well approximated using sinusoidal functions of key frequencies. MLP activations are projected on to these sinusoidal functions produce trigonometric identities from the neurons.
<p>The MLP and attention heads can be approximated well using a 2nd degree polynomials of trigonometric functions of a single frequency.
<p>Ablating key frequencies reduces model performance but the other 95% has improves the performance.
</blockquote>



</main>
</div>
</div>
</body>
</html>
