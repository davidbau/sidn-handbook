{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwZrZMlwVS-Q"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "pip install ninja 2>> install.log\n",
        "git clone https://github.com/SIDN-IAP/global-model-repr.git tutorial_code 2>> install.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA-ClCJQVS-S"
      },
      "outputs": [],
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/tutorial_code')\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_oEu9DqVS-T"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "Import torch, netdissect, matplotlib, and set up some things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqmvjDALVS-U"
      },
      "outputs": [],
      "source": [
        "import torch, os, matplotlib.pyplot as plt\n",
        "from netdissect import nethook, imgviz, show, segmenter, renormalize, upsample, tally, pbar\n",
        "from netdissect import setting\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_grad_enabled(False) # not training anything!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaWOFmxAVS-U"
      },
      "source": [
        "# Load some data\n",
        "\n",
        "**ds** is a dataset of pictures of places.\n",
        "\n",
        "It is the validation set from the Places365 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRFE9DF3VS-V"
      },
      "outputs": [],
      "source": [
        "ds = setting.load_dataset('places', 'val')\n",
        "iv = imgviz.ImageVisualizer(224, source=ds, percent_level=0.99)\n",
        "show(iv.image(ds[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axy8QdM2VS-V"
      },
      "source": [
        "# Load a pretrained classifier model\n",
        "\n",
        "**model** is a pretrained VGG classifier that classifies scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWcOLa2gVS-V"
      },
      "outputs": [],
      "source": [
        "model = setting.load_vgg16()\n",
        "model = nethook.InstrumentedModel(model)\n",
        "model.cuda()\n",
        "renorm = renormalize.renormalizer(source=ds, target='zc')\n",
        "ivsmall = imgviz.ImageVisualizer((56, 56), source=ds, percent_level=0.99)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRsSMD3VS-W"
      },
      "source": [
        "## Warmup: look at the model\n",
        "\n",
        "### Exercise 1.\n",
        "* How many layers does the VGG network have?  `print(model)` will show them.\n",
        "* What is the fully qualified name of the last convolutional layer?  Look at `model.layernames()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQUqyKDMVS-W"
      },
      "outputs": [],
      "source": [
        "# print(model) etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z-hog4yVS-W"
      },
      "source": [
        "In the short example below:\n",
        "* **indexes** is a list of dataset indexes to retrieve.  `i` indicates a dataset index, and `j` is an index into the indexes array.\n",
        "* **batch** is a `12 x 3 x 224 x 224` tensor that stacks up twelve RGB 224x224 images from the dataset.\n",
        "* When we run `model(batch.cuda())`, it scores every image for every class, making a `12 x 365` tensor of scores.\n",
        "* Then `.max(dim=1)` finds the maximum of 365 scores for each image; it returns a (scores, indexes) tuple.\n",
        "* **preds** is a tensor of 12 highest scoring class indexes (each one a number up to 365) predicted by the model.\n",
        "* `iv.image(batch[j])` turns the jth `3 x 224 x 224` tensor into a PIL image for display.\n",
        "* `ds.classes[ds[i][1]]` shows the human ground-truth label for the `i`th image in the dataset.\n",
        "\n",
        "So the loop shows a set of twelve images, each with the dataset label and the model prediction. Scene classification is difficult and sometimes ambiguous; nevertheless the model does reasonably well.\n",
        "\n",
        "### Exercise 2.\n",
        "\n",
        "(Optional.)  Explore the data set, and the model's predictions on the data.\n",
        "\n",
        "* Change the **indexes** array to contain a few `soccer_field` and `baseball_field` images within the data set, that is, a set of indexes `i` for which `ds[i][1]` matches the class number for either of those classes.  A tip: `ds.classes.index('soccer_field')` is 310 and the index of `baseball_field` is 42.\n",
        "* Can you find a baseball field image that is incorrectly classified as a soccer field?\n",
        "\n",
        "Scrutinize the images, and consider how the model might be making its predictions.  What it might be looking for within the images to tell the difference between baseball and soccer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "3ez5wygaVS-W"
      },
      "outputs": [],
      "source": [
        "target_class = ds.classes.index('soccer_field')\n",
        "print(target_class)\n",
        "indexes = range(100, 112)\n",
        "batch = torch.stack([ds[i][0] for i in indexes])\n",
        "_, preds = model(batch.cuda()).max(1)\n",
        "show([[\n",
        "    iv.image(batch[j]),\n",
        "    'label: ' + ds.classes[ds[i][1]],\n",
        "    'pred: ' + ds.classes[preds[j]],\n",
        "    i,\n",
        "] for j, i in enumerate(indexes)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PUxAhPrVS-X"
      },
      "source": [
        "For reference, below is the typical way we evaluate a classifier: check its accuracy on the dataset.  While this gives us a global view of the model (e.g. 53% accuracy), it doesn't show us what the model does internally at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4MvcKgXVS-X"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    correct = 0\n",
        "    tested = 0\n",
        "    for imagebatch, labelbatch in pbar(torch.utils.data.DataLoader(ds, batch_size=100)):\n",
        "        modelpreds = model(imagebatch.cuda()).max(1)[1]\n",
        "        # print(modelpreds.cpu(), labelbatch)\n",
        "        correct += (modelpreds.cpu() == labelbatch).sum() # fixme\n",
        "        tested += len(labelbatch)\n",
        "    print('%d correct out of %d' % (correct, tested))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S4BBjYWVS-X"
      },
      "source": [
        "## Examine raw unit activations.\n",
        "\n",
        "This bit of code shows the output of individual filters in a layer directly.\n",
        "\n",
        "It shows each filter in two ways.  First, it overlays a region of high activation on the image; and on the right, it shows a heatmap of filter activations.\n",
        "\n",
        "### Exercise 3: look at individual activations.\n",
        "\n",
        "* Change the layername, and compare the activation patterns in early convolutional layers, like conv2_1, with later ones, like conv5_3.\n",
        "* Change j to select an image with people in it, and look at all 512 filters of conv5_3.\n",
        "* Do any filters seem to be sensitive to particular body parts in this image?  Which ones?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "8ybJqHodVS-Y"
      },
      "outputs": [],
      "source": [
        "layername = 'features.conv5_3'\n",
        "model.retain_layer(layername)\n",
        "model(batch.cuda())\n",
        "acts = model.retained_layer(layername).cpu()\n",
        "show([\n",
        "    [\n",
        "        [ivsmall.masked_image(batch[imagenum], acts[imagenum], unitnum)],\n",
        "        [ivsmall.heatmap(acts[imagenum], unitnum, mode='nearest')],\n",
        "        'unit %d' % unitnum\n",
        "    ]\n",
        "    for unitnum in range(acts.shape[1])\n",
        "    for imagenum in [6]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF_IzJwMVS-Y"
      },
      "outputs": [],
      "source": [
        "upfn = upsample.upsampler(\n",
        "    target_shape=(56, 56),\n",
        "    data_shape=(7, 7),\n",
        ")\n",
        "\n",
        "def flatten_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    hacts = upfn(acts)\n",
        "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
        "\n",
        "rq = tally.tally_quantile(\n",
        "    flatten_activations,\n",
        "    dataset=ds,\n",
        "    sample_size=1000,\n",
        "    batch_size=100)\n",
        "    #cachefile='results/rq_cache.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGTsv5zMVS-Y"
      },
      "source": [
        "### Exercise 4: look at the ranges of activations\n",
        "\n",
        "The loop above collects statistics of each filter over a sample of 1000 images.\n",
        "What are typical values of the filters?  How often do they fire?\n",
        "\n",
        "* Plot median (0.5 quantile) values of each filter in conv5_3.\n",
        "* Compare the 0.5, 0.8, 0.9, and 0.99 quantiles for each filter.\n",
        "* Do different units activate in different ranges from one another?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFyDHUkyVS-Z"
      },
      "outputs": [],
      "source": [
        "plt.plot(rq.quantiles(0.9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQph-9-vVS-Z"
      },
      "source": [
        "### Exercise 5: examine images that maximize each unit\n",
        "\n",
        "The loop below identifies the images, out of a sample of 1000, that cause each filter to activate strongest.  The current code tallies up images that maximize the mean activation of the filter over the image.\n",
        "* (Optional) Change the code to find images that maximize the maximum acvitation across the image instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOpTlIbyVS-Z"
      },
      "outputs": [],
      "source": [
        "sample_size = 1000\n",
        "\n",
        "def max_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    return acts.view(acts.shape[:2] + (-1,)).max(2)[0]\n",
        "\n",
        "def mean_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    return acts.view(acts.shape[:2] + (-1,)).mean(2)\n",
        "\n",
        "topk = tally.tally_topk(\n",
        "    mean_activations,\n",
        "    dataset=ds,\n",
        "    sample_size=sample_size,\n",
        "    batch_size=100,\n",
        "    cachefile='results/cache_mean_topk.npz'\n",
        ")\n",
        "\n",
        "top_indexes = topk.result()[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9OX1_BVS-Z"
      },
      "source": [
        "Below is a loop that runs the model for each of the top-activating images for a particular unit (12), and then shows where that unit activates within the images.\n",
        "\n",
        "* Change the unit number to examine the behavior of different units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "oec6iAw-VS-Z"
      },
      "outputs": [],
      "source": [
        "show.blocks([\n",
        "    ['unit %d' % u,\n",
        "     'img %d' % i,\n",
        "     'pred: %s' % ds.classes[model(ds[i][0][None].cuda()).max(1)[1].item()],\n",
        "     [iv.masked_image(\n",
        "        ds[i][0],\n",
        "        model.retained_layer(layername)[0],\n",
        "        u)]\n",
        "    ]\n",
        "    for u in [12]\n",
        "    for i in top_indexes[u, :20]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5boeAdwVS-Z"
      },
      "source": [
        "The following code automates the above process for all the units, collecting a visualization of top images for each unit in the network.\n",
        "\n",
        "* Examine `unit_images[u]` for various units `u`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeFrT4qrVS-Z"
      },
      "outputs": [],
      "source": [
        "def compute_activations(image_batch, label_batch):\n",
        "    image_batch = image_batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts_batch = model.retained_layer(layername)\n",
        "    return acts_batch\n",
        "\n",
        "unit_images = iv.masked_images_for_topk(\n",
        "    compute_activations,\n",
        "    ds,\n",
        "    topk,\n",
        "    k=5,\n",
        "    num_workers=10,\n",
        "    pin_memory=True,\n",
        "    cachefile='results/cache_top10images.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF7nMdgAVS-Z"
      },
      "source": [
        "## Loading a segmentation model.\n",
        "\n",
        "To systematically identify units that match semantic concepts better or worse, we can find units that align well with the predictions of a semantic segmentation network.\n",
        "\n",
        "The code below runs and displays segmentations on a batch of images.\n",
        "\n",
        "**seg** is a tensor that assigns a set of semantic segmentation labels to every pixel of an image.  `seg[i, 0]` shows the 0th label for each pixel of the `i`th image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5BpO8GTVS-a",
        "outputId": "fb543554-e5dd-4448-8eda-82f990e5ecfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/tutorial_code/netdissect/upsegmodel/models.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "segmodel, seglabels, segcatlabels = setting.load_segmenter('netpqc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An2mtgQUVS-a"
      },
      "outputs": [],
      "source": [
        "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
        "show([(iv.image(batch[i]), iv.segmentation(seg[i, 0]),\n",
        "            iv.segment_key(seg[i,0], segmodel))\n",
        "            for i in range(len(seg))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKfqEixDVS-a"
      },
      "source": [
        "The code below finds the intersections between every unit's 99th percentile activation, and every segmentation class identified by the semgenter.  It can take a few minutes to run, so you can reduce the sample size if you do not want to wait."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luBYse3kVS-a"
      },
      "outputs": [],
      "source": [
        "level_at_99 = rq.quantiles(0.99).cuda()[None,:,None,None]\n",
        "\n",
        "def compute_selected_segments(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    hacts = upfn(acts)\n",
        "    iacts = (hacts > level_at_99).float() # indicator where > 0.99 percentile.\n",
        "    return tally.conditional_samples(iacts, seg)\n",
        "\n",
        "condi99 = tally.tally_conditional_mean(\n",
        "    compute_selected_segments,\n",
        "    dataset=ds,\n",
        "    sample_size=sample_size,\n",
        "    cachefile='results/cache_condi99.npz')\n",
        "\n",
        "iou99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
        "iou99.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml8Sb5XYVS-a"
      },
      "source": [
        "The code below sorts the units, showing the units with the best match to a segmentation class first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGZQFFQVS-a"
      },
      "outputs": [],
      "source": [
        "iou_unit_label_99 = sorted([(\n",
        "    unit, concept.item(), seglabels[concept], bestiou.item())\n",
        "    for unit, (bestiou, concept) in enumerate(zip(*iou99.max(0)))],\n",
        "    key=lambda x: -x[-1])\n",
        "for unit, concept, label, score in iou_unit_label_99[:20]:\n",
        "    show(['unit %d; iou %g; label \"%s\"' % (unit, score, label),\n",
        "          [unit_images[unit]]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDtKeviVS-a"
      },
      "source": [
        "Which types of patterns are detected across the whole representation?\n",
        "\n",
        "The following code counts up segmentation classes that are matched by units, and plots the histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUKilK_nVS-a"
      },
      "outputs": [],
      "source": [
        "iou_threshold = 0.04\n",
        "unit_label_99 = [\n",
        "        (concept.item(), seglabels[concept],\n",
        "            segcatlabels[concept], bestiou.item())\n",
        "        for (bestiou, concept) in zip(*iou99.max(0))]\n",
        "labelcat_list = [labelcat\n",
        "        for concept, label, labelcat, iou in unit_label_99\n",
        "        if iou > iou_threshold]\n",
        "import IPython\n",
        "IPython.display.SVG(setting.graph_conceptcatlist(labelcat_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipkrotrgVS-b"
      },
      "source": [
        "To delete cached results and run things again, you can remove and recreate results directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmHTDddSVS-b"
      },
      "outputs": [],
      "source": [
        "# rm -rv '../results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lUhytI6VS-b"
      },
      "outputs": [],
      "source": [
        "# mkdir '../results'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBhbaQqpVS-b"
      },
      "source": [
        "# Network Dissection\n",
        "\n",
        "Network dissection is a systematic method for finding and measuring single units (convolutional filters) that match meaningful semantic concepts in a vision network.\n",
        "\n",
        "Our fundamental question is this: how does the network decompose the task of understanding what an image is?  Does it identify any features that are understandable to a human?\n",
        "\n",
        "Simply running this notebook will provide a simple dissection, but at each step, there are exercises for modifying the notebook to find more interesting results.\n",
        "\n",
        "## About the netdissect library\n",
        "\n",
        "The netdissect library contains several useful packages for inspecting internals of a vision network.\n",
        "Here are packages that we use in this notebook:\n",
        "\n",
        " * **nethook** wraps any pytorch model, adding the ability to record or modify any internal computation.\n",
        " * **imgviz** provides ImageVisualizer, that collects together several useful image visualization functions.\n",
        " * **show** arranges nested arrays of PIL images and strings as nicely formatted HTML for display in a notebook.\n",
        " * **segmenter** provides an interface and a pretrained implementation for a semantic segmentation network.\n",
        " * **tally** gathers statistics over a dataset, based on your function to compute features for each datum.\n",
        " * **renormalize** deals with conversions between the zoo of RGB encoding scales typically seen in vision data.\n",
        " * **upsample** provids simple functions for resampling grid data at higher or lower resolutions.\n",
        " * **pbar** is a progress bar.\n",
        "\n",
        "These will be explained a bit more in the exercises below.  Of course you can always run `help(object)` for a bit more information on most things in the library.  For this tutorial we also have a package **settting**, which automatically downloads and creates datasets and pretrained models that we will be looking at."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJezBO7oVS-b"
      },
      "source": [
        "## About pretrained models and data\n",
        "\n",
        "Here are some fixed variables that we define up-front for all the objects that we will be inspecting in this tutorial.\n",
        "\n",
        "* **model** is the network we will look at.  It is a VGG convolutional network, trained to classify images of scenes into one of 365 place categories.  We wrap `model` as a `nethook.InstrumenteModel` so that we can easily retrieve and modiry its internal activations.\n",
        "* **ds** is a small held-out sample from the Places dataset that was used to train the model; each entry is a pytorch tensor representing an image, and an integer representing the class.  A pytorch dataset can be derefernces like an array, so `ds[35]` is a pair `(x, y)` where `x` is a tensor containing RGB image data for a scene and `y` is an integer for the human-given class label.  Classnames are available as `ds.class[y]`.\n",
        "* **renorm** is a function that renormalizes RGB data from the staistically-based scaling used in `ds` to a simple `[-1...1]` range scale.\n",
        "* **segmodel** is a semantic segmentation network trained to recognize a large vocabulary of objects and parts of objects within scenes.  We will use it as a reference, to see if there are any internal filters that approximately match the same concepts.\n",
        "* **seglabels** are human-readable names for the numerical segmentation classes.\n",
        "* **iv** is an image visualization object that visualizes 2d data such as images and heatmaps as 224x224 images.\n",
        "* **ivsmall** is another visualization object, but outputs smaller 56x56 images.\n",
        "* **resfile** is a function that generates filenames in a results subdirectory that we will use for caching data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}