<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/association/index.html">Factual Association</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Transformer Stages</h1>

<div><h5>October 24, 2024 • <em>Achyut Kowshik, Alex Loftus</em></h5>

<h2>Introduction</h2>

<p>
Large Language Models (LLMs) built on transformer architectures are thought to process information in a
series of stages, each responsible for progressively refining their understanding of the input. These
stages allow the model to transition from extracting basic features to forming complex, abstract representations
that ultimately drive meaningful predictions. Understanding how these stages operate provides insights
into the internal workings of the model during inference.

<p>
The three papers in this section each explore different aspects of these stages. The first paper,
<em>"The Remarkable Robustness of LLMs: Stages of Inference?"</em>, finds four characteristic stages in the progression of transformer language models, moving from detokenization and feature extraction to modeling the output probability distribution.

<p>
The second paper, <em>"Emergence of a High-Dimensional Abstraction Phase in Language Transformers"</em>, uses a technique the authors developed previously to model the intrinsic dimensionality of a representation, e.g., the dimensionality of the manifold that it lives on, to discover a distinct phase change in the middle layers of transformers characterized by a spike in the intrinsic dimensionality.

<p>
The third paper, <em>"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"</em>,
delves into how transformers manage conflicting information by balancing factual and counterfactual knowledge.
It reveals that different parts of the model compete, depending on the context, to influence the final output.
This interplay provides insight into how transformers decide which information should take precedence during inference,
highlighting the roles played by different stages of the model.

<h2>The Remarkable Robustness of LLMs</h2>

<p>
In 2024, researchers discovered that large language models exhibit surprising resilience to architectural modifications. Through systematic experiments deleting and reordering layers, they found that models retain up to 95% of their original accuracy even after significant interventions.

<p>
The work, led by Vedang Lad, Wes Gurnee, and Max Tegmark at MIT, studied five state-of-the-art language models ranging from 6.5 to 8 billion parameters. Their investigation revealed four universal stages of inference that characterize transformer processing.

<img src="images/overview_remarkable.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p>
The stages progress from early layers that integrate local context to final layers that refine predictions:

<p>
<b>1. Detokenization:</b> In the initial layers, the model transforms raw token representations into coherent entities. This stage proves highly sensitive to layer deletion, suggesting it performs essential contextualization.

<img src="images/remarkable_fig4.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p>
<b>2. Feature Engineering:</b> The early-to-middle layers build feature representations through attention-heavy computation. This phase shows remarkable robustness to both deletion and reordering of layers.


<img src="images/remarkable_fig6.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p>
<b>3. Prediction Ensembling:</b> The middle-to-late layers mark a transition to MLP-heavy computation, where semantic features convert into concrete next-token predictions through specialized components.

<p>
<b>4. Residual Sharpening:</b> The final layers refine predictions by suppressing irrelevant features and calibrating confidence. Like the first stage, these layers prove sensitive to modification.


<p>
The researchers traced this robustness to the residual connections in transformer architectures. These connections allow models to form ensembles of relatively shallow computational paths, avoiding strong dependencies on any single layer.

<p>
To quantify the robustness, they performed two types of interventions across layers:

<ol>
  <li><strong>Layer Deletion:</strong> Removes individual layers while preserving residual connections.</li>
  <li><strong>Layer Swapping:</strong> Exchanges the order of adjacent layers.</li>
</ol>
<p>The middle layers prove remarkably robust to both interventions:</p>

<img src="images/layer_swap_drop_remarkable.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p> The paper concludes with two case studies: one on attention heads responsible for capturing the context of a token for appropriate prediction (contributing to the detokenization and feature engineering stages) and a second providing evidence of an ensembling effect in prediction and supression neurons (contributing to the prediction ensembling and residual sharpening stages).</p>


<h2>Emergence of a High-Dimensional Abstraction Phase in Language Transformers</h2>

<p>
Previous research has shown that language models compress their inputs onto low-dimensional manifolds. In this paper, research by Emily Cheng and colleagues at Universitat Pompeu Fabra revealed that this compression follows a distinctive pattern. They found a critical phase where representations temporarily expand to higher dimensionality - marking the transition to abstract linguistic processing.

<p>This phase corresponds to a higher intrinsic dimensionality in the residual stream representations. The phase doesn't appear in the presence of random text and is nonexistent in untrained models; the layer in which it appears correlates with LM quality; and and the high-dimensional representations of different networks can predict each other - but they cannot predict either the initial representation of the input nor the representations in later layers.</p>

<p>
To determine the dimensionality of the manifold, the researchers used a metric they had developed in a previous study called the Generalized Ratios Intrinsic Dimension Estimator (GRIDE):
<p>
<img src="images/gride.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">
</p>


Using GRIDE, they analyzed representation geometry across layers in five large language models. Despite the models' 4096-dimensional hidden states, they found that representations generally lie on manifolds of dimension O(10). The peak they found occured in the middle layers for all models:

<img src="images/all_models_smoothed.png" style="max-width:100%; width:800px;" class="mx-auto d-block">

<p>
The peak exhibits several key properties that suggest its functional importance:

The high-dimensional phase emerges during training, disappears when processing random text, and enables cross-model prediction - indicating it reflects learned linguistic structure rather than architecture.

<p>They also used a technique developed to quantify the change in information content between different representations based nearest neighbors rankings, which they called \( \Delta \): </p>

<img src="images/delta.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p>
The researchers used \( \Delta \) to compare the relative change in information content as representations passed through the model, demonstrating a peak that exactly coincided with the increase in intrinsic dimensionality. Using probing experiments, they showed that during this peak, surface-level features become less recoverable, while semantic and syntactic properties become more accessible.

<img src="images/llama_peak_shaded_average_id_surface_tasks.png" style="max-width:100%; width:1000px;" class="mx-auto d-block">

<p>
Critically, better language models show earlier and higher-dimensional peaks. This correlation suggests the high-dimensional phase plays an essential role in linguistic processing - providing an expanded representational space where abstract features can be computed before compression into predictions.

<h2>Competition of Mechanisms</h2>

<p>
This <a href="https://arxiv.org/pdf/2402.11655">paper</a> began as Francesco Ortu's Master's thesis, conducted under the joint guidance of MPI and Area Science Park.
The team, including Zhijing Jin, Professors Alberto Cazzaniga, Bernhard Schölkopf, Diego Doimo, and Professor Mrinmaya Sachan,
collaborated closely to develop and refine the technical ideas and experiments that underpin this work.

<p>
This paper addresses how transformer-based language models handle conflicting information— specifically, how they choose between factual and counterfactual knowledge during inference.
<br><br>

<div style="display: flex; justify-content: space-around;">
  <img src="images/comp-mech.png" style="max-width:100%; width:500px;" class="mx-auto d-block">
  <img src="images/two-mechanisms.png" style="max-width:100%; width:400px;" class="mx-auto d-block">
</div>
<br><br>
<h3>Macroscopic View: Layers and Token Positions </h3>
<p>
The authors analyzed how information flows through different layers and token positions to support either the factual or counterfactual mechanism. They found that, in the initial layers, factual information is primarily represented in the subject position, while counterfactual information is encoded in the attribute position. Later layers, especially the last position, are responsible for integrating this information to produce the final prediction, with the counterfactual mechanism often becoming dominant. This indicates that the competition between mechanisms becomes more pronounced in deeper layers, with the counterfactual mechanism winning out in most cases.
<br><br>
<img src="images/macro-view.png" style="max-width:100%; width:600px;" class="mx-auto d-block">

<h3>Intermediate View: Attribution to Attention and MLP Blocks </h3>
<p>
The study further broke down the contributions of attention and MLP blocks in each layer. The findings showed that attention blocks played a more significant role in the competition between factual and counterfactual mechanisms compared to MLP blocks. The attention blocks were the primary contributors to pushing the model towards the counterfactual mechanism, particularly in the later layers. In contrast, early layers had almost no influence on the competition, highlighting that both factual and counterfactual information is mainly developed and contrasted at later stages.

<img src="images/inter-view.png" style="max-width:100%; width:600px;" class="mx-auto d-block">

<h3>Microscopic View: Contribution of Individual Attention Heads </h3>
<p>
At a microscopic level, the authors examined individual attention heads to understand their roles in the competition of mechanisms. They found that a few specialized heads significantly contribute to either promoting the factual mechanism or enhancing the counterfactual mechanism. Interestingly, both types of heads tended to focus on the attribute position, but those promoting factual recall operated by suppressing the counterfactual token rather than directly enhancing the factual one. This suggests a nuanced approach where factual information is retained by indirectly weakening competing signals.
<br><br>
<img src="images/micro-view.png" style="max-width:100%; width:600px;" class="mx-auto d-block">

<h3>Intrinsic Intervention by Attention Modification </h3>
<p>
The authors performed an intrinsic intervention by modifying the attention values of specific attention heads to enhance the factual recall mechanism. By selectively up-weighting the attention values at certain positions, they were able to dramatically increase the model's factual recall from 4% to 50%. This modification only involved a few entries in the attention matrix (2 in GPT2-small and 3 in Pythia-6.9B), showing that small, targeted interventions can significantly alter the balance between competing mechanisms, thereby improving the reliability of factual responses.
<br><br>
<img src="images/attention-mod.png" style="max-width:100%; width:350px;" class="mx-auto d-block">


<h2>Code Examples</h2>

Here are some notebooks from the authors of the papers we discussed above-
<br><br>
<p>
For the first paper:
<ul>
    <li><a href="https://colab.research.google.com/github/vdlad/Remarkable-Robustness-of-LLMs/blob/main/notebooks/attention_prev5.ipynb"><strong>Mean attention to prev 5 tokens</strong></a><br></li>
    <li><a href="https://colab.research.google.com/github/vdlad/Remarkable-Robustness-of-LLMs/blob/main/notebooks/entropy_calculation.ipynb"><strong>Entropy through Logit Lens</strong></a><br></li>
    <li><a href="https://colab.research.google.com/github/vdlad/Remarkable-Robustness-of-LLMs/blob/main/notebooks/neuron_counter.ipynb"><strong>Neuron counter</strong></a><br></li>
</ul>
Rest of the code can be found on the authors' <a href="https://github.com/vdlad/Remarkable-Robustness-of-LLMs">Github</a>
<br><br>
For the third paper:
<ul>
    <li><a href="https://colab.research.google.com/github/francescortu/comp-mech/blob/refactor/notebooks/experiments.ipynb"><strong>Comp Mech Demo</strong></a><br></li>
</ul>


</main>
</div>
</div>
</body>
</html>
