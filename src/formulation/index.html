{header('Formulation')}

<h5>October 4, 2024 â€¢ <em>Alex Loftus, Michael Ripa, Dmitrii Troitskii</em></h5>

<h3>scoring rubric</h3>
<p>
  <ul>
    <li>+1: introduction poses the question, links the papers
    <li>+1: authors are identified and described
    <li>+1: background context is described, key previous works cited
    <li>+1: diagrams show the main methods or results visually
    <li>+1: contrast is drawn, identifying differences between the papers
    <li>+1: Code examples are included, demonstrating the methods
    <li>+1: Discussion with your own opinions, questions, or perspective
  </ul>
</p>

<p><em>Introduction</em> that poses the question, links the papers, describes authors and background context
introduction rubric:
<ul>
  <li>+1: crisply describes the problem being solved. Cites and links the main papers.
  <li>+1: authors - identifies the authors, says a few words about who they are, where they worked, what they are known for.
  <li>+1: background - give context. how the problem was preivously tackled, why the problem emerged as an interesting topic in the history. Use the paper's own literature review to identify related older papers. Then cite a few of the key older papers.
</ul>
</p>

<h2>A Need for Formulation</h2>
<p>rubric details:
  <ul>
    <li>+1: diagram - describe methods in the paper visually
    <li>+1: contrast - don't just say what each paper does, but focuses on the differences between them.
    <li>+1: Code - help your readers by giving them concrete code samples to run. Colab notebooks with all the installation steps.
    <li>+1: discussion - go beyond the facts and share your own perspective.
  </ul>
</p>
<p>Example of a <a href="/neurons/">link</a> to the neurons page</p>
<p>Example of a <em>formula</em>: \( h_i \in \mathbb{R} \) and an <em>image</em>
{img("images/bigdavid.jpg")}
<p><b>bold text</b>
<p>this is <a href="https://transformer-circuits.pub/2022/toy_model/index.html">an inline link</a></p>

<h2><a href="https://arxiv.org/abs/2405.00208">a primer on transformer language models</a></h2>
<p>
This primer was published recently and serves as a source of unified mathematical notation for techniques described in the interpretability literature. We will focus on everything prior to section 3, since the rest of the paper focuses on specific applications. </p>

<p>
The authors are Javier Ferrando, Marta Ruiz Costa-jussa, Gabriele Sarti, and Arianna Bisazza. Javier is at Universitat Politecnica de Catalunya in Barcelona. He spent the summer at Berkeley, I spoke to him a few times while I was in Barcelona this summer. </p> <p>The final author, Marta Ruiz Costa-jussa, is a research scientist at FAIR Meta in Paris; she was a researcher at Ramon y Cajal / ERC at the Universitat Politecnica de Catalunya for seven years prior. She's been working on safety and gender bias in LLMs recently. </p><p>The other authors, Gabriele Sarti and Arianna Bisazza, are in the netherlands (University of Groningen).
</p>

<h4>Notes</h4>
<p>First: The primer paper is confusing to read for mathematicians because notationally, the vector \( x_i \) is considered a row vector, and consequently, linear transformations on it are right-multiplications.</p>

\[
  \mathbf{x_i} W = \begin{bmatrix}
    x_{1} & x_{2} & \cdots & x_{d}
  \end{bmatrix} 
  \begin{bmatrix}
    \top & & \top \\
    \vec{w_1} & \cdots & \vec{w_m} \\
    \perp & & \perp
  \end{bmatrix}
\]

<p>\( W \) has \( d \) rows. You can think of this matrix multiplication as scaling each row by its corresponding weight in \( \mathbf{x}_i \), and then summing vertically along the row axis. The output row vector is in \( m \) dimensions.</p>
<p> Second: There is a mathematical trick that shows we can lightly ignore the layer norm when building intuition. This is because the weights of the affine transformation can be folded into the following linear layer recursively until the end of the residual stream. </p>
<h3> Prediction as a sum of component outputs </h3>
<p> {img("images/primer_prediction.png", width="60%")} </p>
<p>We get to logits by passing the last token at the end of the residual stream \(x_n^L\) through the unembedding matrix \( W_u \).
We can think of the output of the residual stream as a sum of the attention head outputs and of the feedforward layers. Writing this out leads to an interesting algebraic manipulation: we can simply move the unembedding matrix \(W_U\) inside either sum. This leads us to the conclusion that, at any moment along the residual stream, we can see what's happening in logit space. This is the idea behind, for instance, the logit lens.
</p>
<p> In practice, attention and feedforward layer are calculated sequentially. The attention operation happens per-head, with each head independently summed into the residual stream, and the feedforward layer is applied independently to each position in the sequence.</p>

<p> Notation is as follows: </p>

<p> <ul>
  <li>Initial embeddings \( \mathbf{X} \in \mathbb{R}^{n \times d} \) are row vectors corresponding to a row of \( W_E \in \mathbb{R}^{\mathcal{V} \times d} \) </li>
  <li> The final layer residual stream state is projected into the vocabulary space via \( W_U \in \mathbb{R}^{d \times \mathcal{V}} \). </li>
  <li>An intermediate layer representation \( \mathbf{x}_i^l \) is the representation for token \( i \) in layer \( l \). \( \mathbf{x} \) is \( \mathbf{x}_i^0 \). </li>
  <li> \( \mathbf{X}^l \in \mathbb{R}^{n \times d} \) represents activations for layer \( l \) </li>
  <li> \( \mathbf{X}_{\leq i}^l \) is the layer \( l \) representation up to position \( i \). </li>
</ul></p>

<h3>Attention Block</h3>
{img("images/primer_attn_output.png", width="60%")}
<p>Attention is the communication between residual stream tokens. The outputs of the attention operation are written into the residual stream.</p>

<p>The attention operation at the \( l_{th} \) layer is simply the sum of all attention head outputs at that layer into the residual stream.

<h4> How does attention modify a single token? </h4>
{img("images/primer_attn.png", width="60%")}
<p> Zoom into token \( i \). The attention operation for this token at layer \( l \), head \( h \), works as follows:
  <ol>
    <li> Loop through every other token in the sequence \( x_j \) for \( j \leq i \). </li>
    <li> Read each \( x_j \) into a space that can be writeable into the residual stream</li>
    <li> Weight that \( x_j \) by its attention with \( x_i \), and then write it into the residual stream  </li>
  </ol>
<p>

<h4> How is the attention score calculated? </h4>
{img("images/primer_a_i.png", width="60%")}
</p>
<p>The attention vector \(\mathbf{a}_i^{l,h} \) has attention values \( a_{i,j}^{l,h} \). Each are calculated by</p>
  <ol>
    <li> Reading the corresponding \( x_j \) into a space such that its dot product with \( x_i \) is monotonic with its attention score: \( W_{QK}^h x_j^{\top l-1} \) </li>
    <li> Taking the actual dot product of the two to get an attention value, and then normalizing by the temperature \( \sqrt{d_k} \) 
    <li> Softmaxing the result to get values between 0 and 1 </li>
    </li>
    
  </ol>


<h3> Feedforward Block </h3>
<p>{img("images/primer_ffn.png", width="60%")}</p>
<p>The feedforward block is applied independently to each position in the sequence. It is a two layer neural network with a nonlinearity in between. We generally project into a higher dimensional space (often \( 4d \)) and then back into the residual stream. </p>


<h4> Pattern Detecting </h4>
  {img("images/primer_ffn_kv.png", width="60%")}
<p> We can think of the columns of \( W_{in}^l \) acting as pattern detectors over the activations.

Each neuron defines how strongly its corresponding row vector in \( W_{out} \) should be activated. The result is a linear combinationn of row vectors of \( W_{out} \).
</p>


\[
\text{FFN} = g\left( \begin{bmatrix} x_{1} & x_{2} & \cdots & x_{d} \end{bmatrix} 
  \begin{bmatrix}
    \top & & \top \\
    \mathbf{w_u} & \cdots & \mathbf{w_{d_{\text{FFN}}}} \\
    \perp & & \perp
  \end{bmatrix} \right)
 \begin{bmatrix}
  \leftarrow \mathbf{w_u} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w_{d_{\text{FFN}}}} \rightarrow
  \end{bmatrix} \\
\]
\[ =
  \begin{bmatrix} n_{1} & \cdots & n_{d_{\text{FFN}}} \end{bmatrix}   
  \begin{bmatrix}
  \leftarrow \mathbf{w_u} \rightarrow \\
  \vdots \\
  \leftarrow \mathbf{w_{d_{\text{FFN}}}} \rightarrow
  \end{bmatrix} \\


\]


<!-- <h3> attention block </h3>
<ul>
 <li> \( \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) = \sum_{j \leq i} a_{i,j}^{l,h} \mathbf{x}_j^{l-1} \mathbf{W}_{OV}^{l,h} \). Importantly here, \( \mathbf{W}_{OV}^{l,h} = \mathbf{W}_O^{l,h} \mathbf{W}_V^{l,h} \), and \( \mathbf{W}_O^{l,h} \in \mathbb{R}^{d \times d_h} \) and \( \mathbf{W}_V^{l,h} \in \mathbb{R}^{d_h \times d} \).
<li> \( \mathbf{a}_i^{l,h} = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_Q^{l,h} (\mathbf{X}_{\leq i}^{l-1} \mathbf{W}_K^{l,h})^\top}{\sqrt{d_k}} \right) = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_{Qk}^{h} \mathbf{X}_{\leq i}^{(l-1)\top}}{\sqrt{d_k}} \right) \)
<li>\( \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) = \sum_{h=1}^H \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) \)</li>
<li> Output of the attention block: \( \mathbf{x}_i^{\text{mid},l} = \mathbf{x}_i^{l-1} + \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) \)</li>
</p>

<h3> MLP block </h3>
<ul>
  <li> \( \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) = g(\mathbf{x}_i^{mid,l} \mathbf{W}_{in}^l) \mathbf{W}_{out}^l \), where \( g(\cdot) \) is a nonlinearity, and \( \mathbf{W}_{in}^l \in \mathbb{R}^{d \times d_{\text{ffn}}} \) and \( \mathbf{W}_{out}^l \in \mathbb{R}^{d_{\text{ffn}} \times d} \).</li> 
  <li> \( \mathbf{x}_i^{\text{out},l} = \mathbf{x}_i^{\text{mid},l} + \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) \)</li>
</ul> -->

<p></p>
<h2>a mathematical framework for transformer circuits</h2>
<h2>analyzing transformers in embedding space</h2>


<blockquote class="blockquote">
  this is a blockquote
</blockquote>

<p>align latex:

\[ \hat{x} = W^T h = \begin{bmatrix}
\phantom{0} & w_1^T & \phantom{0} \\ \hline
\phantom{0} & w_2^T & \phantom{0} \\ \hline
 & \vdots & \\ \hline
\phantom{0} & w_n^T & \phantom{0}
\end{bmatrix} h
=
\begin{bmatrix}
w_1^T h \\
w_2^T h \\
\vdots \\
w_n^T h
\end{bmatrix}
\]

<h2>Discussion Questions</h2>

<ul>
<li>Why is it that concatenating attention head outputs and then running the result through the output weight matrrix \( W_o \) is the same as running through \( W_{OV} \) inside attention heads? (page 3 of primer, bottom of the page)
<li>second question
</ul>

{footer()}
