{header('Formulation')}

<h5>October 4, 2024 â€¢ <em>Alex Loftus, Michael Ripa, Dmitrii Troitskii</em></h5>

<h3>scoring rubric</h3>
<p>
  <ul>
    <li>+1: introduction poses the question, links the papers
    <li>+1: authors are identified and described
    <li>+1: background context is described, key previous works cited
    <li>+1: diagrams show the main methods or results visually
    <li>+1: contrast is drawn, identifying differences between the papers
    <li>+1: Code examples are included, demonstrating the methods
    <li>+1: Discussion with your own opinions, questions, or perspective
  </ul>
</p>

<p><em>Introduction</em> that poses the question, links the papers, describes authors and background context
introduction rubric:
<ul>
  <li>+1: crisply describes the problem being solved. Cites and links the main papers.
  <li>+1: authors - identifies the authors, says a few words about who they are, where they worked, what they are known for.
  <li>+1: background - give context. how the problem was preivously tackled, why the problem emerged as an interesting topic in the history. Use the paper's own literature review to identify related older papers. Then cite a few of the key older papers.
</ul>
</p>

<h2>A Need for Formulation</h2>
<p>rubric details:
  <ul>
    <li>+1: diagram - describe methods in the paper visually
    <li>+1: contrast - don't just say what each paper does, but focuses on the differences between them.
    <li>+1: Code - help your readers by giving them concrete code samples to run. Colab notebooks with all the installation steps.
    <li>+1: discussion - go beyond the facts and share your own perspective.
  </ul>
</p>
<p>Example of a <a href="/neurons/">link</a> to the neurons page</p>
<p>Example of a <em>formula</em>: \( h_i \in \mathbb{R} \) and an <em>image</em>
{img("images/bigdavid.jpg")}
<p><b>bold text</b>
<p>this is <a href="https://transformer-circuits.pub/2022/toy_model/index.html">an inline link</a></p>

<h2>a primer on transformer language models</h2>
<p>
  author background:
  Javier Ferrando is at Universitat Politecnica de Catalunya, in Barcelona. He actually spent the summer at Berkeley, I spoke to him a few times while I was in Barcelona this summer. The final author, Marta Ruiz Costa-jussa, is a research scientist at FAIR Meta in Paris; she was a researcher at Ramon y Cajal / ERC at the Universitat Politecnica de Catalunya for seven years prior. She's been working on safety and gender bias in LLMs recently. The other authors, Gabriele Sarti and Arianna Bisazza, are in the netherlands (University of Groningen).
</p>

<p>
<h3> basic formulation: </h3>
<ul>
  <li>Embeddings \( \mathbf{x} \in \mathbb{R}^d \) are row vectors corresponding to a row of \( W_E \in \mathbb{R}^{\mathcal{V} \times d} \), where \( \mathcal{V} \) is the vocabulary size. The final layer residual stream state is projected into the vocabulary space via the unembedding matrix \( W_U \in \mathbb{R}^{d \times \mathcal{V}} \).
  <li>An intermediate layer representation \( \mathbf{x}_i^l \) is the representation for token \( i \) in layer \( l \). \( \mathbf{x} \) is \( \mathbf{x}_i^0 \). The matrix \( \mathbf{X}^l \in \mathbb{R}^{n \times d} \) represents all the intermediate layer representations for layer \( l \), where \( n \) is the sequence length. \( \mathbf{X}_{\leq i}^l \) is the layer \( l \) representation up to position \( i \).
  <li> layer norm trick: the weights of the affine transformation can be folded into the following linear layer, so we can pretty much ignore it (from a math perspective) until the end.
</ul>
<h3> attention block </h3>
<ul>
 <li> \( \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) = \sum_{j \leq i} a_{i,j}^{l,h} \mathbf{x}_j^{l-1} \mathbf{W}_{OV}^{l,h} \). Importantly here, \( \mathbf{W}_{OV}^{l,h} = \mathbf{W}_O^{l,h} \mathbf{W}_V^{l,h} \), and \( \mathbf{W}_O^{l,h} \in \mathbb{R}^{d \times d_h} \) and \( \mathbf{W}_V^{l,h} \in \mathbb{R}^{d_h \times d} \).
<li> \( \mathbf{a}_i^{l,h} = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_Q^{l,h} (\mathbf{X}_{\leq i}^{l-1} \mathbf{W}_K^{l,h})^\top}{\sqrt{d_k}} \right) = \text{softmax} \left( \frac{\mathbf{x}_i^{l-1} \mathbf{W}_{Qk}^{h} \mathbf{X}_{\leq i}^{(l-1)\top}}{\sqrt{d_k}} \right) \)
<li>\( \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) = \sum_{h=1}^H \text{Attn}^{l,h} (\mathbf{X}_{\leq i}^{l-1}) \)</li>
<li> Output of the attention block: \( \mathbf{x}_i^{\text{mid},l} = \mathbf{x}_i^{l-1} + \text{Attn}^l (\mathbf{X}_{\leq i}^{l-1}) \)</li>
</p>

<h3> MLP block </h3>
<ul>
  <li> \( \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) = g(\mathbf{x}_i^{mid,l} \mathbf{W}_{in}^l) \mathbf{W}_{out}^l \), where \( g(\cdot) \) is a nonlinearity, and \( \mathbf{W}_{in}^l \in \mathbb{R}^{d \times d_{\text{ffn}}} \) and \( \mathbf{W}_{out}^l \in \mathbb{R}^{d_{\text{ffn}} \times d} \).</li> 
  <li> \( \mathbf{x}_i^{\text{out},l} = \mathbf{x}_i^{\text{mid},l} + \text{FFN}^l (\mathbf{x}_i^{\text{mid},l}) \)</li>
</ul>

<p></p>
<h2>a mathematical framework for transformer circuits</h2>
<h2>analyzing transformers in embedding space</h2>


<blockquote class="blockquote">
  this is a blockquote
</blockquote>

<p>align latex:

\[ \hat{x} = W^T h = \begin{bmatrix}
\phantom{0} & w_1^T & \phantom{0} \\ \hline
\phantom{0} & w_2^T & \phantom{0} \\ \hline
 & \vdots & \\ \hline
\phantom{0} & w_n^T & \phantom{0}
\end{bmatrix} h
=
\begin{bmatrix}
w_1^T h \\
w_2^T h \\
\vdots \\
w_n^T h
\end{bmatrix}
\]

<h2>Discussion Questions</h2>

<ul>
<li>first question
<li>second question
</ul>

{footer()}
