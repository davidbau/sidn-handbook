{header('Automated Interpretability')}

<div>
  <h5>December 3, 2024 â€¢ <em>David Atkinson, Sheridan Feucht</em></h5>

  Is it possible to automate the process of interpreting units in a neural network? 
  Maybe, if we turn LLMs back on themselves. The papers for today's class focus on 
  how we can use LLMs to explain the inputs/outputs of units within a network. 

  <h2>
    MAIA: A Multimodal Automated Interpretability Agent
  </h2>
  TODO (David)
  
  <h2>
    Language Models Can Explain Neurons in Language Models
  </h2>
  TODO (David)

  <h2>
    Explaining Black Box Text Modules in Natural Language With Language Models
  </h2>
  TODO (Sheridan)


  <h2>Code Resources</h2>
  <p>TODO</p>


  {footer()}
</div>
