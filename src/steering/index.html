<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The Development of Activation Steering</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Mathjax -->
<script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">The Development of Activation Steering</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">

<h1 class="mt-5">Steering</h1>

<h5>November 24, 2024 â€¢ <em>Alex Loftus, Rohit Gandikota, Dmitrii Troitskii</em></h5>

<p>
We now examine the emergence of activation steering as a method for controlling large language models. This technique has developed rapidly over the past year, with three key papers establishing its theoretical foundations and practical applications.
</p>

<p>
Our examination focuses on how activation steering evolved from initial insights about model representations to a practical technique for controlling model behavior. We trace this development through three papers that build on each other, each making distinct contributions to our understanding of how to manipulate transformer internal states.
</p>

<p>
The first paper, "Understanding How CodeLLMs (Mis)Predict Types" by Lucchetti and Guha, demonstrates the potential of activation steering through its application to type prediction in code models. The second, "Inference-Time Intervention" by Li et al., formalizes steering techniques and shows their effectiveness for enhancing model truthfulness. The third paper, "Representation Surgery" by Singh et al., provides theoretical foundations by characterizing optimal steering functions and establishing formal guarantees.
</p>

<h2>Understanding How CodeLLMs (Mis)Predict Types</h2>

<p>This paper was published by Francesca Luchetti and Arjun Guha, both at Northeastern University. Francesca is a PhD student in Arjun's lab, and Arjun is a professor. Their group focuses on coding LLMs.</p>


<p>
Lucchetti and Guha investigate type prediction in code language models, focusing on Python and TypeScript. They investigate what happens when a model mispredicts a type, which models can be made to do by applying semantics-preserving edits to code. 

<img src="images/type_definition.png" style="max-width:60%; width:1200px;" class="mx-auto d-block">

Starting with a correct type prediction, we can apply semantics-preserving edits to the prompt until the type prediction fails. By using activation steering, the authors are able to steer the model back to the correct prediction - making models more robust against semantically irrelevant prompt features. Interestingly, steering vectors computed from Python code are effective at correcting TypeScript mispredictions, and vice-versa.

Their key insight is that models often contain robust mechanisms for tasks that can be accessed through activation steering, even when surface behavior suggests otherwise.
</p>

<p><b>Methodology</b> They built steering datasets for the 1B and 7B parameter StarCoderBase models, both of which are trained to fill-in-the-middle. The dataset is pulled from github rather than using prompt templates; ManyTypes4Py was used for Python type prediction, which features code from 5,382 Python projects that use type annotation. They collect triplets \( (x+, x-, t) \), where t is the type of the prompt x+, and x- is a semantically-equivalent prompt that is misclassified. x- is obtained by renaming the variable, removing the type annotation, renaming a user-defined type, and renaming a builtin type. </p>

<p>A brief sidenote to explain fill-in-the-middle (FIM): FIM is a training technique where coding LLMs learn to predict missing segments between a prefix and suffix. For example, given <code>x: </code> and <code> = "hello world"</code>, the model learns to predict <code>str</code> as the type annotation. While inference typically uses left-to-right generation, this training helps models better understand code structure.

<img src="images/fim_example.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

The relevance here is that FIM is used to ensure that the last token is always the type annotation, which is the target of the steering task.</p>

<p>They then take these steering triplets, apply forward pases to both the positive and negative sample, and save residual stream activations on the last token. Steering vectors are computed per-layer with the following formula:

<img src="images/steering_formula.png" style="max-width:60%; width:1200px;" class="mx-auto d-block">


where \(\mathcal{D}\) is the set of steering triplets, \(A_l (x) \) is the residual stream activation at some layer \(l\) and prompt \(x\). The intuition behind this equation is that the distance between positive and negative pairs in activation space encodes the transformation for steering towards the correct type prediction. The last token in all prompts is <code>fim_middle</code>, the type which is being predicted.</p>

<p>The model is steered by adding the steering vector \(t_l \) to the last token's residual stream at layer \(l\).</p>


<p><b>Experiments and Results</b> They find which layers are most effective for steering using a layer ablation method, evaluate the accuracy of steering on different datasets, and explore patching both on single layers and sets of adjacent layers.

<img src="images/steering_accuracy.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

</p>

<p>Once they determine which layers are most effective for steering, they apply it and report results in both python and typescript. It works well pretty much across the board compared to steering with a random vector.</p>
<img src="images/steering_results.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

<p><b></b></p>

<p>
An interesting finding is that steering vectors computed from one programming language transfer effectively to another - suggesting models learn fundamental, language-agnostic representations of programming concepts.

<img src="images/steering_cross_language.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">

They also compare steering to fine-tuning. It turns out that steering is approximately as effective as fine-tuning:

<img src="images/steering_finetuning.png" style="max-width:60%; width:1200px;" class="mx-auto d-block">

</p>

<h2>Making Language Models More Truthful: A Deep Dive into Inference-Time Intervention</h2>

<h3>The Problem: Truth-Telling in Language Models</h3>
    <p>Large Language Models (LLMs) have become increasingly sophisticated, but they still struggle with a fundamental issue: they don't always tell the truth, even when they "know" the correct answer. This fascinating paper from Harvard researchers introduces a novel technique called Inference-Time Intervention (ITI) to address this problem.</p>

    <p>The key insight here is that LLMs might have an internal representation of truth, even when they produce falsehoods in their outputs. This suggests an interesting interpretability question: can we identify and manipulate these truth-related representations to make models more truthful?</p>

    <img src="images/li_main_figure.png" style="max-width:50%; width:1200px;" class="mx-auto d-block" alt="Example of LLaMA responses with and without intervention">
    <!-- <div class="figure-caption">Example responses from LLaMA with and without intervention, showing how ITI can improve truthfulness.</div> -->

<h3>Finding "Truth" in Model Activations</h3>
    <p>The authors take a novel approach by using linear probes to identify which attention heads in the model are most associated with truthful outputs. What they found is remarkable - certain attention heads appear to specialize in truth-telling, with some achieving up to 83.3% accuracy in predicting whether an output will be truthful.</p>

    <img src="images/li_probes.png" style="max-width:100%; width:1200px;" class="mx-auto d-block" alt="Probe accuracies across attention heads">
    <!-- <div class="figure-caption">Linear probe accuracies across different attention heads in LLaMA-7B, showing specialized heads for truth-telling.</div> -->

    <p>The geometry of these truth representations is particularly interesting. When visualizing the activation space, they found that truthful and false statements form distinct clusters, suggesting that the model internally encodes some notion of truth versus falsehood.</p>

<h3>The Intervention Method</h3>
    <p>Rather than modifying model weights or fine-tuning, ITI works by identifying the most truth-associated attention heads and calculating a "truth direction" in their activation space. During inference, activations in these heads are shifted along their truth directions. This intervention is remarkably minimal and computationally inexpensive.</p>

    <img src="images/li_manipulation.png" style="max-width:50%; width:1200px; alt="Diagram of ITI mechanism">
    <!-- <div class="figure-caption"> Schematic of how ITI works during inference, showing the manipulation of attention head activations.</div> -->


<h3>Detailed Results: Breaking Down the Impact</h3>
    
    <p>The results of ITI are particularly interesting when we look at how it performs across different types of questions. Figure 5 from the paper shows a fascinating breakdown of improvement across various subcategories in the TruthfulQA dataset.</p>

    <img src="images/li_main_results.png" style="max-width:100%; width:1200px; alt="Performance across subcategories">
    <!-- <div class="figure-caption"> True*informative scores split across subcategories on LLaMA-7B, sorted by the difference between baseline and ITI.</div> -->

    <!-- <div class="results-highlight"> -->
    <p>What's particularly striking about these results is the consistency of improvement across categories. The intervention doesn't just work well for one type of question - it shows improvements across most subcategories, from health-related questions to historical facts. For instance:</p>

    <p>In the "Health" category, the model's truthfulness improved dramatically, correcting common misconceptions about medical treatments and dietary advice. Questions about law and economics also saw substantial improvements, suggesting that the intervention helps the model be more precise with factual information rather than defaulting to common but incorrect beliefs.</p>

    <p>Perhaps most intriguingly, the improvement isn't uniform across categories. Some categories show much larger gains than others, but there's no clear pattern suggesting why certain types of questions are more amenable to improvement. This hints at something fundamental about how different types of knowledge are encoded in the model's weights.</p>
    <!-- </div> -->

    <p>The robustness of these improvements is further validated by the model's performance on out-of-distribution datasets. When tested on Natural Questions, TriviaQA, and MMLU, the intervention continued to show improvements, albeit smaller ones. This suggests that the "truth" directions identified by ITI capture something fundamental about how the model represents factual information, not just specific patterns in the training data.</p>

    
    <p>The impact of this intervention is substantial. On the TruthfulQA benchmark, ITI improves truth-telling from 32.5% to 65.1% on Alpaca. What's particularly impressive is that this improvement comes with minimal computational overhead and requires only hundreds of examples to calibrate - a stark contrast to expensive fine-tuning approaches.</p>

<h3>My Technical Take</h3>
    <p>From an interpretability perspective, this paper opens up fascinating questions about how truth is represented in neural networks. The fact that truth can be represented as linear directions in activation space is surprising and suggests that models might learn to encode truth values in a more structured way than previously thought.</p>

    <p>The strong localization of truth-related computations to specific attention heads is quite interesting. This aligns with other work showing attention head specialization, but raises questions about why truth-telling would be modularized in this way. Are these heads truly computing truth, or are they detecting patterns that merely correlate with truthful outputs?</p>

    <p>However, there are important limitations to consider. The definition of "truth" is limited to the TruthfulQA benchmark's conception, and there's a clear trade-off between truthfulness and helpfulness that needs more investigation. The paper also doesn't fully explore what other aspects of model behavior might be affected by the intervention.</p>
  


<h2>Representation Surgery</h2>
    
    <p>This paper develops the theoretical foundations for steering functions - tools that help guide language model outputs toward desired characteristics.</p>
    
<h3>Formulation</h3>
    
    <p>The work builds on LEACE (<a href="https://arxiv.org/abs/2306.03819">Belrose, 2023</a>), which introduced optimal affine guarding functions for erasing concepts from model representations. The key insight is that by matching the averages (means) of representations between concepts, we can <strong>ensure affine guardedness</strong>.</p>
    
    <p>The authors first propose a simple solution that matches means. This transformation shifts representations of concept \(c\) toward concept \(c'\) by \((Î¼_{c'} - Î¼_c)\) while leaving representations that already have concept \(c'\) unchanged. This approach minimizes changes while ensuring the means match, providing theoretical backing for the "activation shift" technique used in <a href="https://arxiv.org/abs/2306.03341">Li (2024)</a>.</p>

    <p>\[\min_{s \in \text{Aff}_s(D)} \mathbb{E}\left[\|\mathbf{H} - s(\mathbf{H})\|_2^2\right]\]</p>
    <p>\[\text{subject to } \mathbb{E}[s(\mathbf{H}_c)] = \mathbb{E}[s(\mathbf{H}_{c'})]\]</p>
    <p>\[\text{has a solution}\]</p>
    <p>\[\quad s^*(\mathbf{H})(\mathbf{s}) = \begin{cases}
        \mathbf{H}(\mathbf{s}) + \boldsymbol{\mu}_{c'} - \mathbf{W}^* \boldsymbol{\mu}_c & \text{if } \phi(\mathbf{s}) = c \\[1ex]
        \mathbf{H}(\mathbf{s}) & \text{if } \phi(\mathbf{s}) = c'
    \end{cases} \qquad (1)\]</p>
    <p>\[\text{where } \mathbf{W}^* = \mathbf{I}\]</p>
    
    <p>However, matching only means leaves open the possibility of recovering the original concepts through non-linear methods.</p>
    
    <p>To address this, the authors extend their approach to match both means and covariances. This more sophisticated version applies an affine transformation \(\mathbf{W}\mathbf{H} + \mathbf{b}\) to concept c representations, where \(\mathbf{W}\) is derived from covariance matrices and \(\mathbf{b}\) ensures mean alignment. This preserves both mean behavior and relationships between features while leaving concept \(c'\) unchanged.</p>
    
    <p>\[\min_{s \in \text{Aff}_s(D)} \mathbb{E}\left[\|\mathbf{H} - s(\mathbf{H})\|_2^2\right]\]</p>
    <p>\[\text{subject to } \begin{aligned}
        & \mathbb{E}[s(\mathbf{H}_c)] = \mathbb{E}[s(\mathbf{H}_{c'})] \\
        & \mathbb{E}[s(\mathbf{H}_c)s(\mathbf{H}_c)^\top] = \mathbb{E}[s(\mathbf{H}_{c'})s(\mathbf{H}_{c'})^\top]
    \end{aligned}\]</p>
    <p>\[\text{has the solution}\]</p>
    <p>\[\quad s^*(\mathbf{H})(\mathbf{s}) = \begin{cases}
        \mathbf{W}^*\mathbf{H}(\mathbf{s}) + \mathbf{b}^* & \text{if } \phi(\mathbf{s}) = c \\[1ex]
        \mathbf{H}(\mathbf{s}) & \text{if } \phi(\mathbf{s}) = c'
    \end{cases} \qquad (2)\]</p>
    <p>\[\text{where we define}\]</p>
    <p>\[\mathbf{W}^* = \boldsymbol{\Sigma}_c^{-\frac{1}{2}}(\boldsymbol{\Sigma}_c^{\frac{1}{2}}\boldsymbol{\Sigma}_{c'}\boldsymbol{\Sigma}_c^{\frac{1}{2}})^{\frac{1}{2}}\boldsymbol{\Sigma}_c^{-\frac{1}{2}}\]</p>
    <p>\[\mathbf{b}^* = -\mathbf{W}^*\boldsymbol{\mu}_c + \boldsymbol{\mu}_{c'}\]</p>
    
<h3>Experiments</h3>

    <p>The authors test their steering functions in several scenarios:</p>
    
    <ol>
        <li>
            <p><strong>Gender Bias in Profession Classification</strong>: They train a linear probe to predict professions from the last layer representation of the last token of Llama 2, using biographies of gendered professionals. Performance is measured through both prediction accuracy and <strong>true positive rate</strong> (expected to be minimized).</p>
    
            <p>Results show that while mean-matching performs similarly to LEACE, the mean-and-covariance matching method achieves significantly better results compared to other approaches.</p>
    
            <img src="images/affine_1.png" style="max-width:70%; width:1200px;" class="mx-auto d-block">
        </li>
    
        <li>
            <p><strong>Toxicity Reduction</strong>: The authors apply their steering functions to reduce toxicity during text generation by intervening in the last hidden representation at each inference step.</p>
    
            <img src="images/affine_2.png" style="max-width:80%; width:1200px;" class="mx-auto d-block">
    
            <p>While both proposed methods successfully mitigate toxicity, they don't achieve state-of-the-art performance. This might be due to the mismatch between training (using last token representations) and inference (applying interventions at each generation step).</p>
    
            <p>However, their methods offer a key advantage: unlike baselines that require either fine-tuning or gradient computation during inference, these steering functions need neither, making them more computationally efficient.</p>
    
            <img src="images/affine_3.png" style="max-width:100%; width:1200px;" class="mx-auto d-block">
        </li>
    </ol>


<h2>Discussion Questions</h2>

<ul>
<li>How do the theoretical guarantees from Representation Surgery relate to the empirical findings about language transfer in the CodeLLM paper?</li>
<li> Is it possible to create steering functions that match moments of distribution beyond the second moment? What are the limitations and benefits of doing so?</li>
<li>What are the tradeoffs between targeting specific attention heads versus intervening in the full residual stream?</li>
<li>How might these techniques generalize beyond code and truthfulness to other aspects of model behavior?</li>
<li>What are the limitations of current steering methods and what theoretical advances might help overcome them?</li>
</ul>

</main>
</div>
</div>
</body>
</html>