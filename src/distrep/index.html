{header('Distributed Representations')}

<h5>September 17, 2024 â€¢ <em>David Bau</em></h5>

<p>Today we begin a segment of the class dealing with
<em>understanding representations</em>: understanding
the ways that information might be encoded in the
patterns of neural activations in a neural network.

<p>In the previous chapter on <a href="/neurons/">neurons</a>,
we examined some of the efforts to visualize and understand the
"concepts" that might be represented by individual neurons in
an artificial neural network.  The idea that individual neurons
might directly correspond to interpretable variables is the
"Local Representation" hypothesis.  As we saw, some individual
neurons do seem encode specific concepts. However, in practice,
many other individual neurons seem to lack clear mappings
to discrete understandable concepts. 

<p>
Hinton, McClelland, and Rumelhart have advocated the
view that all the neurons work together to store the aggregate
information in a network, and further that that this collective
cooperation is essentially irreducible.  The hypothesis that
every neuron is involved in encoding many different concepts is the
<a href="https://stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf">Distributed Representation</a>
model, and they have advocated this viewpoint since their 1987 work
"Parallel Distributed Processing" (PDP).  This distributed
representation view is the prevailing view among neuoscientists;
(most believe that the local view of individual neurons as concepts
is not true).

<p>If information is inherently distributed, then instead of
examining individual neurons \( h_i \in \mathbb{R} \) as scalar variables, we
should examine the <em>vector space</em> \( \mathbf{h} \in \mathbb{R}^N \)
of all the neurons together; we call this collective vector space the
<em>representation space</em> of the network.  In a feedforward
neural network, each layer of neurons is a bottleneck that fully
determines all the subsequent information in a network, so we will
typically look at the representation vector for the neurons in
a single layer \( \mathbf{h} \ in \mathbb{R}^d \), where the dimensionality
\( d \) is given by the number of neurons in a layer.

{img("images/zeiler-fig-7.jpg")}

<p>In 1987, the PDP authors examined several
possible ways of encoding information in a distributed fashion
within a representation vector space.  For example, in their
Figure 2 (pictured above), they observed the following dilemma.
Suppose neurons collaborate in two groups to encode an \( (x, y) \)
combination, like a point on a plane.  It leads to the problem that
if you use the same scheme to encode two points, there is not an
ambiguity for which \( x \) goes with which \( y \).  Thus they
faced a puzzle: when sets of neurons distribute information, how do they
encode it effectively?

<p>We begin today by looking at two recent papers that tackle this
representation encoding question from a geometric point of view. Both
begin with mathematical principles and look for some empirical support.

<h2>Toy Models of Superposition</h2>

<p>The paper <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy
Models of Superposition</a> is a blog post by Nelson Elhage and Chris Olah's group at Anthropic
in collaboration with Martin Wattenberg at Harvard. (As an aside: you might wonder
why this was published as a blog instead of a pdf paper.  For years, Olah has been
a big advocate of publishing research in non-traditionally-peer-reviewed blog formats.)

<h3>The Goal: Enumeration</h3>

<p>To understand the motivation of the paper, it is helpful to start at the end; in
their section 9, they explain that they would like to understand how to <b>enumerate</b>
all the features in a representation:

<blockquote class="blockquote">
We'd like a way to have confidence that models will never do certain behaviors such
as "deliberately deceive" or "manipulate." Today, it's unclear how one might show this,
but we believe a promising tool would be the ability to
<em>identify and enumerate over all features</em>.
</blockquote>

<p>This team had previously developed catalogs of neurons (as we read about) when
they previously worked at OpenAI.  They note that if each interpertable
variable corresponded to one neuron, then "you could enumerate over features
by enumerating over neurons."  Yet if we are in a distributed representation
where every neuron is involved in encoding multple variables, then it leaves
you with an unsolved problem: if the way each variable is encoded is unknown,
it becomes unclear how to find the representations of all the variables, or
even how to identify what the variables are.

<p>Nevertheless, the Elhage team is optimistic that it should be possible
to enumerate the variables, if we can somehow reverse-engineer the
distributed representation encodings learned by a neural network.

<h3>The Data: Synthetic Sparse Features</h3>

<p>To begin to tackle this reverse-engineering problem, the paper takes
the approach of training very small neural networks on very small problems
for which they can define a ground truth for the "interpretable variables."
Then they ask: what is the actual learned encoding?  How does the network
represent information about these variables?

<p>To play the role of interpretable variables, they propose "sparse features."
A sparse feature is a variable that is typically zero, but that becomes
positive in the rare but interesting situations where there is something to say.
(Hinton 1987 wrote about this idea; Elhage also suggests reading
<a href="https://www.sciencedirect.com/science/article/pii/S0042698997001697">Olshausen 1997</a>.)
For example, a sparse feature might be the information "there is a dog's head in the photo."
Since most photos don't include dog heads, it's usually zero, but sometimes it
is positive, and it could be a large number if there are lot of dogs in a
specific instance (for example).

<p>In their various experiments, Elhage does not use "real" data for features but
rather synthesizes random sparse features, which gives them control over the
following two characteristics for each feature \( x_i \):

<p> \( S_i \) is the sparsity of the \(i\)th feature, that is the probabilitity it is zero.
<p> \( I_i \) is the importance of the \(i\)th feature, the cost incurred if information about it was lost.

<p>For example, if \( (1 - S_i) = 0.01 \), then the \( i \)th feature is nonzero only 1\% of the time.
<p>And if \( I_i = 2 \) and \( I_j = 1 \), then the \( i \)th feature has twice the weight of the
\( j \)th feature in the reconstruction loss.

<p>For most of their experimens, they choose a uniform sparsity \( S \) for all the features
and select a varying importance and \( I_i \) for each of \( n \) features, and then
they synthesize fake data by creating random vectors \( [ x_1, x_2, ..., x_n ] \) that follow
the sparsity probabilities.  They use this data to train neural networks
on various tasks, then analyze the representations that are learned.  Since they know
the exact values and characteristics of their sparse features, they can measure
how those characteristics relate to different types of representations.

<h3>The Tasks: Autoencoding, Adversarial Attack, and Absolute Value</h3>

<p>The basic task they examine is autoencoding: learning a two-layer network that can squeeze the
information of many features \( x_i \) through a smaller number of hidden unit neurons,
while still being able to reconstruct \( x_i \) in the end.

{img("images/bottleneck.png", 30)}

<p>They make two unusual choices. One is to begin by studying a totally linear network
with no nonlinearities in the middle.  The other is to constrain the second network layer
to have the same weights as the first, just transposed.  That is, the first layer has
weights that act as follows:

\[ h = Wx = \left[ \begin{array}{c|c|c|c}
\phantom{0} & \phantom{0}  & & \phantom{0} \\
 w_1 & w_2 & ... & w_n \\
\phantom{0} & \phantom{0}  & & \phantom{0}
\end{array} \right] \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} =
\sum_i w_i x_i
\]

<p>And then the second layer has exactly the same weights, but transposed:

\[ \hat{x} = W^T h = \begin{bmatrix}
\phantom{0} & w_1^T & \phantom{0} \\ \hline
\phantom{0} & w_2^T & \phantom{0} \\ \hline
 & \vdots & \\ \hline
\phantom{0} & w_n^T & \phantom{0}
\end{bmatrix} h
=
\begin{bmatrix}
w_1^T h \\
w_2^T h \\
\vdots \\
w_n^T h
\end{bmatrix}
\]

<p>In other words: the first matrix is an embedding matrix that encodes the \(i\)th
feature as the vector \(w_i\), and the second matrix is a projection matrix
that decodes the \(i\)th feature by using the (Euclidean) inner product that
evaluates the component in the direction of \(w_i\) again.  For both encoding and
decoding $w_i$ can be thought of as the vector embedding of the \(i\)th feature.

<p>With this basic starting point, they train the following variations:

<p>Fully linear autoencoding: \( \hat{x} = W^T W x + b\),
 with \( \hat{x}_i \rightarrow x_i \).
<p>Rectified linear autoencoding: \( \hat{x} = \mathrm{ReLU}(W^T W x + b) \),
 with \( \hat{x}_i \rightarrow x_i \).
<p>Adversarial attack: \( \hat{x} = \mathrm{ReLU}(W^T W (x + \epsilon + b) \),
 with \( \epsilon \) chosen to maximize error in \(\hat{x}\).
<p>Absolute value: \( \hat{x} = \mathrm{ReLU}(W_2^T \mathrm{ReLU}(W_1 x) + b) \),
 with \( \hat{x}_i \rightarrow |x_i| \).

<p>(The "absolute value" setting is the most conventional neural network setting,
discarding the coupling of the weights and introducing a nonlinearity at the hidden layer.)

<h3>Conditions that Lead to Superposition</h3>

<p>The main finding of the paper is the counterintuitive result that sparsity
in the underlying features that is \( (1 - S) \) close to zero, leads to
<em>more</em> superposition and more entanglement, rather than less.

{img("images/sparsity.png", 100)}

<p>The ordinary boring solution for the network, shown on the left of the figure above
is what they call "PCA-like behavior".  When PCA is used to encode an \(n\) dimiensional problem
in a smaller \(m\) dimensional space, PCA simply finds the most important vector
directions in the distribution, directly encodes those, and then drops all the other
directions, setting those components to zero.  They found that without nonlinearities,
training always resulted in this PCA choice.  They also found that, even with nonlinearities,
the PCA choice was found when the data was not sparse, i.e., when all the features were
active most of the time.

<p>However, when we have both sparsity and nonlinearity, then the network exploits an
opportunity to encode <em>more</em> than \(m\) features within the \(m\)-dimensional vector space.
This sacrifices the orthogonal independence of the \(w_i\) feature embedding vectors, as shown
in this figure:

{img("images/overbasis.png", 90)}

<p>Thus, they argue, that the entanglement seen in distributed representations is inevitable:
it's not just about a choice of basis that is unknown, but it is about a choice of
embedding that <em>overcrowds</em> the available dimensions, and that does not cleanly
correspond to a basis.

<p>They supply a very nice <a href="{colab_link('colab/exact_toys.ipynb')}">python notebook
that derives an exact solution</a> for the optimal "amount" of dimensionality that should
be allocated to each feature, and they show that this model matches what is found
by the optimizer in practice.

{img("image/theorysweep.png", 30)}

<p>Then they examine learning dynamics and the exact geometry of the layout of the
vectors that are learned when they are in superposition, and they find some pretty
symmetries that correspond to the optimizer finding the corners of
regular polytopes in low dimensions.

{img("image/dynamics.jpg", 100)}

<h3>"Fractional Dimensions," Superposition, and Adversarial Attack</h3>

<p>One of the interesting quantities defined by the paper is their fractional "dimension".
In an ordinary PCA solution, all the feature vectors \( w_i \) are orthogonal
and consume a whole dimension, leaving space for only \(m\) features to be represented.
But they observe that a feature in superposition with other features will consume
<em>less</em> than a whole dimension, and they quantify the "dimensionality" of the \(i\)th
feature as:

\[
d_i = \frac{ ||w_i||^2 } { \sum_{j} (\hat{w}_i \cdot w_j)^2 } = \frac{ ||w_i||^4 }{\sum_{j} (w_{i} \cdot w_j)^2 }
\]

<p>

<h2>The Linear Representation Hypothesis</h2>

<h3>The Data: Semantic Vector Offset Directions</h3>

<h3>The Task: Find a Causal Inner Product</h3>





<p>
<a href="{colab_link('colab/toy_models.ipynb')}">colab link</a>

{footer()}
